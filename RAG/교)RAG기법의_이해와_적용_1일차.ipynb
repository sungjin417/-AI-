{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sungjin417/sesac-phython/blob/master/%EA%B5%90)RAG%EA%B8%B0%EB%B2%95%EC%9D%98_%EC%9D%B4%ED%95%B4%EC%99%80_%EC%A0%81%EC%9A%A9_1%EC%9D%BC%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LT1pd04BDos"
      },
      "source": [
        "\n",
        "# üåº RAGÍ∏∞Î≤ïÏùò Ïù¥Ìï¥ÏôÄ Ï†ÅÏö© - 1Ï∞®Ïãú(24.11.28)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wl4ymzYhczY4"
      },
      "outputs": [],
      "source": [
        "# pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7CILs0YCfP53"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import RecursiveUrlLoader # ÏûêÎèôÏúºÎ°ú ÌÅ¨Î°§ÎßÅ\n",
        "DOCS_PAGE='https://blog.vllm.ai/2023/06/20/vllm.html'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rp5ZmcQuza5E"
      },
      "outputs": [],
      "source": [
        "loader = RecursiveUrlLoader(DOCS_PAGE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sw-w4e-LBDow"
      },
      "outputs": [],
      "source": [
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYfN3CjmBDow",
        "outputId": "e31057b5-a54f-411c-dc00-375be9346cfa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://blog.vllm.ai/2023/06/20/vllm.html', 'content_type': 'text/html; charset=utf-8', 'title': 'vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog', 'description': 'GitHub | Documentation | Paper', 'language': 'en'}, page_content='<!DOCTYPE html>\\n<html lang=\"en\"><head>\\n  <meta charset=\"utf-8\">\\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"><!-- Begin Jekyll SEO tag v2.8.0 -->\\n<title>vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog</title>\\n<meta name=\"generator\" content=\"Jekyll v4.3.3\" />\\n<meta property=\"og:title\" content=\"vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention\" />\\n<meta name=\"author\" content=\"Woosuk Kwon*, Zhuohan Li*, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica (* Equal Contribution)\" />\\n<meta property=\"og:locale\" content=\"en_US\" />\\n<meta name=\"description\" content=\"GitHub | Documentation | Paper\" />\\n<meta property=\"og:description\" content=\"GitHub | Documentation | Paper\" />\\n<meta property=\"og:site_name\" content=\"vLLM Blog\" />\\n<meta property=\"og:type\" content=\"article\" />\\n<meta property=\"article:published_time\" content=\"2023-06-20T00:00:00-07:00\" />\\n<meta name=\"twitter:card\" content=\"summary\" />\\n<meta property=\"twitter:title\" content=\"vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention\" />\\n<script type=\"application/ld+json\">\\n{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"author\":{\"@type\":\"Person\",\"name\":\"Woosuk Kwon*, Zhuohan Li*, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica (* Equal Contribution)\"},\"dateModified\":\"2023-06-20T00:00:00-07:00\",\"datePublished\":\"2023-06-20T00:00:00-07:00\",\"description\":\"GitHub | Documentation | Paper\",\"headline\":\"vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"/2023/06/20/vllm.html\"},\"url\":\"/2023/06/20/vllm.html\"}</script>\\n<!-- End Jekyll SEO tag -->\\n<link rel=\"stylesheet\" href=\"/assets/css/style.css\"><link type=\"application/atom+xml\" rel=\"alternate\" href=\"/feed.xml\" title=\"vLLM Blog\" /><script async src=\"https://www.googletagmanager.com/gtag/js?id=G-9C5R3JR3QS\"></script>\\n<script>\\n  window[\\'ga-disable-G-9C5R3JR3QS\\'] = window.doNotTrack === \"1\" || navigator.doNotTrack === \"1\" || navigator.doNotTrack === \"yes\" || navigator.msDoNotTrack === \"1\";\\n  window.dataLayer = window.dataLayer || [];\\n  function gtag(){window.dataLayer.push(arguments);}\\n  gtag(\\'js\\', new Date());\\n\\n  gtag(\\'config\\', \\'G-9C5R3JR3QS\\');\\n</script>\\n\\n</head>\\n<body><header class=\"site-header\">\\n\\n  <div class=\"wrapper\"><a class=\"site-title\" rel=\"author\" href=\"/\">vLLM Blog</a></div>\\n</header>\\n<main class=\"page-content\" aria-label=\"Content\">\\n      <div class=\"wrapper\">\\n        <article class=\"post h-entry\" itemscope itemtype=\"http://schema.org/BlogPosting\"><br><p align=\"center\"><picture><img src=\"/assets/logos/vllm-logo-text-light.png\" width=\"65%\"></picture></p><br><header class=\"post-header\">\\n    <h1 class=\"post-title p-name\" itemprop=\"name headline\">vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention</h1>\\n    <p class=\"post-meta\"><time class=\"dt-published\" datetime=\"2023-06-20T00:00:00-07:00\" itemprop=\"datePublished\">\\n        Jun 20, 2023\\n      </time>‚Ä¢ \\n          <span itemprop=\"author\" itemscope itemtype=\"http://schema.org/Person\">\\n            <span class=\"p-author h-card\" itemprop=\"name\">Woosuk Kwon*, Zhuohan Li*, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica (* Equal Contribution)</span></span></p>\\n  </header>\\n\\n  <div class=\"post-content e-content\" itemprop=\"articleBody\">\\n    <p align=\"center\" style=\"margin-top:-15px\">\\n<a href=\"https://github.com/vllm-project/vllm\"><b>GitHub</b></a> | <a href=\"https://vllm.readthedocs.io/en/latest/\"><b>Documentation</b></a> | <a href=\"https://arxiv.org/pdf/2309.06180.pdf\"><b>Paper</b></a>\\n</p>\\n\\n<p>LLMs promise to fundamentally change how we use AI across all industries. However, actually serving these models is challenging and can be surprisingly slow even on expensive hardware. Today we are excited to introduce vLLM, an open-source library for fast LLM inference and serving. vLLM utilizes <strong>PagedAttention</strong>, our new attention algorithm that effectively manages attention keys and values. vLLM equipped with PagedAttention redefines the new state of the art in LLM serving: it delivers up to 24x higher throughput than HuggingFace Transformers, without requiring any model architecture changes.</p>\\n\\n<p>vLLM has been developed at UC Berkeley and deployed at <a href=\"https://chat.lmsys.org\">Chatbot Arena and Vicuna Demo</a> for the past two months. It is the core technology that makes LLM serving affordable even for a small research team like LMSYS with limited compute resources. Try out vLLM now with a single command at our <a href=\"https://github.com/vllm-project/vllm\">GitHub repository</a>.</p>\\n\\n<h3 id=\"beyond-state-of-the-art-performance\">Beyond State-of-the-art Performance</h3>\\n\\n<p>We compare the throughput of vLLM with <a href=\"https://huggingface.co/docs/transformers/main_classes/text_generation\">HuggingFace Transformers (HF)</a>, the most popular LLM library and <a href=\"https://github.com/huggingface/text-generation-inference\">HuggingFace Text Generation Inference (TGI)</a>, the previous state of the art. We evaluate in two settings: LLaMA-7B on an NVIDIA A10G GPU and LLaMA-13B on an NVIDIA A100 GPU (40GB). We sample the requests‚Äô input/output lengths from the ShareGPT dataset. In our experiments, vLLM achieves up to <strong>24x</strong> higher throughput compared to HF and up to <strong>3.5x</strong> higher throughput than TGI.</p>\\n\\n<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/perf_a100_n1_light.png\" width=\"45%\" />\\n</picture><picture>\\n<img src=\"/assets/figures/perf_a10g_n1_light.png\" width=\"45%\" />\\n</picture><br />\\nServing throughput when each request asks for <em> one output completion</em>. vLLM achieves 14x - 24x higher throughput than HF and 2.2x - 2.5x higher throughput than TGI.\\n</p>\\n\\n<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/perf_a100_n3_light.png\" width=\"45%\" />\\n</picture><picture>\\n<img src=\"/assets/figures/perf_a10g_n3_light.png\" width=\"45%\" />\\n</picture>\\n<br />Serving throughput when each request asks for <em>three parallel output completions</em>. vLLM achieves 8.5x - 15x higher throughput than HF and 3.3x - 3.5x higher throughput than TGI.\\n</p>\\n\\n<h3 id=\"the-secret-sauce-pagedattention\">The Secret Sauce: PagedAttention</h3>\\n\\n<p>In vLLM, we identify that the performance of LLM serving is bottlenecked by memory. In the autoregressive decoding process, all the input tokens to the LLM produce their attention key and value tensors, and these tensors are kept in GPU memory to generate next tokens. These cached key and value tensors are often referred to as KV cache. The KV cache is</p>\\n<ul>\\n  <li><em>Large:</em> Takes up to 1.7GB for a single sequence in LLaMA-13B.</li>\\n  <li><em>Dynamic:</em> Its size depends on the sequence length, which is highly variable and unpredictable.\\nAs a result, efficiently managing the KV cache presents a significant challenge. We find that existing systems waste <strong>60% ‚Äì 80%</strong> of memory due to fragmentation and over-reservation.</li>\\n</ul>\\n\\n<p>To address this problem, we introduce <strong>PagedAttention</strong>, an attention algorithm inspired by the classic idea of virtual memory and paging in operating systems. Unlike the traditional attention algorithms, PagedAttention allows storing continuous keys and values in non-contiguous memory space. Specifically, PagedAttention partitions the KV cache of each sequence into blocks, each block containing the keys and values for a fixed number of tokens. During the attention computation, the PagedAttention kernel identifies and fetches these blocks efficiently.</p>\\n\\n<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/annimation0.gif\" width=\"80%\" />\\n</picture>\\n<br />\\n<em>PagedAttention:</em> KV Cache are partitioned into blocks. Blocks do not need to be contiguous in memory space.\\n</p>\\n\\n<p>Because the blocks do not need to be contiguous in memory, we can manage the keys and values in a more flexible way as in OS‚Äôs virtual memory: one can think of blocks as pages, tokens as bytes, and sequences as processes. The contiguous <em>logical blocks</em> of a sequence are mapped to non-contiguous <em>physical blocks</em> via a block table. The physical blocks are allocated on demand as new tokens are generated.</p>\\n\\n<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/annimation1.gif\" width=\"100%\" />\\n</picture>\\n<br />\\nExample generation process for a request with PagedAttention.\\n</p>\\n\\n<p>In PagedAttention, memory waste only happens in the last block of a sequence. In practice, this results in near-optimal memory usage, with a mere waste of under 4%. This boost in memory efficiency proves highly beneficial: It allows the system to batch more sequences together, increase GPU utilization, and thereby significantly increase the throughput as shown in the performance result above.</p>\\n\\n<p>PagedAttention has another key advantage: efficient memory sharing. For example, in <em>parallel sampling</em>, multiple output sequences are generated from the same prompt. In this case, the computation and memory for the prompt can be shared between the output sequences.</p>\\n\\n<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/annimation2.gif\" width=\"80%\" />\\n</picture>\\n<br />\\nExample of parallel sampling.\\n</p>\\n\\n<p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p>\\n\\n<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/annimation3.gif\" width=\"100%\" />\\n</picture>\\n<br />\\nExample generation process for a request that samples multiple outputs.\\n</p>\\n\\n<p>PageAttention‚Äôs memory sharing greatly reduces the memory overhead of complex sampling algorithms, such as parallel sampling and beam search, cutting their memory usage by up to 55%. This can translate into up to 2.2x improvement in throughput. This makes such sampling methods practical in LLM services.</p>\\n\\n<p>PagedAttention is the core technology behind vLLM, our LLM inference and serving engine that supports a variety of models with high performance and an easy-to-use interface. For more technical details about vLLM and PagedAttention, check out our <a href=\"https://github.com/vllm-project/vllm\">GitHub repo</a> and stay tuned for our paper.</p>\\n\\n<h3 id=\"the-silent-hero-behind-lmsys-vicuna-and-chatbot-arena\">The Silent Hero Behind LMSYS Vicuna and Chatbot Arena</h3>\\n\\n<p>This April, <a href=\"https://lmsys.org\">LMSYS</a> developed the popular Vicuna chatbot models and made them publicly available. Since then, Vicuna has been served in <a href=\"https://arena.lmsys.org/\">Chatbot Arena</a> for millions of users. Initially, LMSYS FastChat adopted a HF Transformers based <a href=\"https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/model_worker.py\">serving backend</a> to serve the chat demo. As the demo became more popular, the peak traffic ramped up several times, making the HF backend a significant bottleneck. The LMSYS and vLLM team have worked together and soon developed the FastChat-vLLM integration to use vLLM <a href=\"https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/vllm_worker.py\">as the new backend</a> in order to support the growing demands (up to 5x more traffic). In an early <a href=\"https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/test_throughput.py\">internal micro-benchmark</a> by LMSYS, the vLLM serving backend can <strong>achieve up to 30x higher throughput than an initial HF backend.</strong></p>\\n\\n<p>Since mid-April, the most popular models such as Vicuna, Koala, and LLaMA, have all been successfully served using the FastChat-vLLM integration ‚Äì With FastChat as the multi-model chat serving frontend and vLLM as the inference backend, LMSYS is able to harness a limited number of university-sponsored GPUs to serve Vicuna to millions of users with <em>high throughput</em> and <em>low latency</em>. LMSYS is expanding the use of vLLM to a wider range of models, including Databricks Dolly, LAION‚Äôs OpenAsssiant, and Stability AI‚Äôs stableLM. The <a href=\"https://vllm.readthedocs.io/en/latest/models/supported_models.html\">support for more models</a> is being developed and forthcoming.</p>\\n\\n<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/lmsys_traffic.png\" width=\"100%\" />\\n</picture>\\n<br />\\nRequests served by FastChat-vLLM integration in the Chatbot Arena between April to May. Indeed, more than half of the requests to Chatbot Arena use vLLM as the inference backend.\\n</p>\\n\\n<p>This utilization of vLLM has also significantly reduced operational costs. With vLLM, LMSYS was able to cut the number of GPUs used for serving the above traffic by 50%. vLLM has been handling an average of 30K requests daily and a peak of 60K, which is a clear demonstration of vLLM‚Äôs robustness.</p>\\n\\n<h3 id=\"get-started-with-vllm\">Get started with vLLM</h3>\\n\\n<p>Install vLLM with the following command (check out our <a href=\"https://vllm.readthedocs.io/en/latest/getting_started/installation.html\">installation guide</a> for more):</p>\\n\\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>pip <span class=\"nb\">install </span>vllm\\n</code></pre></div></div>\\n\\n<p>vLLM can be used for both offline inference and online serving. To use vLLM for offline inference, you can import vLLM and use the <code class=\"language-plaintext highlighter-rouge\">LLM</code> class in your Python scripts:</p>\\n\\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"n\">vllm</span> <span class=\"kn\">import</span> <span class=\"n\">LLM</span>\\n\\n<span class=\"n\">prompts</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">Hello, my name is</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">The capital of France is</span><span class=\"sh\">\"</span><span class=\"p\">]</span>  <span class=\"c1\"># Sample prompts.\\n</span><span class=\"n\">llm</span> <span class=\"o\">=</span> <span class=\"nc\">LLM</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">lmsys/vicuna-7b-v1.3</span><span class=\"sh\">\"</span><span class=\"p\">)</span>  <span class=\"c1\"># Create an LLM.\\n</span><span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"p\">.</span><span class=\"nf\">generate</span><span class=\"p\">(</span><span class=\"n\">prompts</span><span class=\"p\">)</span>  <span class=\"c1\"># Generate texts from the prompts.\\n</span></code></pre></div></div>\\n\\n<p>To use vLLM for online serving, you can start an OpenAI API-compatible server via:</p>\\n\\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>python <span class=\"nt\">-m</span> vllm.entrypoints.openai.api_server <span class=\"nt\">--model</span> lmsys/vicuna-7b-v1.3\\n</code></pre></div></div>\\n\\n<p>You can query the server with the same format as OpenAI API:</p>\\n\\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>curl http://localhost:8000/v1/completions <span class=\"se\">\\\\</span>\\n    <span class=\"nt\">-H</span> <span class=\"s2\">\"Content-Type: application/json\"</span> <span class=\"se\">\\\\</span>\\n    <span class=\"nt\">-d</span> <span class=\"s1\">\\'{\\n        \"model\": \"lmsys/vicuna-7b-v1.3\",\\n        \"prompt\": \"San Francisco is a\",\\n        \"max_tokens\": 7,\\n        \"temperature\": 0\\n    }\\'</span>\\n</code></pre></div></div>\\n\\n<p>For more ways to use vLLM, please check out the <a href=\"https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html\">quickstart guide</a>.</p>\\n\\n<p><br /></p>\\n\\n<hr />\\n\\n<p><em>Blog written by Woosuk Kwon and Zhuohan Li (UC Berkeley). Special thanks to Hao Zhang for the integration of vLLM and FastChat and for writing the corresponding section. We thank the entire team\\u200a‚Äî\\u200aSiyuan Zhuang, Ying Sheng, Lianmin Zheng (UC Berkeley), Cody Yu (Independent Researcher), Joey Gonzalez (UC Berkeley), Hao Zhang (UC Berkeley &amp; UCSD), and Ion Stoica (UC Berkeley).</em></p>\\n\\n  </div><a class=\"u-url\" href=\"/2023/06/20/vllm.html\" hidden></a>\\n</article>\\n\\n      </div>\\n    </main><footer class=\"site-footer h-card\">\\n  <data class=\"u-url\" href=\"/\"></data>\\n\\n  <div class=\"wrapper\">\\n\\n    <div class=\"footer-col-wrapper\">\\n      <div class=\"footer-col\">\\n        <!-- <p class=\"feed-subscribe\">\\n          <a href=\"/feed.xml\">\\n            <svg class=\"svg-icon orange\">\\n              <use xlink:href=\"/assets/minima-social-icons.svg#rss\"></use>\\n            </svg><span>Subscribe</span>\\n          </a>\\n        </p> -->\\n        <ul class=\"contact-list\">\\n          <li class=\"p-name\">¬© 2024. vLLM Team. All rights reserved.</li>\\n          <li><a href=\"https://github.com/vllm-project/vllm\">https://github.com/vllm-project/vllm</a></li>\\n        </ul>\\n      </div>\\n      <div class=\"footer-col\">\\n        <p></p>\\n      </div>\\n    </div>\\n\\n    <div class=\"social-links\"><ul class=\"social-media-list\"></ul>\\n</div>\\n\\n  </div>\\n\\n</footer>\\n</body>\\n\\n</html>\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shAP35B-BDow",
        "outputId": "448dec57-ccca-4359-b019-05bbfd242c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "document Í∞úÏàò : 1\n"
          ]
        }
      ],
      "source": [
        "num_documents = len(docs)\n",
        "print(f'document Í∞úÏàò : {num_documents}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLX80BVGz0SZ",
        "outputId": "ccb3b559-bf96-4640-e6ce-e08f7b49c797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "N_GPU = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTlSGVvfBDox",
        "outputId": "05da5723-efe9-4e92-fec9-34c4bba9e841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vmN-D8C30Gsj"
      },
      "outputs": [],
      "source": [
        "model_name = \"BAAI/bge-large-en-v1.5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-CL_Cv3BDox",
        "outputId": "33ab7a80-a4bc-4a80-9ee5-307888ee14e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "encoder = SentenceTransformer(model_name, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "byQqhQMwBDox"
      },
      "outputs": [],
      "source": [
        "embedding_dim = encoder.get_sentence_embedding_dimension()\n",
        "max_seq_length = encoder.get_max_seq_length()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Î¨∏Ïû• ÏûÑÎ≤†Îî© Ïãú ÏÉùÏÑ±ÎêòÎäî Î≤°ÌÑ∞Ïùò Ï∞®Ïõê ÌÅ¨Í∏∞ : {embedding_dim}')\n",
        "print(f'Î™®Îç∏Ïù¥ Ï≤òÎ¶¨Ìï† Ïàò ÏûàÎäî ÏûÖÎ†• Î¨∏Ïû•Ïùò ÏµúÎåÄ ÌÜ†ÌÅ∞ Í∏∏Ïù¥ : {max_seq_length}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUHiwsfYEg2D",
        "outputId": "45c7f860-db15-48ac-e695-17a54b068601"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Î¨∏Ïû• ÏûÑÎ≤†Îî© Ïãú ÏÉùÏÑ±ÎêòÎäî Î≤°ÌÑ∞Ïùò Ï∞®Ïõê ÌÅ¨Í∏∞ : 1024\n",
            "Î™®Îç∏Ïù¥ Ï≤òÎ¶¨Ìï† Ïàò ÏûàÎäî ÏûÖÎ†• Î¨∏Ïû•Ïùò ÏµúÎåÄ ÌÜ†ÌÅ∞ Í∏∏Ïù¥ : 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYfFN0h41x25",
        "outputId": "2f673792-6af3-4498-8966-9cfc3a9d60be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chunk_size : 512, chunk_overlap : 51.0\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import numpy as np\n",
        "\n",
        "chunk_size = 512\n",
        "chunk_overlap = np.round(chunk_size * 0.1, 0) # Ïò§Ï∞®Î≤îÏúÑÎ•º ÎëêÏñ¥ Í≤πÏπòÍ≤å ÌïòÎäî Í±∞(Î¨∏Îß•Ïù¥ Ï§ëÍ∞ÑÏóê Ïß§Î¶¨Î©¥ ÎÇ¥Ïö©Ïù¥ Ïù¥Ïñ¥ÏßÄÏßÄ ÏïäÍ∏∞ ÎïåÎ¨∏Ïóê)\n",
        "# 10 ~ 20 % Í∞Ä Ï†ÅÎãπ ÎÑàÎ¨¥ ÏßßÏúºÎ©¥ Ïó¨Ï†ÑÏù¥ Î¨∏Îß•Ïù¥ ÎÅäÍ∏∞Í≥†, ÎÑàÎ¨¥ Í∏∏Ïñ¥ÎèÑ Î¨∏Ï†úÍ∞Ä ÏÉùÍ∏∞Í∏∞ ÎïåÎ¨∏Ïóê\n",
        "print(f'chunk_size : {chunk_size}, chunk_overlap : {chunk_overlap}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rXPOLGqBDox",
        "outputId": "36ecb038-4ae6-4c16-f9f7-21ebd937dbda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÏïàÎÖïÌïòÏÑ∏Ïöî ÏïÑÏ£º Í∏¥ ÌÖçÏä§Ìä∏Î•º\n",
            "ÌÖçÏä§Ìä∏Î•º ÎÇòÎà†Î≥¥Í≤†ÏäµÎãàÎã§. Î¨∏Ïû•\n",
            "Î¨∏Ïû• Îã®ÏúÑÎ°ú Ïñ¥ÎñªÍ≤å ÎÇòÎàÑÏñ¥Ïïº\n",
            "ÎÇòÎàÑÏñ¥Ïïº Ìï†ÍπåÏöî?\n"
          ]
        }
      ],
      "source": [
        "# chunk_overlapÏùÑ Ïù¥Ìï¥ÌïòÍ∏∞ ÏúÑÌïú ÏòàÏãú\n",
        "long_text = 'ÏïàÎÖïÌïòÏÑ∏Ïöî ÏïÑÏ£º Í∏¥ ÌÖçÏä§Ìä∏Î•º ÎÇòÎà†Î≥¥Í≤†ÏäµÎãàÎã§. Î¨∏Ïû• Îã®ÏúÑÎ°ú Ïñ¥ÎñªÍ≤å ÎÇòÎàÑÏñ¥Ïïº Ìï†ÍπåÏöî?'\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 20,\n",
        "    chunk_overlap = 5\n",
        "\n",
        ")\n",
        "text_chunks = text_splitter.split_text(long_text)\n",
        "for chunk in text_chunks:\n",
        "  print(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oNfU1t8wBDoy"
      },
      "outputs": [],
      "source": [
        "child_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = chunk_size,\n",
        "    chunk_overlap = chunk_overlap\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yilJXueBDoy",
        "outputId": "9d074f17-f866-45d3-9060-c72b0f5ae2e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1Í∞úÏùò docsÍ∞Ä 52Í∞úÏùò Î¨∏ÏûêÎ°ú ÎÇòÎâòÏóàÎã§\n"
          ]
        }
      ],
      "source": [
        "chunks = child_splitter.split_documents(docs)\n",
        "print(f'{len(docs)}Í∞úÏùò docsÍ∞Ä {len(chunks)}Í∞úÏùò Î¨∏ÏûêÎ°ú ÎÇòÎâòÏóàÎã§') # docsÎäî Î≥¥ÌÜµ 1Í∞úÍ∞Ä ÏïÑÎãàÎùº Ïó¨Îü¨Í∞úÏù¥Îã§ Ï∞∏Í≥†!!!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1TDST1bJBDoy"
      },
      "outputs": [],
      "source": [
        "string_list = []\n",
        "for doc in chunks:\n",
        "  if hasattr(doc, 'page_content'): # page_contentÎ•º ÌôïÏù∏ÌïòÎäî ÏΩîÎìú\n",
        "  # hasattr(Í∞ùÏ≤¥, ÏÜçÏÑ±Î™Ö) -> Í∞ùÏ≤¥Ïóê ÏÜçÏÑ±Î™ÖÏù¥ ÏûàÎäîÏßÄ\n",
        "    string_list.append(doc.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiyMiRh0BDoy",
        "outputId": "9defc9d8-a3ff-44ee-a023-0b7980842afd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<!DOCTYPE html>\\n<html lang=\"en\"><head>\\n  <meta charset=\"utf-8\">\\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"><!-- Begin Jekyll SEO tag v2.8.0 -->\\n<title>vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog</title>\\n<meta name=\"generator\" content=\"Jekyll v4.3.3\" />\\n<meta property=\"og:title\" content=\"vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention\" />',\n",
              " '<meta name=\"author\" content=\"Woosuk Kwon*, Zhuohan Li*, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica (* Equal Contribution)\" />\\n<meta property=\"og:locale\" content=\"en_US\" />\\n<meta name=\"description\" content=\"GitHub | Documentation | Paper\" />\\n<meta property=\"og:description\" content=\"GitHub | Documentation | Paper\" />\\n<meta property=\"og:site_name\" content=\"vLLM Blog\" />\\n<meta property=\"og:type\" content=\"article\" />',\n",
              " '<meta property=\"og:type\" content=\"article\" />\\n<meta property=\"article:published_time\" content=\"2023-06-20T00:00:00-07:00\" />\\n<meta name=\"twitter:card\" content=\"summary\" />\\n<meta property=\"twitter:title\" content=\"vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention\" />\\n<script type=\"application/ld+json\">',\n",
              " '{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"author\":{\"@type\":\"Person\",\"name\":\"Woosuk Kwon*, Zhuohan Li*, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica (* Equal Contribution)\"},\"dateModified\":\"2023-06-20T00:00:00-07:00\",\"datePublished\":\"2023-06-20T00:00:00-07:00\",\"description\":\"GitHub | Documentation | Paper\",\"headline\":\"vLLM: Easy, Fast, and Cheap LLM Serving with',\n",
              " 'Easy, Fast, and Cheap LLM Serving with PagedAttention\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"/2023/06/20/vllm.html\"},\"url\":\"/2023/06/20/vllm.html\"}</script>',\n",
              " '<!-- End Jekyll SEO tag -->\\n<link rel=\"stylesheet\" href=\"/assets/css/style.css\"><link type=\"application/atom+xml\" rel=\"alternate\" href=\"/feed.xml\" title=\"vLLM Blog\" /><script async src=\"https://www.googletagmanager.com/gtag/js?id=G-9C5R3JR3QS\"></script>\\n<script>\\n  window[\\'ga-disable-G-9C5R3JR3QS\\'] = window.doNotTrack === \"1\" || navigator.doNotTrack === \"1\" || navigator.doNotTrack === \"yes\" || navigator.msDoNotTrack === \"1\";\\n  window.dataLayer = window.dataLayer || [];',\n",
              " \"window.dataLayer = window.dataLayer || [];\\n  function gtag(){window.dataLayer.push(arguments);}\\n  gtag('js', new Date());\",\n",
              " 'gtag(\\'config\\', \\'G-9C5R3JR3QS\\');\\n</script>\\n\\n</head>\\n<body><header class=\"site-header\">',\n",
              " '<div class=\"wrapper\"><a class=\"site-title\" rel=\"author\" href=\"/\">vLLM Blog</a></div>\\n</header>\\n<main class=\"page-content\" aria-label=\"Content\">\\n      <div class=\"wrapper\">\\n        <article class=\"post h-entry\" itemscope itemtype=\"http://schema.org/BlogPosting\"><br><p align=\"center\"><picture><img src=\"/assets/logos/vllm-logo-text-light.png\" width=\"65%\"></picture></p><br><header class=\"post-header\">',\n",
              " '<h1 class=\"post-title p-name\" itemprop=\"name headline\">vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention</h1>\\n    <p class=\"post-meta\"><time class=\"dt-published\" datetime=\"2023-06-20T00:00:00-07:00\" itemprop=\"datePublished\">\\n        Jun 20, 2023\\n      </time>‚Ä¢ \\n          <span itemprop=\"author\" itemscope itemtype=\"http://schema.org/Person\">',\n",
              " '<span class=\"p-author h-card\" itemprop=\"name\">Woosuk Kwon*, Zhuohan Li*, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica (* Equal Contribution)</span></span></p>\\n  </header>',\n",
              " '<div class=\"post-content e-content\" itemprop=\"articleBody\">\\n    <p align=\"center\" style=\"margin-top:-15px\">\\n<a href=\"https://github.com/vllm-project/vllm\"><b>GitHub</b></a> | <a href=\"https://vllm.readthedocs.io/en/latest/\"><b>Documentation</b></a> | <a href=\"https://arxiv.org/pdf/2309.06180.pdf\"><b>Paper</b></a>\\n</p>',\n",
              " '<p>LLMs promise to fundamentally change how we use AI across all industries. However, actually serving these models is challenging and can be surprisingly slow even on expensive hardware. Today we are excited to introduce vLLM, an open-source library for fast LLM inference and serving. vLLM utilizes <strong>PagedAttention</strong>, our new attention algorithm that effectively manages attention keys and values. vLLM equipped with PagedAttention redefines the new state of the art in LLM serving: it delivers',\n",
              " 'new state of the art in LLM serving: it delivers up to 24x higher throughput than HuggingFace Transformers, without requiring any model architecture changes.</p>',\n",
              " '<p>vLLM has been developed at UC Berkeley and deployed at <a href=\"https://chat.lmsys.org\">Chatbot Arena and Vicuna Demo</a> for the past two months. It is the core technology that makes LLM serving affordable even for a small research team like LMSYS with limited compute resources. Try out vLLM now with a single command at our <a href=\"https://github.com/vllm-project/vllm\">GitHub repository</a>.</p>\\n\\n<h3 id=\"beyond-state-of-the-art-performance\">Beyond State-of-the-art Performance</h3>',\n",
              " '<p>We compare the throughput of vLLM with <a href=\"https://huggingface.co/docs/transformers/main_classes/text_generation\">HuggingFace Transformers (HF)</a>, the most popular LLM library and <a href=\"https://github.com/huggingface/text-generation-inference\">HuggingFace Text Generation Inference (TGI)</a>, the previous state of the art. We evaluate in two settings: LLaMA-7B on an NVIDIA A10G GPU and LLaMA-13B on an NVIDIA A100 GPU (40GB). We sample the requests‚Äô input/output lengths from the ShareGPT',\n",
              " 'requests‚Äô input/output lengths from the ShareGPT dataset. In our experiments, vLLM achieves up to <strong>24x</strong> higher throughput compared to HF and up to <strong>3.5x</strong> higher throughput than TGI.</p>',\n",
              " '<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/perf_a100_n1_light.png\" width=\"45%\" />\\n</picture><picture>\\n<img src=\"/assets/figures/perf_a10g_n1_light.png\" width=\"45%\" />\\n</picture><br />\\nServing throughput when each request asks for <em> one output completion</em>. vLLM achieves 14x - 24x higher throughput than HF and 2.2x - 2.5x higher throughput than TGI.\\n</p>',\n",
              " '<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/perf_a100_n3_light.png\" width=\"45%\" />\\n</picture><picture>\\n<img src=\"/assets/figures/perf_a10g_n3_light.png\" width=\"45%\" />\\n</picture>\\n<br />Serving throughput when each request asks for <em>three parallel output completions</em>. vLLM achieves 8.5x - 15x higher throughput than HF and 3.3x - 3.5x higher throughput than TGI.\\n</p>\\n\\n<h3 id=\"the-secret-sauce-pagedattention\">The Secret Sauce: PagedAttention</h3>',\n",
              " '<p>In vLLM, we identify that the performance of LLM serving is bottlenecked by memory. In the autoregressive decoding process, all the input tokens to the LLM produce their attention key and value tensors, and these tensors are kept in GPU memory to generate next tokens. These cached key and value tensors are often referred to as KV cache. The KV cache is</p>\\n<ul>\\n  <li><em>Large:</em> Takes up to 1.7GB for a single sequence in LLaMA-13B.</li>',\n",
              " '<li><em>Dynamic:</em> Its size depends on the sequence length, which is highly variable and unpredictable.\\nAs a result, efficiently managing the KV cache presents a significant challenge. We find that existing systems waste <strong>60% ‚Äì 80%</strong> of memory due to fragmentation and over-reservation.</li>\\n</ul>',\n",
              " '<p>To address this problem, we introduce <strong>PagedAttention</strong>, an attention algorithm inspired by the classic idea of virtual memory and paging in operating systems. Unlike the traditional attention algorithms, PagedAttention allows storing continuous keys and values in non-contiguous memory space. Specifically, PagedAttention partitions the KV cache of each sequence into blocks, each block containing the keys and values for a fixed number of tokens. During the attention computation, the',\n",
              " 'of tokens. During the attention computation, the PagedAttention kernel identifies and fetches these blocks efficiently.</p>',\n",
              " '<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/annimation0.gif\" width=\"80%\" />\\n</picture>\\n<br />\\n<em>PagedAttention:</em> KV Cache are partitioned into blocks. Blocks do not need to be contiguous in memory space.\\n</p>',\n",
              " '<p>Because the blocks do not need to be contiguous in memory, we can manage the keys and values in a more flexible way as in OS‚Äôs virtual memory: one can think of blocks as pages, tokens as bytes, and sequences as processes. The contiguous <em>logical blocks</em> of a sequence are mapped to non-contiguous <em>physical blocks</em> via a block table. The physical blocks are allocated on demand as new tokens are generated.</p>',\n",
              " '<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/annimation1.gif\" width=\"100%\" />\\n</picture>\\n<br />\\nExample generation process for a request with PagedAttention.\\n</p>',\n",
              " '<p>In PagedAttention, memory waste only happens in the last block of a sequence. In practice, this results in near-optimal memory usage, with a mere waste of under 4%. This boost in memory efficiency proves highly beneficial: It allows the system to batch more sequences together, increase GPU utilization, and thereby significantly increase the throughput as shown in the performance result above.</p>',\n",
              " '<p>PagedAttention has another key advantage: efficient memory sharing. For example, in <em>parallel sampling</em>, multiple output sequences are generated from the same prompt. In this case, the computation and memory for the prompt can be shared between the output sequences.</p>\\n\\n<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/annimation2.gif\" width=\"80%\" />\\n</picture>\\n<br />\\nExample of parallel sampling.\\n</p>',\n",
              " '<p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p>',\n",
              " '<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/annimation3.gif\" width=\"100%\" />\\n</picture>\\n<br />\\nExample generation process for a request that samples multiple outputs.\\n</p>\\n\\n<p>PageAttention‚Äôs memory sharing greatly reduces the memory overhead of complex sampling algorithms, such as parallel sampling and beam search, cutting their memory usage by up to 55%. This can translate into up to 2.2x improvement in throughput. This makes such sampling methods practical in LLM services.</p>',\n",
              " '<p>PagedAttention is the core technology behind vLLM, our LLM inference and serving engine that supports a variety of models with high performance and an easy-to-use interface. For more technical details about vLLM and PagedAttention, check out our <a href=\"https://github.com/vllm-project/vllm\">GitHub repo</a> and stay tuned for our paper.</p>\\n\\n<h3 id=\"the-silent-hero-behind-lmsys-vicuna-and-chatbot-arena\">The Silent Hero Behind LMSYS Vicuna and Chatbot Arena</h3>',\n",
              " '<p>This April, <a href=\"https://lmsys.org\">LMSYS</a> developed the popular Vicuna chatbot models and made them publicly available. Since then, Vicuna has been served in <a href=\"https://arena.lmsys.org/\">Chatbot Arena</a> for millions of users. Initially, LMSYS FastChat adopted a HF Transformers based <a href=\"https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/model_worker.py\">serving backend</a> to serve the chat demo. As the demo became more popular, the peak traffic ramped up several times,',\n",
              " 'popular, the peak traffic ramped up several times, making the HF backend a significant bottleneck. The LMSYS and vLLM team have worked together and soon developed the FastChat-vLLM integration to use vLLM <a href=\"https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/vllm_worker.py\">as the new backend</a> in order to support the growing demands (up to 5x more traffic). In an early <a href=\"https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/test_throughput.py\">internal micro-benchmark</a> by',\n",
              " 'micro-benchmark</a> by LMSYS, the vLLM serving backend can <strong>achieve up to 30x higher throughput than an initial HF backend.</strong></p>',\n",
              " '<p>Since mid-April, the most popular models such as Vicuna, Koala, and LLaMA, have all been successfully served using the FastChat-vLLM integration ‚Äì With FastChat as the multi-model chat serving frontend and vLLM as the inference backend, LMSYS is able to harness a limited number of university-sponsored GPUs to serve Vicuna to millions of users with <em>high throughput</em> and <em>low latency</em>. LMSYS is expanding the use of vLLM to a wider range of models, including Databricks Dolly, LAION‚Äôs',\n",
              " 'of models, including Databricks Dolly, LAION‚Äôs OpenAsssiant, and Stability AI‚Äôs stableLM. The <a href=\"https://vllm.readthedocs.io/en/latest/models/supported_models.html\">support for more models</a> is being developed and forthcoming.</p>',\n",
              " '<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/lmsys_traffic.png\" width=\"100%\" />\\n</picture>\\n<br />\\nRequests served by FastChat-vLLM integration in the Chatbot Arena between April to May. Indeed, more than half of the requests to Chatbot Arena use vLLM as the inference backend.\\n</p>',\n",
              " '<p>This utilization of vLLM has also significantly reduced operational costs. With vLLM, LMSYS was able to cut the number of GPUs used for serving the above traffic by 50%. vLLM has been handling an average of 30K requests daily and a peak of 60K, which is a clear demonstration of vLLM‚Äôs robustness.</p>\\n\\n<h3 id=\"get-started-with-vllm\">Get started with vLLM</h3>',\n",
              " '<p>Install vLLM with the following command (check out our <a href=\"https://vllm.readthedocs.io/en/latest/getting_started/installation.html\">installation guide</a> for more):</p>\\n\\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>pip <span class=\"nb\">install </span>vllm\\n</code></pre></div></div>',\n",
              " '<p>vLLM can be used for both offline inference and online serving. To use vLLM for offline inference, you can import vLLM and use the <code class=\"language-plaintext highlighter-rouge\">LLM</code> class in your Python scripts:</p>\\n\\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"n\">vllm</span> <span class=\"kn\">import</span> <span class=\"n\">LLM</span>',\n",
              " '<span class=\"n\">prompts</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">Hello, my name is</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">The capital of France is</span><span class=\"sh\">\"</span><span class=\"p\">]</span>  <span class=\"c1\"># Sample prompts.',\n",
              " '</span><span class=\"n\">llm</span> <span class=\"o\">=</span> <span class=\"nc\">LLM</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">lmsys/vicuna-7b-v1.3</span><span class=\"sh\">\"</span><span class=\"p\">)</span>  <span class=\"c1\"># Create an LLM.',\n",
              " '</span><span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"p\">.</span><span class=\"nf\">generate</span><span class=\"p\">(</span><span class=\"n\">prompts</span><span class=\"p\">)</span>  <span class=\"c1\"># Generate texts from the prompts.\\n</span></code></pre></div></div>',\n",
              " '<p>To use vLLM for online serving, you can start an OpenAI API-compatible server via:</p>\\n\\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>python <span class=\"nt\">-m</span> vllm.entrypoints.openai.api_server <span class=\"nt\">--model</span> lmsys/vicuna-7b-v1.3\\n</code></pre></div></div>\\n\\n<p>You can query the server with the same format as OpenAI API:</p>',\n",
              " '<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>curl http://localhost:8000/v1/completions <span class=\"se\">\\\\</span>\\n    <span class=\"nt\">-H</span> <span class=\"s2\">\"Content-Type: application/json\"</span> <span class=\"se\">\\\\</span>\\n    <span class=\"nt\">-d</span> <span class=\"s1\">\\'{\\n        \"model\": \"lmsys/vicuna-7b-v1.3\",\\n        \"prompt\": \"San Francisco is a\",\\n        \"max_tokens\": 7,\\n        \"temperature\": 0\\n    }\\'</span>',\n",
              " '\"temperature\": 0\\n    }\\'</span>\\n</code></pre></div></div>',\n",
              " '<p>For more ways to use vLLM, please check out the <a href=\"https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html\">quickstart guide</a>.</p>\\n\\n<p><br /></p>\\n\\n<hr />',\n",
              " '<p><br /></p>\\n\\n<hr />\\n\\n<p><em>Blog written by Woosuk Kwon and Zhuohan Li (UC Berkeley). Special thanks to Hao Zhang for the integration of vLLM and FastChat and for writing the corresponding section. We thank the entire team\\u200a‚Äî\\u200aSiyuan Zhuang, Ying Sheng, Lianmin Zheng (UC Berkeley), Cody Yu (Independent Researcher), Joey Gonzalez (UC Berkeley), Hao Zhang (UC Berkeley &amp; UCSD), and Ion Stoica (UC Berkeley).</em></p>\\n\\n  </div><a class=\"u-url\" href=\"/2023/06/20/vllm.html\" hidden></a>\\n</article>',\n",
              " '</div>\\n    </main><footer class=\"site-footer h-card\">\\n  <data class=\"u-url\" href=\"/\"></data>\\n\\n  <div class=\"wrapper\">',\n",
              " '<div class=\"footer-col-wrapper\">\\n      <div class=\"footer-col\">\\n        <!-- <p class=\"feed-subscribe\">\\n          <a href=\"/feed.xml\">\\n            <svg class=\"svg-icon orange\">\\n              <use xlink:href=\"/assets/minima-social-icons.svg#rss\"></use>\\n            </svg><span>Subscribe</span>\\n          </a>\\n        </p> -->\\n        <ul class=\"contact-list\">\\n          <li class=\"p-name\">¬© 2024. vLLM Team. All rights reserved.</li>',\n",
              " '<li><a href=\"https://github.com/vllm-project/vllm\">https://github.com/vllm-project/vllm</a></li>\\n        </ul>\\n      </div>\\n      <div class=\"footer-col\">\\n        <p></p>\\n      </div>\\n    </div>',\n",
              " '<div class=\"social-links\"><ul class=\"social-media-list\"></ul>\\n</div>\\n\\n  </div>\\n\\n</footer>\\n</body>\\n\\n</html>']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "string_list # page_contentÏùò Í∞íÎßå Îã¥ÍπÄ!!!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFu7OvcoBDoy",
        "outputId": "c06c6c3c-906d-482d-f615-185f9d207597"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0234,  0.0221, -0.0177,  ..., -0.0006, -0.0043,  0.0399],\n",
              "        [ 0.0171,  0.0109, -0.0180,  ..., -0.0039, -0.0265,  0.0146],\n",
              "        [ 0.0079,  0.0002,  0.0203,  ..., -0.0080, -0.0263,  0.0426],\n",
              "        ...,\n",
              "        [ 0.0074,  0.0103,  0.0246,  ...,  0.0150, -0.0150,  0.0111],\n",
              "        [-0.0002,  0.0002,  0.0041,  ...,  0.0242, -0.0120, -0.0135],\n",
              "        [-0.0334, -0.0142,  0.0187,  ..., -0.0048, -0.0167,  0.0039]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "embeddings = torch.tensor(encoder.encode(string_list))\n",
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brKZ-PAJBDoy",
        "outputId": "ef13be1f-43c7-45b0-d259-2af7715e276a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.2433148e-03,  3.0656536e-03, -2.4589787e-03, ...,\n",
              "        -7.9131096e-05, -5.9362175e-04,  5.5376114e-03],\n",
              "       [ 2.3663254e-03,  1.5079604e-03, -2.4914653e-03, ...,\n",
              "        -5.3539872e-04, -3.6686282e-03,  2.0307114e-03],\n",
              "       [ 1.0944124e-03,  3.0741558e-05,  2.8103061e-03, ...,\n",
              "        -1.1119934e-03, -3.6409216e-03,  5.9099812e-03],\n",
              "       ...,\n",
              "       [ 1.0273707e-03,  1.4269428e-03,  3.4093512e-03, ...,\n",
              "         2.0777669e-03, -2.0858389e-03,  1.5358157e-03],\n",
              "       [-3.2893062e-05,  2.6786507e-05,  5.6232640e-04, ...,\n",
              "         3.3581476e-03, -1.6645045e-03, -1.8742131e-03],\n",
              "       [-4.6384879e-03, -1.9680921e-03,  2.5938142e-03, ...,\n",
              "        -6.6451193e-04, -2.3227520e-03,  5.3908327e-04]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# ÏñºÎßàÎÇò Í∞ÄÍπåÏö¥ÏßÄ, ÏñºÎßàÎÇò Ïú†ÏÇ¨ÌïúÏßÄ ÌåêÎã®ÌïòÍ∏∞ ÏúÑÌï¥ Ï†ïÍ∑úÌôîÎ•º ÏßÑÌñâ Ìï¥Ï§òÏïº Ìï®\n",
        "embeddings = np.array(embeddings / np.linalg.norm(embeddings))\n",
        "# vector Ï†ïÍ∑úÌôî : ÏÉÅÎåÄÏ†ÅÏù∏ Í∞íÏùÑ Ïú†ÏßÄÌïòÎ©∞ ÌÅ¨Í∏∞Î•º Ï°∞Ï†ïÌïòÏó¨ Ïú†ÏÇ¨ÎèÑ, Í±∞Î¶¨ Îì±ÏùÑ ÎπÑÍµêÌïòÍ∏∞ ÏúÑÌï¥\n",
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converted_values = list(map(np.float32, embeddings)) # np.float32 ÌòïÌÉúÎ°ú Î≥ÄÌôò Ìï¥Ï§ò\n",
        "converted_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXgh154_O2TY",
        "outputId": "2edfdeaa-f8a2-46c7-a3ed-d52dfcd0627f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-3.2433148e-03,  3.0656536e-03, -2.4589787e-03, ...,\n",
              "        -7.9131096e-05, -5.9362175e-04,  5.5376114e-03], dtype=float32),\n",
              " array([ 0.00236633,  0.00150796, -0.00249147, ..., -0.0005354 ,\n",
              "        -0.00366863,  0.00203071], dtype=float32),\n",
              " array([ 1.0944124e-03,  3.0741558e-05,  2.8103061e-03, ...,\n",
              "        -1.1119934e-03, -3.6409216e-03,  5.9099812e-03], dtype=float32),\n",
              " array([ 1.4564579e-03,  4.7587943e-03, -1.1864128e-03, ...,\n",
              "         4.6959500e-05, -3.8631826e-03,  4.2156610e-03], dtype=float32),\n",
              " array([-0.00234334,  0.00301518, -0.00062454, ..., -0.00045515,\n",
              "        -0.00390585,  0.00814049], dtype=float32),\n",
              " array([ 0.00089627,  0.00172723, -0.00284757, ..., -0.00452556,\n",
              "        -0.00457108,  0.00146858], dtype=float32),\n",
              " array([ 0.00058679,  0.00072843,  0.00045178, ..., -0.00290517,\n",
              "        -0.00711539,  0.00377408], dtype=float32),\n",
              " array([ 0.00090235, -0.00144551,  0.003178  , ..., -0.00347104,\n",
              "        -0.00448765, -0.00245557], dtype=float32),\n",
              " array([-0.00286663, -0.00056181,  0.00069572, ...,  0.00428381,\n",
              "        -0.0041407 ,  0.00419675], dtype=float32),\n",
              " array([-0.00170382,  0.0008716 ,  0.00045565, ...,  0.00068737,\n",
              "        -0.00353624,  0.00341763], dtype=float32),\n",
              " array([-0.00033992,  0.00338333, -0.00052054, ..., -0.00210664,\n",
              "        -0.00246633, -0.00181073], dtype=float32),\n",
              " array([ 0.00090208,  0.00155809,  0.00030557, ...,  0.00289384,\n",
              "        -0.00221624,  0.00016802], dtype=float32),\n",
              " array([ 0.0037785 ,  0.00504247, -0.00458908, ...,  0.00396034,\n",
              "        -0.00822092,  0.00745772], dtype=float32),\n",
              " array([ 0.00017619,  0.00475262, -0.00318038, ...,  0.00038712,\n",
              "         0.00046006,  0.00323881], dtype=float32),\n",
              " array([ 0.00536634,  0.0040162 , -0.00550269, ...,  0.0026647 ,\n",
              "        -0.00344907,  0.00365885], dtype=float32),\n",
              " array([ 0.00590479,  0.00457241, -0.00010731, ...,  0.00132502,\n",
              "        -0.00327376, -0.00135045], dtype=float32),\n",
              " array([ 5.1231761e-03,  4.6478650e-03, -2.1065034e-05, ...,\n",
              "        -1.9535355e-03,  1.9883150e-03,  1.9978567e-03], dtype=float32),\n",
              " array([ 0.0010511 ,  0.00061121, -0.00297844, ..., -0.0002659 ,\n",
              "        -0.00096913, -0.00275768], dtype=float32),\n",
              " array([-0.00063501, -0.00190928, -0.00156237, ..., -0.0003695 ,\n",
              "        -0.00055172, -0.00120339], dtype=float32),\n",
              " array([ 0.00532599,  0.00670937, -0.00151724, ...,  0.00244851,\n",
              "        -0.00421873,  0.00162094], dtype=float32),\n",
              " array([ 1.1536278e-03,  2.4134165e-03, -1.1857501e-03, ...,\n",
              "        -1.9944129e-03, -3.3590184e-03,  4.2802971e-05], dtype=float32),\n",
              " array([-0.00102228,  0.0026736 ,  0.00101624, ...,  0.00076339,\n",
              "        -0.00490934,  0.00680752], dtype=float32),\n",
              " array([-0.00316147,  0.00308819,  0.00030403, ..., -0.00121894,\n",
              "        -0.00406617,  0.00664632], dtype=float32),\n",
              " array([ 0.00010854,  0.00150665,  0.00056457, ..., -0.00267947,\n",
              "        -0.00217572,  0.00038373], dtype=float32),\n",
              " array([ 0.00136343,  0.00347556,  0.00267971, ..., -0.00244323,\n",
              "        -0.00236043,  0.00465025], dtype=float32),\n",
              " array([-0.00205563,  0.00042378,  0.00341684, ..., -0.00472107,\n",
              "         0.0005803 , -0.00064954], dtype=float32),\n",
              " array([ 0.0004355 ,  0.00307822,  0.00379696, ..., -0.0017164 ,\n",
              "        -0.00272473,  0.00509586], dtype=float32),\n",
              " array([-0.00108326, -0.00061473,  0.00206593, ..., -0.00446715,\n",
              "        -0.00097362, -0.00212472], dtype=float32),\n",
              " array([-0.00178407,  0.00075584,  0.00523119, ..., -0.00324854,\n",
              "         0.00066812,  0.00461642], dtype=float32),\n",
              " array([-0.0007447 ,  0.00361824,  0.00026924, ..., -0.00280029,\n",
              "        -0.00171983,  0.00405306], dtype=float32),\n",
              " array([ 0.00250481,  0.00179906, -0.00267613, ...,  0.00233764,\n",
              "        -0.00527784,  0.00457851], dtype=float32),\n",
              " array([ 4.4189771e-03,  1.6580053e-03, -6.6356123e-03, ...,\n",
              "         3.5354567e-03, -7.8449018e-05,  6.3076284e-04], dtype=float32),\n",
              " array([ 0.00368375,  0.0039462 , -0.00474868, ...,  0.00108517,\n",
              "        -0.00269073,  0.00476531], dtype=float32),\n",
              " array([ 0.00101565,  0.0045815 , -0.00540525, ...,  0.00198672,\n",
              "        -0.00260209,  0.00172382], dtype=float32),\n",
              " array([ 0.00716784,  0.00515065, -0.00558711, ...,  0.00128413,\n",
              "        -0.00228453,  0.00302942], dtype=float32),\n",
              " array([ 0.00235073,  0.00250033, -0.00528111, ...,  0.00016123,\n",
              "         0.00076741, -0.00019067], dtype=float32),\n",
              " array([ 3.4229760e-03, -5.9852580e-05, -6.2472746e-03, ...,\n",
              "         1.1645186e-03, -6.5876325e-03,  5.3777336e-04], dtype=float32),\n",
              " array([ 0.003505  ,  0.00784357, -0.00655244, ...,  0.00076426,\n",
              "        -0.00408672,  0.00624986], dtype=float32),\n",
              " array([ 0.00140754,  0.00071476, -0.00243148, ..., -0.00011373,\n",
              "        -0.0059534 ,  0.00721259], dtype=float32),\n",
              " array([ 0.00462404,  0.00308917, -0.00488838, ...,  0.00378616,\n",
              "        -0.00863215,  0.00846082], dtype=float32),\n",
              " array([ 0.00092361, -0.00499602,  0.00608224, ..., -0.00294426,\n",
              "         0.00092975,  0.0004321 ], dtype=float32),\n",
              " array([ 0.00428621,  0.00244717,  0.00575314, ...,  0.00169229,\n",
              "        -0.00457199,  0.00400438], dtype=float32),\n",
              " array([-0.00071132, -0.00219813,  0.00385765, ..., -0.00158455,\n",
              "         0.00098547,  0.00111446], dtype=float32),\n",
              " array([ 0.00219731,  0.00670665, -0.0016524 , ...,  0.00106445,\n",
              "        -0.00616707,  0.00635823], dtype=float32),\n",
              " array([ 0.00123736,  0.00028444,  0.0023472 , ...,  0.00058971,\n",
              "        -0.00207878, -0.00196852], dtype=float32),\n",
              " array([-0.00023591, -0.00268639,  0.00243654, ...,  0.00032815,\n",
              "        -0.00023122, -0.0054553 ], dtype=float32),\n",
              " array([ 0.00307935,  0.00268344, -0.00251925, ..., -0.00130099,\n",
              "        -0.00031635,  0.00498034], dtype=float32),\n",
              " array([ 0.00487575,  0.00235609, -0.00333384, ...,  0.00165933,\n",
              "        -0.00339294,  0.00285497], dtype=float32),\n",
              " array([-0.0054122 ,  0.00047968,  0.00410507, ...,  0.00020605,\n",
              "        -0.00195576, -0.00254822], dtype=float32),\n",
              " array([ 0.00102737,  0.00142694,  0.00340935, ...,  0.00207777,\n",
              "        -0.00208584,  0.00153582], dtype=float32),\n",
              " array([-3.2893062e-05,  2.6786507e-05,  5.6232640e-04, ...,\n",
              "         3.3581476e-03, -1.6645045e-03, -1.8742131e-03], dtype=float32),\n",
              " array([-0.00463849, -0.00196809,  0.00259381, ..., -0.00066451,\n",
              "        -0.00232275,  0.00053908], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_list = []\n",
        "for chunk, vector in zip(chunks,converted_values):\n",
        "  # chunk : ÌòÑÏû¨ ÌÖåÏä§Ìä∏ Ï°∞Í∞Å\n",
        "  # vector : Ìï¥Îãπ ÌÖåÏä§Ìä∏ Ï°∞Í∞ÅÏùò ÏûÑÎ≤†Îî© Î≤°ÌÑ∞\n",
        "  chunk_dict = {\n",
        "      'chunk' : chunk.page_content,\n",
        "      'source' : chunk.metadata.get('source',''), # ÏóÜÏúºÎ©¥ Îπà Î∞∞Ïó¥ Í∞í Ï†ÄÏû•\n",
        "      'vector' : vector\n",
        "  }\n",
        "  dict_list.append(chunk_dict)"
      ],
      "metadata": {
        "id": "1PNpB8MXPY2E"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQJW3N6HQ0q1",
        "outputId": "2c366ebb-50d6-4ae4-97ed-c13252ece658"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'chunk': '<!DOCTYPE html>\\n<html lang=\"en\"><head>\\n  <meta charset=\"utf-8\">\\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"><!-- Begin Jekyll SEO tag v2.8.0 -->\\n<title>vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention | vLLM Blog</title>\\n<meta name=\"generator\" content=\"Jekyll v4.3.3\" />\\n<meta property=\"og:title\" content=\"vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention\" />',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-3.2433148e-03,  3.0656536e-03, -2.4589787e-03, ...,\n",
              "         -7.9131096e-05, -5.9362175e-04,  5.5376114e-03], dtype=float32)},\n",
              " {'chunk': '<meta name=\"author\" content=\"Woosuk Kwon*, Zhuohan Li*, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica (* Equal Contribution)\" />\\n<meta property=\"og:locale\" content=\"en_US\" />\\n<meta name=\"description\" content=\"GitHub | Documentation | Paper\" />\\n<meta property=\"og:description\" content=\"GitHub | Documentation | Paper\" />\\n<meta property=\"og:site_name\" content=\"vLLM Blog\" />\\n<meta property=\"og:type\" content=\"article\" />',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00236633,  0.00150796, -0.00249147, ..., -0.0005354 ,\n",
              "         -0.00366863,  0.00203071], dtype=float32)},\n",
              " {'chunk': '<meta property=\"og:type\" content=\"article\" />\\n<meta property=\"article:published_time\" content=\"2023-06-20T00:00:00-07:00\" />\\n<meta name=\"twitter:card\" content=\"summary\" />\\n<meta property=\"twitter:title\" content=\"vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention\" />\\n<script type=\"application/ld+json\">',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 1.0944124e-03,  3.0741558e-05,  2.8103061e-03, ...,\n",
              "         -1.1119934e-03, -3.6409216e-03,  5.9099812e-03], dtype=float32)},\n",
              " {'chunk': '{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"author\":{\"@type\":\"Person\",\"name\":\"Woosuk Kwon*, Zhuohan Li*, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica (* Equal Contribution)\"},\"dateModified\":\"2023-06-20T00:00:00-07:00\",\"datePublished\":\"2023-06-20T00:00:00-07:00\",\"description\":\"GitHub | Documentation | Paper\",\"headline\":\"vLLM: Easy, Fast, and Cheap LLM Serving with',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 1.4564579e-03,  4.7587943e-03, -1.1864128e-03, ...,\n",
              "          4.6959500e-05, -3.8631826e-03,  4.2156610e-03], dtype=float32)},\n",
              " {'chunk': 'Easy, Fast, and Cheap LLM Serving with PagedAttention\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"/2023/06/20/vllm.html\"},\"url\":\"/2023/06/20/vllm.html\"}</script>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.00234334,  0.00301518, -0.00062454, ..., -0.00045515,\n",
              "         -0.00390585,  0.00814049], dtype=float32)},\n",
              " {'chunk': '<!-- End Jekyll SEO tag -->\\n<link rel=\"stylesheet\" href=\"/assets/css/style.css\"><link type=\"application/atom+xml\" rel=\"alternate\" href=\"/feed.xml\" title=\"vLLM Blog\" /><script async src=\"https://www.googletagmanager.com/gtag/js?id=G-9C5R3JR3QS\"></script>\\n<script>\\n  window[\\'ga-disable-G-9C5R3JR3QS\\'] = window.doNotTrack === \"1\" || navigator.doNotTrack === \"1\" || navigator.doNotTrack === \"yes\" || navigator.msDoNotTrack === \"1\";\\n  window.dataLayer = window.dataLayer || [];',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00089627,  0.00172723, -0.00284757, ..., -0.00452556,\n",
              "         -0.00457108,  0.00146858], dtype=float32)},\n",
              " {'chunk': \"window.dataLayer = window.dataLayer || [];\\n  function gtag(){window.dataLayer.push(arguments);}\\n  gtag('js', new Date());\",\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00058679,  0.00072843,  0.00045178, ..., -0.00290517,\n",
              "         -0.00711539,  0.00377408], dtype=float32)},\n",
              " {'chunk': 'gtag(\\'config\\', \\'G-9C5R3JR3QS\\');\\n</script>\\n\\n</head>\\n<body><header class=\"site-header\">',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00090235, -0.00144551,  0.003178  , ..., -0.00347104,\n",
              "         -0.00448765, -0.00245557], dtype=float32)},\n",
              " {'chunk': '<div class=\"wrapper\"><a class=\"site-title\" rel=\"author\" href=\"/\">vLLM Blog</a></div>\\n</header>\\n<main class=\"page-content\" aria-label=\"Content\">\\n      <div class=\"wrapper\">\\n        <article class=\"post h-entry\" itemscope itemtype=\"http://schema.org/BlogPosting\"><br><p align=\"center\"><picture><img src=\"/assets/logos/vllm-logo-text-light.png\" width=\"65%\"></picture></p><br><header class=\"post-header\">',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.00286663, -0.00056181,  0.00069572, ...,  0.00428381,\n",
              "         -0.0041407 ,  0.00419675], dtype=float32)},\n",
              " {'chunk': '<h1 class=\"post-title p-name\" itemprop=\"name headline\">vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention</h1>\\n    <p class=\"post-meta\"><time class=\"dt-published\" datetime=\"2023-06-20T00:00:00-07:00\" itemprop=\"datePublished\">\\n        Jun 20, 2023\\n      </time>‚Ä¢ \\n          <span itemprop=\"author\" itemscope itemtype=\"http://schema.org/Person\">',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.00170382,  0.0008716 ,  0.00045565, ...,  0.00068737,\n",
              "         -0.00353624,  0.00341763], dtype=float32)},\n",
              " {'chunk': '<span class=\"p-author h-card\" itemprop=\"name\">Woosuk Kwon*, Zhuohan Li*, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Yu, Joey Gonzalez, Hao Zhang, and Ion Stoica (* Equal Contribution)</span></span></p>\\n  </header>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.00033992,  0.00338333, -0.00052054, ..., -0.00210664,\n",
              "         -0.00246633, -0.00181073], dtype=float32)},\n",
              " {'chunk': '<div class=\"post-content e-content\" itemprop=\"articleBody\">\\n    <p align=\"center\" style=\"margin-top:-15px\">\\n<a href=\"https://github.com/vllm-project/vllm\"><b>GitHub</b></a> | <a href=\"https://vllm.readthedocs.io/en/latest/\"><b>Documentation</b></a> | <a href=\"https://arxiv.org/pdf/2309.06180.pdf\"><b>Paper</b></a>\\n</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00090208,  0.00155809,  0.00030557, ...,  0.00289384,\n",
              "         -0.00221624,  0.00016802], dtype=float32)},\n",
              " {'chunk': '<p>LLMs promise to fundamentally change how we use AI across all industries. However, actually serving these models is challenging and can be surprisingly slow even on expensive hardware. Today we are excited to introduce vLLM, an open-source library for fast LLM inference and serving. vLLM utilizes <strong>PagedAttention</strong>, our new attention algorithm that effectively manages attention keys and values. vLLM equipped with PagedAttention redefines the new state of the art in LLM serving: it delivers',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.0037785 ,  0.00504247, -0.00458908, ...,  0.00396034,\n",
              "         -0.00822092,  0.00745772], dtype=float32)},\n",
              " {'chunk': 'new state of the art in LLM serving: it delivers up to 24x higher throughput than HuggingFace Transformers, without requiring any model architecture changes.</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00017619,  0.00475262, -0.00318038, ...,  0.00038712,\n",
              "          0.00046006,  0.00323881], dtype=float32)},\n",
              " {'chunk': '<p>vLLM has been developed at UC Berkeley and deployed at <a href=\"https://chat.lmsys.org\">Chatbot Arena and Vicuna Demo</a> for the past two months. It is the core technology that makes LLM serving affordable even for a small research team like LMSYS with limited compute resources. Try out vLLM now with a single command at our <a href=\"https://github.com/vllm-project/vllm\">GitHub repository</a>.</p>\\n\\n<h3 id=\"beyond-state-of-the-art-performance\">Beyond State-of-the-art Performance</h3>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00536634,  0.0040162 , -0.00550269, ...,  0.0026647 ,\n",
              "         -0.00344907,  0.00365885], dtype=float32)},\n",
              " {'chunk': '<p>We compare the throughput of vLLM with <a href=\"https://huggingface.co/docs/transformers/main_classes/text_generation\">HuggingFace Transformers (HF)</a>, the most popular LLM library and <a href=\"https://github.com/huggingface/text-generation-inference\">HuggingFace Text Generation Inference (TGI)</a>, the previous state of the art. We evaluate in two settings: LLaMA-7B on an NVIDIA A10G GPU and LLaMA-13B on an NVIDIA A100 GPU (40GB). We sample the requests‚Äô input/output lengths from the ShareGPT',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00590479,  0.00457241, -0.00010731, ...,  0.00132502,\n",
              "         -0.00327376, -0.00135045], dtype=float32)},\n",
              " {'chunk': 'requests‚Äô input/output lengths from the ShareGPT dataset. In our experiments, vLLM achieves up to <strong>24x</strong> higher throughput compared to HF and up to <strong>3.5x</strong> higher throughput than TGI.</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 5.1231761e-03,  4.6478650e-03, -2.1065034e-05, ...,\n",
              "         -1.9535355e-03,  1.9883150e-03,  1.9978567e-03], dtype=float32)},\n",
              " {'chunk': '<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/perf_a100_n1_light.png\" width=\"45%\" />\\n</picture><picture>\\n<img src=\"/assets/figures/perf_a10g_n1_light.png\" width=\"45%\" />\\n</picture><br />\\nServing throughput when each request asks for <em> one output completion</em>. vLLM achieves 14x - 24x higher throughput than HF and 2.2x - 2.5x higher throughput than TGI.\\n</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.0010511 ,  0.00061121, -0.00297844, ..., -0.0002659 ,\n",
              "         -0.00096913, -0.00275768], dtype=float32)},\n",
              " {'chunk': '<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/perf_a100_n3_light.png\" width=\"45%\" />\\n</picture><picture>\\n<img src=\"/assets/figures/perf_a10g_n3_light.png\" width=\"45%\" />\\n</picture>\\n<br />Serving throughput when each request asks for <em>three parallel output completions</em>. vLLM achieves 8.5x - 15x higher throughput than HF and 3.3x - 3.5x higher throughput than TGI.\\n</p>\\n\\n<h3 id=\"the-secret-sauce-pagedattention\">The Secret Sauce: PagedAttention</h3>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.00063501, -0.00190928, -0.00156237, ..., -0.0003695 ,\n",
              "         -0.00055172, -0.00120339], dtype=float32)},\n",
              " {'chunk': '<p>In vLLM, we identify that the performance of LLM serving is bottlenecked by memory. In the autoregressive decoding process, all the input tokens to the LLM produce their attention key and value tensors, and these tensors are kept in GPU memory to generate next tokens. These cached key and value tensors are often referred to as KV cache. The KV cache is</p>\\n<ul>\\n  <li><em>Large:</em> Takes up to 1.7GB for a single sequence in LLaMA-13B.</li>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00532599,  0.00670937, -0.00151724, ...,  0.00244851,\n",
              "         -0.00421873,  0.00162094], dtype=float32)},\n",
              " {'chunk': '<li><em>Dynamic:</em> Its size depends on the sequence length, which is highly variable and unpredictable.\\nAs a result, efficiently managing the KV cache presents a significant challenge. We find that existing systems waste <strong>60% ‚Äì 80%</strong> of memory due to fragmentation and over-reservation.</li>\\n</ul>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 1.1536278e-03,  2.4134165e-03, -1.1857501e-03, ...,\n",
              "         -1.9944129e-03, -3.3590184e-03,  4.2802971e-05], dtype=float32)},\n",
              " {'chunk': '<p>To address this problem, we introduce <strong>PagedAttention</strong>, an attention algorithm inspired by the classic idea of virtual memory and paging in operating systems. Unlike the traditional attention algorithms, PagedAttention allows storing continuous keys and values in non-contiguous memory space. Specifically, PagedAttention partitions the KV cache of each sequence into blocks, each block containing the keys and values for a fixed number of tokens. During the attention computation, the',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.00102228,  0.0026736 ,  0.00101624, ...,  0.00076339,\n",
              "         -0.00490934,  0.00680752], dtype=float32)},\n",
              " {'chunk': 'of tokens. During the attention computation, the PagedAttention kernel identifies and fetches these blocks efficiently.</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.00316147,  0.00308819,  0.00030403, ..., -0.00121894,\n",
              "         -0.00406617,  0.00664632], dtype=float32)},\n",
              " {'chunk': '<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/annimation0.gif\" width=\"80%\" />\\n</picture>\\n<br />\\n<em>PagedAttention:</em> KV Cache are partitioned into blocks. Blocks do not need to be contiguous in memory space.\\n</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00010854,  0.00150665,  0.00056457, ..., -0.00267947,\n",
              "         -0.00217572,  0.00038373], dtype=float32)},\n",
              " {'chunk': '<p>Because the blocks do not need to be contiguous in memory, we can manage the keys and values in a more flexible way as in OS‚Äôs virtual memory: one can think of blocks as pages, tokens as bytes, and sequences as processes. The contiguous <em>logical blocks</em> of a sequence are mapped to non-contiguous <em>physical blocks</em> via a block table. The physical blocks are allocated on demand as new tokens are generated.</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00136343,  0.00347556,  0.00267971, ..., -0.00244323,\n",
              "         -0.00236043,  0.00465025], dtype=float32)},\n",
              " {'chunk': '<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/annimation1.gif\" width=\"100%\" />\\n</picture>\\n<br />\\nExample generation process for a request with PagedAttention.\\n</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.00205563,  0.00042378,  0.00341684, ..., -0.00472107,\n",
              "          0.0005803 , -0.00064954], dtype=float32)},\n",
              " {'chunk': '<p>In PagedAttention, memory waste only happens in the last block of a sequence. In practice, this results in near-optimal memory usage, with a mere waste of under 4%. This boost in memory efficiency proves highly beneficial: It allows the system to batch more sequences together, increase GPU utilization, and thereby significantly increase the throughput as shown in the performance result above.</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.0004355 ,  0.00307822,  0.00379696, ..., -0.0017164 ,\n",
              "         -0.00272473,  0.00509586], dtype=float32)},\n",
              " {'chunk': '<p>PagedAttention has another key advantage: efficient memory sharing. For example, in <em>parallel sampling</em>, multiple output sequences are generated from the same prompt. In this case, the computation and memory for the prompt can be shared between the output sequences.</p>\\n\\n<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/annimation2.gif\" width=\"80%\" />\\n</picture>\\n<br />\\nExample of parallel sampling.\\n</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.00108326, -0.00061473,  0.00206593, ..., -0.00446715,\n",
              "         -0.00097362, -0.00212472], dtype=float32)},\n",
              " {'chunk': '<p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.00178407,  0.00075584,  0.00523119, ..., -0.00324854,\n",
              "          0.00066812,  0.00461642], dtype=float32)},\n",
              " {'chunk': '<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/annimation3.gif\" width=\"100%\" />\\n</picture>\\n<br />\\nExample generation process for a request that samples multiple outputs.\\n</p>\\n\\n<p>PageAttention‚Äôs memory sharing greatly reduces the memory overhead of complex sampling algorithms, such as parallel sampling and beam search, cutting their memory usage by up to 55%. This can translate into up to 2.2x improvement in throughput. This makes such sampling methods practical in LLM services.</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.0007447 ,  0.00361824,  0.00026924, ..., -0.00280029,\n",
              "         -0.00171983,  0.00405306], dtype=float32)},\n",
              " {'chunk': '<p>PagedAttention is the core technology behind vLLM, our LLM inference and serving engine that supports a variety of models with high performance and an easy-to-use interface. For more technical details about vLLM and PagedAttention, check out our <a href=\"https://github.com/vllm-project/vllm\">GitHub repo</a> and stay tuned for our paper.</p>\\n\\n<h3 id=\"the-silent-hero-behind-lmsys-vicuna-and-chatbot-arena\">The Silent Hero Behind LMSYS Vicuna and Chatbot Arena</h3>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00250481,  0.00179906, -0.00267613, ...,  0.00233764,\n",
              "         -0.00527784,  0.00457851], dtype=float32)},\n",
              " {'chunk': '<p>This April, <a href=\"https://lmsys.org\">LMSYS</a> developed the popular Vicuna chatbot models and made them publicly available. Since then, Vicuna has been served in <a href=\"https://arena.lmsys.org/\">Chatbot Arena</a> for millions of users. Initially, LMSYS FastChat adopted a HF Transformers based <a href=\"https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/model_worker.py\">serving backend</a> to serve the chat demo. As the demo became more popular, the peak traffic ramped up several times,',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 4.4189771e-03,  1.6580053e-03, -6.6356123e-03, ...,\n",
              "          3.5354567e-03, -7.8449018e-05,  6.3076284e-04], dtype=float32)},\n",
              " {'chunk': 'popular, the peak traffic ramped up several times, making the HF backend a significant bottleneck. The LMSYS and vLLM team have worked together and soon developed the FastChat-vLLM integration to use vLLM <a href=\"https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/vllm_worker.py\">as the new backend</a> in order to support the growing demands (up to 5x more traffic). In an early <a href=\"https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/test_throughput.py\">internal micro-benchmark</a> by',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00368375,  0.0039462 , -0.00474868, ...,  0.00108517,\n",
              "         -0.00269073,  0.00476531], dtype=float32)},\n",
              " {'chunk': 'micro-benchmark</a> by LMSYS, the vLLM serving backend can <strong>achieve up to 30x higher throughput than an initial HF backend.</strong></p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00101565,  0.0045815 , -0.00540525, ...,  0.00198672,\n",
              "         -0.00260209,  0.00172382], dtype=float32)},\n",
              " {'chunk': '<p>Since mid-April, the most popular models such as Vicuna, Koala, and LLaMA, have all been successfully served using the FastChat-vLLM integration ‚Äì With FastChat as the multi-model chat serving frontend and vLLM as the inference backend, LMSYS is able to harness a limited number of university-sponsored GPUs to serve Vicuna to millions of users with <em>high throughput</em> and <em>low latency</em>. LMSYS is expanding the use of vLLM to a wider range of models, including Databricks Dolly, LAION‚Äôs',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00716784,  0.00515065, -0.00558711, ...,  0.00128413,\n",
              "         -0.00228453,  0.00302942], dtype=float32)},\n",
              " {'chunk': 'of models, including Databricks Dolly, LAION‚Äôs OpenAsssiant, and Stability AI‚Äôs stableLM. The <a href=\"https://vllm.readthedocs.io/en/latest/models/supported_models.html\">support for more models</a> is being developed and forthcoming.</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00235073,  0.00250033, -0.00528111, ...,  0.00016123,\n",
              "          0.00076741, -0.00019067], dtype=float32)},\n",
              " {'chunk': '<p align=\"center\">\\n<picture>\\n<img src=\"/assets/figures/lmsys_traffic.png\" width=\"100%\" />\\n</picture>\\n<br />\\nRequests served by FastChat-vLLM integration in the Chatbot Arena between April to May. Indeed, more than half of the requests to Chatbot Arena use vLLM as the inference backend.\\n</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 3.4229760e-03, -5.9852580e-05, -6.2472746e-03, ...,\n",
              "          1.1645186e-03, -6.5876325e-03,  5.3777336e-04], dtype=float32)},\n",
              " {'chunk': '<p>This utilization of vLLM has also significantly reduced operational costs. With vLLM, LMSYS was able to cut the number of GPUs used for serving the above traffic by 50%. vLLM has been handling an average of 30K requests daily and a peak of 60K, which is a clear demonstration of vLLM‚Äôs robustness.</p>\\n\\n<h3 id=\"get-started-with-vllm\">Get started with vLLM</h3>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.003505  ,  0.00784357, -0.00655244, ...,  0.00076426,\n",
              "         -0.00408672,  0.00624986], dtype=float32)},\n",
              " {'chunk': '<p>Install vLLM with the following command (check out our <a href=\"https://vllm.readthedocs.io/en/latest/getting_started/installation.html\">installation guide</a> for more):</p>\\n\\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>pip <span class=\"nb\">install </span>vllm\\n</code></pre></div></div>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00140754,  0.00071476, -0.00243148, ..., -0.00011373,\n",
              "         -0.0059534 ,  0.00721259], dtype=float32)},\n",
              " {'chunk': '<p>vLLM can be used for both offline inference and online serving. To use vLLM for offline inference, you can import vLLM and use the <code class=\"language-plaintext highlighter-rouge\">LLM</code> class in your Python scripts:</p>\\n\\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"n\">vllm</span> <span class=\"kn\">import</span> <span class=\"n\">LLM</span>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00462404,  0.00308917, -0.00488838, ...,  0.00378616,\n",
              "         -0.00863215,  0.00846082], dtype=float32)},\n",
              " {'chunk': '<span class=\"n\">prompts</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">Hello, my name is</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">The capital of France is</span><span class=\"sh\">\"</span><span class=\"p\">]</span>  <span class=\"c1\"># Sample prompts.',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00092361, -0.00499602,  0.00608224, ..., -0.00294426,\n",
              "          0.00092975,  0.0004321 ], dtype=float32)},\n",
              " {'chunk': '</span><span class=\"n\">llm</span> <span class=\"o\">=</span> <span class=\"nc\">LLM</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">lmsys/vicuna-7b-v1.3</span><span class=\"sh\">\"</span><span class=\"p\">)</span>  <span class=\"c1\"># Create an LLM.',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00428621,  0.00244717,  0.00575314, ...,  0.00169229,\n",
              "         -0.00457199,  0.00400438], dtype=float32)},\n",
              " {'chunk': '</span><span class=\"n\">outputs</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"p\">.</span><span class=\"nf\">generate</span><span class=\"p\">(</span><span class=\"n\">prompts</span><span class=\"p\">)</span>  <span class=\"c1\"># Generate texts from the prompts.\\n</span></code></pre></div></div>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.00071132, -0.00219813,  0.00385765, ..., -0.00158455,\n",
              "          0.00098547,  0.00111446], dtype=float32)},\n",
              " {'chunk': '<p>To use vLLM for online serving, you can start an OpenAI API-compatible server via:</p>\\n\\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>python <span class=\"nt\">-m</span> vllm.entrypoints.openai.api_server <span class=\"nt\">--model</span> lmsys/vicuna-7b-v1.3\\n</code></pre></div></div>\\n\\n<p>You can query the server with the same format as OpenAI API:</p>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00219731,  0.00670665, -0.0016524 , ...,  0.00106445,\n",
              "         -0.00616707,  0.00635823], dtype=float32)},\n",
              " {'chunk': '<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>curl http://localhost:8000/v1/completions <span class=\"se\">\\\\</span>\\n    <span class=\"nt\">-H</span> <span class=\"s2\">\"Content-Type: application/json\"</span> <span class=\"se\">\\\\</span>\\n    <span class=\"nt\">-d</span> <span class=\"s1\">\\'{\\n        \"model\": \"lmsys/vicuna-7b-v1.3\",\\n        \"prompt\": \"San Francisco is a\",\\n        \"max_tokens\": 7,\\n        \"temperature\": 0\\n    }\\'</span>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00123736,  0.00028444,  0.0023472 , ...,  0.00058971,\n",
              "         -0.00207878, -0.00196852], dtype=float32)},\n",
              " {'chunk': '\"temperature\": 0\\n    }\\'</span>\\n</code></pre></div></div>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.00023591, -0.00268639,  0.00243654, ...,  0.00032815,\n",
              "         -0.00023122, -0.0054553 ], dtype=float32)},\n",
              " {'chunk': '<p>For more ways to use vLLM, please check out the <a href=\"https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html\">quickstart guide</a>.</p>\\n\\n<p><br /></p>\\n\\n<hr />',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00307935,  0.00268344, -0.00251925, ..., -0.00130099,\n",
              "         -0.00031635,  0.00498034], dtype=float32)},\n",
              " {'chunk': '<p><br /></p>\\n\\n<hr />\\n\\n<p><em>Blog written by Woosuk Kwon and Zhuohan Li (UC Berkeley). Special thanks to Hao Zhang for the integration of vLLM and FastChat and for writing the corresponding section. We thank the entire team\\u200a‚Äî\\u200aSiyuan Zhuang, Ying Sheng, Lianmin Zheng (UC Berkeley), Cody Yu (Independent Researcher), Joey Gonzalez (UC Berkeley), Hao Zhang (UC Berkeley &amp; UCSD), and Ion Stoica (UC Berkeley).</em></p>\\n\\n  </div><a class=\"u-url\" href=\"/2023/06/20/vllm.html\" hidden></a>\\n</article>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00487575,  0.00235609, -0.00333384, ...,  0.00165933,\n",
              "         -0.00339294,  0.00285497], dtype=float32)},\n",
              " {'chunk': '</div>\\n    </main><footer class=\"site-footer h-card\">\\n  <data class=\"u-url\" href=\"/\"></data>\\n\\n  <div class=\"wrapper\">',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.0054122 ,  0.00047968,  0.00410507, ...,  0.00020605,\n",
              "         -0.00195576, -0.00254822], dtype=float32)},\n",
              " {'chunk': '<div class=\"footer-col-wrapper\">\\n      <div class=\"footer-col\">\\n        <!-- <p class=\"feed-subscribe\">\\n          <a href=\"/feed.xml\">\\n            <svg class=\"svg-icon orange\">\\n              <use xlink:href=\"/assets/minima-social-icons.svg#rss\"></use>\\n            </svg><span>Subscribe</span>\\n          </a>\\n        </p> -->\\n        <ul class=\"contact-list\">\\n          <li class=\"p-name\">¬© 2024. vLLM Team. All rights reserved.</li>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([ 0.00102737,  0.00142694,  0.00340935, ...,  0.00207777,\n",
              "         -0.00208584,  0.00153582], dtype=float32)},\n",
              " {'chunk': '<li><a href=\"https://github.com/vllm-project/vllm\">https://github.com/vllm-project/vllm</a></li>\\n        </ul>\\n      </div>\\n      <div class=\"footer-col\">\\n        <p></p>\\n      </div>\\n    </div>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-3.2893062e-05,  2.6786507e-05,  5.6232640e-04, ...,\n",
              "          3.3581476e-03, -1.6645045e-03, -1.8742131e-03], dtype=float32)},\n",
              " {'chunk': '<div class=\"social-links\"><ul class=\"social-media-list\"></ul>\\n</div>\\n\\n  </div>\\n\\n</footer>\\n</body>\\n\\n</html>',\n",
              "  'source': 'https://blog.vllm.ai/2023/06/20/vllm.html',\n",
              "  'vector': array([-0.00463849, -0.00196809,  0.00259381, ..., -0.00066451,\n",
              "         -0.00232275,  0.00053908], dtype=float32)}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "chunks = [\n",
        "  Document(page_content = 'Ï≤´ Î≤àÏß∏ Î≥∏Î¨∏', metadata = {'source' : 'doc1.com'}),\n",
        "  Document(page_content = 'Îëê Î≤àÏß∏ Î≥∏Î¨∏', metadata = {'source' : 'doc2.com'})\n",
        "]\n",
        "\n",
        "converted_values = [\n",
        "  [0.1, 0.2, 0.3],\n",
        "  [0.4, 0.5, 0.6]\n",
        "]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "6b3re9HJRoG8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "{\n",
        "  'chunk' : 'Ï≤´ Î≤àÏß∏ Î≥∏Î¨∏',\n",
        "  'source' : 'doc1.com',\n",
        "  'vector' : [0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "YtXllwvmSagl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Uh1-_U7cdOg6"
      },
      "outputs": [],
      "source": [
        "# pip install pymilvus # Î≤°ÌÑ∞ ÎîîÎπÑÎ•º Ï†ÄÏû•ÌïòÎäî ÎîîÎπÑÏù¥Î¶Ñ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "so25T1Oc-ZIC"
      },
      "outputs": [],
      "source": [
        "from pymilvus import MilvusClient # Î¶¨ÎàÑÏä§ÌôòÍ≤ΩÏóêÏÑú ÏÇ¨Ïö© Í∞ÄÎä•!!\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czWdnd-vBDoy",
        "outputId": "4ce83a9d-a6f5-4f79-93c8-7721a4a101ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:pymilvus.milvus_client.milvus_client:Created new connection using: 3c0a6edc927145fa92cd4d6b04c3635c\n"
          ]
        }
      ],
      "source": [
        "mc = MilvusClient('milvus_test.db') # ÌòïÏãù milvus(ÏùºÎ∞òÏ†ÅÏúºÎ°ú Î∂ôÏûÑ)_Ïù¥Î¶Ñ.db"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collection_name = 'milvus_docs'\n",
        "# CRUD : Create, Read, Update, Delete"
      ],
      "metadata": {
        "id": "5JSW5DMzYLVt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_Oc3kbjBDoy",
        "outputId": "7a2e29ab-2c19-4d78-fe10-c0c3de7428ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:pymilvus.milvus_client.milvus_client:Successfully created collection: milvus_docs\n",
            "DEBUG:pymilvus.milvus_client.milvus_client:Successfully created an index on collection: milvus_docs\n"
          ]
        }
      ],
      "source": [
        "mc.create_collection(collection_name, # collection_name = 'milvus_docs'\n",
        "                     embedding_dim, # Î≤°ÌÑ∞Ïùò Ï∞®ÏõêÍ∞íÏùÑ Î∞òÌôò Î∞õÏùÄ Í∞í\n",
        "                     consustency_level = 'Eventually', # Îç∞Ïù¥ÌÑ∞Ïùò ÏùºÍ¥ÄÏÑ± ÏàòÏ§Ä(Ïã§ÏãúÍ∞ÑÏùò ÏµúÏã† Îç∞Ïù¥ÌÑ∞Î•º Ï≤òÎ¶¨Ìï¥Ïïº ÌïòÎäî ÏßÄÏùò Î¨∏Ï†ú)\n",
        "                     # ÏùºÎ∞òÏ†Å(Í≤ÄÏÉâ Îì±) : 'Eventually'\n",
        "                     # Í∞ïÌïòÍ≤å(Ï£ºÏãùÎì±) : Strong\n",
        "                     auto_id = True, # index ÏûêÎèô ÏÉùÏÑ±\n",
        "                     overwrite = True) # ÎçÆÏñ¥Ïì∞Í∏∞(Îç∞Ïù¥ÌÑ∞ ÏµúÏã†Ìôî)\n",
        "                     # overwirte : Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎäî Ïª¨Î†âÏÖòÏù¥ ÏûàÏúºÎ©¥ ÎçÆÏñ¥Ïì∞Í∏∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKfelQREBDoz",
        "outputId": "ff09f0f3-0534-4c4b-cee6-c342187ad752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Îç∞Ïù¥ÌÑ∞ ÏÇΩÏûÖ ÏãúÏûë!!!\n",
            "ÏÇΩÏûÖÎêú Î≤°ÌÑ∞(Îç∞Ïù¥ÌÑ∞)Ïùò Í∞úÏàò : 52ÏÇΩÏûÖÏóê Í±∏Î¶∞ ÏãúÍ∞Ñ : 0.03 Ï¥à\n"
          ]
        }
      ],
      "source": [
        "print('Îç∞Ïù¥ÌÑ∞ ÏÇΩÏûÖ ÏãúÏûë!!!')\n",
        "start_time = time.time()\n",
        "\n",
        "mc.insert(\n",
        "    collection_name,\n",
        "    data = dict_list,\n",
        "    progress_bar = True\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f'ÏÇΩÏûÖÎêú Î≤°ÌÑ∞(Îç∞Ïù¥ÌÑ∞)Ïùò Í∞úÏàò : {len(dict_list)}', end='')\n",
        "print(f'ÏÇΩÏûÖÏóê Í±∏Î¶∞ ÏãúÍ∞Ñ : {round(end_time - start_time, 2)} Ï¥à')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "L26IgSMIBDoz"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ywjdeBZeBDoz"
      },
      "outputs": [],
      "source": [
        "sample_question = 'What is PagedAttention?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "90JHeaEoBDoz"
      },
      "outputs": [],
      "source": [
        "query_embeddings = torch.tensor(encoder.encode([sample_question]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNrjatmxBDoz",
        "outputId": "de3398dc-b7e1-48ee-e7c1-92fa05541326"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0537, -0.0164,  0.0005,  ..., -0.0165, -0.0059,  0.0475]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "query_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcbkhNqvBDoz",
        "outputId": "a6e72bc6-314b-41d0-acb4-85d5974e001e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0537, -0.0164,  0.0005,  ..., -0.0165, -0.0059,  0.0475]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "query_embddings = F.normalize(query_embeddings, p = 2, dim = 1)  # ÌïòÎÇòÏùò ÌñâÏùÑ Ï†ïÍ∑úÌôî Ìï†Îïå ÏÇ¨Ïö©Í∞ÄÎä•\n",
        "# p : Ï†ïÍ∑úÌôî Ìï†Îïå ÏÇ¨Ïö©Ìï† Î≤°ÌÑ∞Ïùò Í∏∏Ïù¥\n",
        "# dim : Ï∞®Ïõê(Ï∂ï)ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Ìï†ÏßÄ Í≤∞Ï†ï\n",
        "query_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-3eMvaoBDoz",
        "outputId": "d41bd912-b9e6-40d3-d86b-915bbc08d89c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-0.0537494 , -0.01643682,  0.00046268, ..., -0.01646634,\n",
              "        -0.00591078,  0.04753314], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "query_embeddings = list(map(np.float32, query_embeddings)) # array ÌòïÌÉúÎ°ú Ï†ÄÏû•\n",
        "query_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFF1NSkPBDoz",
        "outputId": "f7e1b80c-f2ac-49a2-dec0-7d27460e5661"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chunk', 'source', 'vector']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "output_fields = list(dict_list[0].keys())\n",
        "output_fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lA-ovuGhBDoz"
      },
      "outputs": [],
      "source": [
        "# vector Í∞íÏùÄ ÌïÑÏöî ÏóÜÍ∏∞ ÎïåÎ¨∏Ïóê Ï†úÍ±∞\n",
        "output_fields.remove('vector')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "an8AfujnBDoz"
      },
      "outputs": [],
      "source": [
        "top_k = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "OnCaLKiJBDoz"
      },
      "outputs": [],
      "source": [
        "result = mc.search(\n",
        "    collection_name,\n",
        "    data = query_embeddings,\n",
        "    output_fields = output_fields,\n",
        "    limit = top_k,\n",
        "    consistency_level = 'Eventually'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqjhaYX0fh07",
        "outputId": "caabe775-9860-426c-d519-ac239d17a6c0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "data: ['[{\\'id\\': 454237455411314716, \\'distance\\': 0.780035138130188, \\'entity\\': {\\'chunk\\': \\'<p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p>\\', \\'source\\': \\'https://blog.vllm.ai/2023/06/20/vllm.html\\'}}, {\\'id\\': 454236856952553604, \\'distance\\': 0.780035138130188, \\'entity\\': {\\'chunk\\': \\'<p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p>\\', \\'source\\': \\'https://blog.vllm.ai/2023/06/20/vllm.html\\'}}, {\\'id\\': 454236601796526108, \\'distance\\': 0.780035138130188, \\'entity\\': {\\'chunk\\': \\'<p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p>\\', \\'source\\': \\'https://blog.vllm.ai/2023/06/20/vllm.html\\'}}, {\\'id\\': 454236856952553606, \\'distance\\': 0.7609796524047852, \\'entity\\': {\\'chunk\\': \\'<p>PagedAttention is the core technology behind vLLM, our LLM inference and serving engine that supports a variety of models with high performance and an easy-to-use interface. For more technical details about vLLM and PagedAttention, check out our <a href=\"https://github.com/vllm-project/vllm\">GitHub repo</a> and stay tuned for our paper.</p>\\\\n\\\\n<h3 id=\"the-silent-hero-behind-lmsys-vicuna-and-chatbot-arena\">The Silent Hero Behind LMSYS Vicuna and Chatbot Arena</h3>\\', \\'source\\': \\'https://blog.vllm.ai/2023/06/20/vllm.html\\'}}]'] "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "kUC0K5WnBDo0",
        "outputId": "f565ad42-1099-4f59-9264-a6dd4d8c03e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "result[0][0]['entity']['chunk']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "UTsASV7xBDo0",
        "outputId": "1da541ae-56b5-46b4-bd09-82e567878677"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "result[0][1]['entity']['chunk']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[0][2]['entity']['chunk']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "omqhTRVjf1eV",
        "outputId": "fbb14398-1111-4a8b-ba67-029b8350a0d6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[0][3]['entity']['chunk']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Hy5VlhA-f1O6",
        "outputId": "1c7b32a5-7d0b-4c5c-f06c-f7c69dcca7f7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<p>PagedAttention is the core technology behind vLLM, our LLM inference and serving engine that supports a variety of models with high performance and an easy-to-use interface. For more technical details about vLLM and PagedAttention, check out our <a href=\"https://github.com/vllm-project/vllm\">GitHub repo</a> and stay tuned for our paper.</p>\\n\\n<h3 id=\"the-silent-hero-behind-lmsys-vicuna-and-chatbot-arena\">The Silent Hero Behind LMSYS Vicuna and Chatbot Arena</h3>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Í∞ÄÏû• Ïú†ÏÇ¨ÎèÑ ÎÜíÏùÄ ÌïòÎÇòÏùò Îã®Ïùº chunkÎßå Ï∂îÍ∞ÄÌïòÎäî Í≤ΩÏö∞\n",
        "# contexts = []\n",
        "# # Í≤ÄÏÉâÏùÑ Ìï¥ÏÑú Ï∞æÏïÑÏò®Í±∞ÏßÄ Î™®Îç∏Ïù¥ ÎåÄÎãµÌïúÍ≤å ÏïÑÎãàÍ∏∞ ÎïåÎ¨∏Ïóê..\n",
        "# contexts.append(result[0][0]['entity']['chunk'])"
      ],
      "metadata": {
        "id": "QYpBUia9mb4k"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contexts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmDIF7M4mzc5",
        "outputId": "23f8414c-2f10-45d9-839b-e8106ee8d346"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p>']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# top_k Ïùò chunkÎ•º Î™®Îëê Ï∂îÍ∞ÄÌïòÎäî Í≤ΩÏö∞\n",
        "contexts = []\n",
        "for i in range(top_k):\n",
        "  contexts.append(result[0][i]['entity']['chunk'])\n",
        "contexts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFvbHmiQnJ_N",
        "outputId": "ff242a99-712b-46ff-c9d7-007c33db6374"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p>',\n",
              " '<p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p>',\n",
              " '<p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p>',\n",
              " '<p>PagedAttention is the core technology behind vLLM, our LLM inference and serving engine that supports a variety of models with high performance and an easy-to-use interface. For more technical details about vLLM and PagedAttention, check out our <a href=\"https://github.com/vllm-project/vllm\">GitHub repo</a> and stay tuned for our paper.</p>\\n\\n<h3 id=\"the-silent-hero-behind-lmsys-vicuna-and-chatbot-arena\">The Silent Hero Behind LMSYS Vicuna and Chatbot Arena</h3>']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contexts_combined = ' '.join(contexts)\n",
        "contexts_combined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "13n-VsNDnEAf",
        "outputId": "b97794d4-f533-4e19-de96-39f46ac23c0d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p> <p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p> <p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p> <p>PagedAttention is the core technology behind vLLM, our LLM inference and serving engine that supports a variety of models with high performance and an easy-to-use interface. For more technical details about vLLM and PagedAttention, check out our <a href=\"https://github.com/vllm-project/vllm\">GitHub repo</a> and stay tuned for our paper.</p>\\n\\n<h3 id=\"the-silent-hero-behind-lmsys-vicuna-and-chatbot-arena\">The Silent Hero Behind LMSYS Vicuna and Chatbot Arena</h3>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"Ïú†ÏÇ¨ÎèÑÍ∞Ä ÎÜíÍ≤å ÎÇòÏò® Context : {contexts_combined}\"\"\"\n",
        "prompts = [system_prompt]\n",
        "prompts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV5BNKhIn5qo",
        "outputId": "57c493dd-01bf-41d4-9ff6-6f96852ae311"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ïú†ÏÇ¨ÎèÑÍ∞Ä ÎÜíÍ≤å ÎÇòÏò® Context : <p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p> <p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p> <p>PagedAttention naturally enables memory sharing through its block table. Similar to how processes share physical pages, different sequences in PagedAttention can share the blocks by mapping their logical blocks to the same physical block. To ensure safe sharing, PagedAttention keeps track of the reference counts of the physical blocks and implements the <em>Copy-on-Write</em> mechanism.</p> <p>PagedAttention is the core technology behind vLLM, our LLM inference and serving engine that supports a variety of models with high performance and an easy-to-use interface. For more technical details about vLLM and PagedAttention, check out our <a href=\"https://github.com/vllm-project/vllm\">GitHub repo</a> and stay tuned for our paper.</p>\\n\\n<h3 id=\"the-silent-hero-behind-lmsys-vicuna-and-chatbot-arena\">The Silent Hero Behind LMSYS Vicuna and Chatbot Arena</h3>']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "c_p28xiif4mn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e5de9e60-8f5a-4414-8994-9b6bbfbd534f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HFhf_CZGDjjpoyCySVNIEpvkqOSnHnqwlbsWjPT\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "hf_token = input('HF')\n",
        "login(token = hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "KusWdIC2f55r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc06d53b-d2b6-4e1e-d730-c0fb9e87f604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vllm in /usr/local/lib/python3.10/dist-packages (0.6.4.post1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from vllm) (4.66.6)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.45.2 in /usr/local/lib/python3.10/dist-packages (from vllm) (4.46.2)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.20.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from vllm) (4.25.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from vllm) (3.11.2)\n",
            "Requirement already satisfied: openai>=1.45.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (1.54.4)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.10/dist-packages (from vllm) (0.32.1)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.9.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from vllm) (10.4.0)\n",
            "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.21.0)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (7.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.7.0)\n",
            "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.9 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.10.9)\n",
            "Requirement already satisfied: outlines<0.1,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.0.46)\n",
            "Requirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.10/dist-packages (from vllm) (4.12.2)\n",
            "Requirement already satisfied: filelock>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from vllm) (3.16.1)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.10/dist-packages (from vllm) (0.2.1.1.post4)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from vllm) (24.0.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.10/dist-packages (from vllm) (0.18.6)\n",
            "Requirement already satisfied: gguf==0.10.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.10.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from vllm) (8.5.0)\n",
            "Requirement already satisfied: mistral-common>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from mistral-common[opencv]>=1.5.0->vllm) (1.5.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from vllm) (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from vllm) (0.8.0)\n",
            "Requirement already satisfied: compressed-tensors==0.8.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.8.0)\n",
            "Requirement already satisfied: ray>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.39.0)\n",
            "Requirement already satisfied: nvidia-ml-py>=12.560.30 in /usr/local/lib/python3.10/dist-packages (from vllm) (12.560.30)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision==0.20.1 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.20.1+cu121)\n",
            "Requirement already satisfied: xformers==0.0.28.post3 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.0.28.post3)\n",
            "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.115.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->vllm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->vllm) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->vllm) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->vllm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->vllm) (1.3.0)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.41.3)\n",
            "Requirement already satisfied: interegular>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer<0.11,>=0.10.9->vllm) (0.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer<0.11,>=0.10.9->vllm) (24.2)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from mistral-common>=1.5.0->mistral-common[opencv]>=1.5.0->vllm) (4.23.0)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mistral-common[opencv]>=1.5.0->vllm) (4.10.0.84)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.45.0->vllm) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.45.0->vllm) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.45.0->vllm) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.45.0->vllm) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.45.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: lark in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (1.2.2)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (1.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (3.1.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (5.6.3)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (0.60.0)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (0.35.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (3.1.0)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (24.6.1)\n",
            "Requirement already satisfied: pyairports in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (2.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm) (2.23.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (8.1.7)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.1.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm) (2024.8.30)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.6.0->vllm) (2024.9.11)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.19.1->vllm) (0.26.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.45.2->vllm) (0.4.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (2.4.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (24.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (4.0.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->vllm) (3.21.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (1.0.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (14.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.45.0->vllm) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.45.0->vllm) (1.0.7)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.5.0->mistral-common[opencv]>=1.5.0->vllm) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.5.0->mistral-common[opencv]>=1.5.0->vllm) (0.21.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (0.70.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->vllm) (3.0.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->outlines<0.1,>=0.0.43->vllm) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->outlines<0.1,>=0.0.43->vllm) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# pip install vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "RhmpopeLgN_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "857ba99e-0fcf-4e7d-f458-b4e33cd4947a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "Successfully installed triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "# pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "rJreji_Rf7dg"
      },
      "outputs": [],
      "source": [
        "import vllm, torch\n",
        "from vllm import LLM, SamplingParams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "an_oYyBDgBzl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "model_torun = \"meta-llama/Llama-3.2-1B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU Î©îÎ™®Î¶¨ Ï∫êÏãú ÏßÄÏö∞Í∏∞\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "k9WojiHFrbAR"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(model = model_torun,\n",
        "          enforce_eager = True,\n",
        "          dtype = torch.float16,\n",
        "          gpu_memory_utilization = 0.9,\n",
        "          max_model_len = 1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500,
          "referenced_widgets": [
            "31f36e7f85804ebb97fba63eecc8e5ad",
            "a73b19498c3a4636af70fea9ae55436c",
            "910716d175e64104acf451d9195f8991",
            "4f592550f5224b7e8fd711dbe8060a29",
            "aa900d71b34f47748fede8e9b32f2e1a",
            "eddde34319ac41a5bae8c10c6f746eb8",
            "56316327df634965bfadcd4a5d408cc8",
            "66405af142ab494fa97bfe071143d7db",
            "f3b269797227454f8671eafbf70ff011",
            "79c48dbb95624549a8adf9be469d770b",
            "7f697cdcf4784bc1bf3beade0dafd1ff",
            "946a1781dc3449b1b9bad5710e5d9c97",
            "dd8b2fdb65f7466fb823d91ab6b0a6c2",
            "19ceb78becad499cb8a08bd789bc32b0",
            "2c4b4c1327c7470ba174f54ea116d5e6",
            "574fb885edf4470191ba112087182bab",
            "6d975d2a2561449fb36411b85487e0f7",
            "4236f92e117f439dbc71d0831bc994d2",
            "4bd48e7af642429ab6f53bdc94e63ee3",
            "bb635452bec74cee942339e730a26653",
            "204e0f52c3eb419495c4c1ad05739138",
            "afe333679eda4e9aa44a6b5d9a5a36fc",
            "ee13cb0ac7d8497db6b7723483742205",
            "7f9ded113787403aa26d2b402f92a9cc",
            "ed994b3a362947ce87a965587b07502c",
            "5c0730c483c544b0b392f293048de01f",
            "7f64cc8cc3df4bd5a58ce5a5f711f021",
            "fbdb470d52014c7598577b5ffef59e2f",
            "75c84087c6d848eea6ee48fdc74888b3",
            "a921a219935a49c7af2a3f5f04a8c139",
            "4a1e4120d0ef4388869c110c9a514ad3",
            "8214d893c9194555a35cf3d2bffc37a5",
            "22f8bea967cd4ead9ab1de8e3cb53412",
            "21157fbf4617445d9fac9be92b84edfe",
            "c5b84cee5dec4432a44e959ae9d49a71",
            "8258a174f7ba478ea5a42aac892e6e6f",
            "c5af4c01ab2a4539922e6f18c8af9d9c",
            "7200dde8ad4c494e92c8146b3d176efc",
            "7e1b45e9212e407195ae7dca5d88c2ba",
            "50c8ab5109e44fbd8910f4bec89432a0",
            "cd33341029d04d3da2ec6cfaad56914e",
            "418c96a721204a9089908fd923a8d5a0",
            "5f25e10f0f764085a4a7efd424ea9b25",
            "686cfe75065c4accafc68619068886eb",
            "700f93263cc7459088a7667c70eeea50",
            "805d0d32e0ba4bbd87a72bd01e94a42c",
            "e973e0c6574343f0834f34248202c408",
            "840c9a5435dc469d8ec0391f16812595",
            "f4036280a0ed4f52a57bdb3583c86c80",
            "7fd2fc2bb3f84291a5a856263a6fe973",
            "682f860e6ffa4c26b31d7ccb5070c2a5",
            "eaf4c931dd3d4e63b4703e72fc9a606f",
            "bb93b904daff4564803bd8101b73b9b5",
            "01668123db954189ba4560f8847a99f7",
            "001e505cf2314291899367e1e1106977",
            "559a6d4b31db41ba91c4f6508ee7d697",
            "b4be18d72b1f4b6c9ee4155d9a510527",
            "1b7a9e1fb97e49ffa26efed26ce6cdf9",
            "f116331cb07741b3a2a00f4a2fb04d1b",
            "336a04b83355458b9ce125425cf68583",
            "368ca25513d747098701b7fa9b5baee0",
            "6622a943baf34c078e470f6875c46e54",
            "efb86e32f6904eecbecd17a49fd1147e",
            "9d2e4f931e7a4e208b6f0de18497875e",
            "a42cbc4c90094866ac7c076cb88fb128",
            "73cab685900f4362a341101132324ba9",
            "5ec6ed04f3f14b42b2db4f8fd6c58a7b",
            "f29650a290ec4bcb9c5eddf4d1caf74d",
            "69d8826c3b5043489ad4a45ee725947c",
            "5af4743706b94f6f812ca590e779bc72",
            "651f5a7eb7d54e2f9f7506f8d0573419",
            "901f2f87ed8f4acaa17118c2127636e0",
            "6a85207f1aed4d838d4cf03da6562dec",
            "ad0a9901db534ffbb45a30be9799a277",
            "f26b2896393d44d3954d97f93125d5a1",
            "2e198dbfb529420db4875ba894caf9c3",
            "7fa613c1860a41699d8a61ccb89602d8"
          ]
        },
        "id": "Wwkech4lra67",
        "outputId": "750c14ad-9bfe-4b6f-b1a1-71f2dfd7f74b"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31f36e7f85804ebb97fba63eecc8e5ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 11-28 07:34:21 config.py:1865] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 11-28 07:34:31 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
            "WARNING 11-28 07:34:31 config.py:503] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "INFO 11-28 07:34:31 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "946a1781dc3449b1b9bad5710e5d9c97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee13cb0ac7d8497db6b7723483742205"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21157fbf4617445d9fac9be92b84edfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "700f93263cc7459088a7667c70eeea50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-28 07:34:34 selector.py:261] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 11-28 07:34:34 selector.py:144] Using XFormers backend.\n",
            "INFO 11-28 07:34:35 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
            "INFO 11-28 07:34:35 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "559a6d4b31db41ba91c4f6508ee7d697"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-28 07:35:36 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ec6ed04f3f14b42b2db4f8fd6c58a7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 11-28 07:35:41 model_runner.py:1077] Loading model weights took 2.3185 GB\n",
            "INFO 11-28 07:35:41 worker.py:232] Memory profiling results: total_gpu_memory=14.75GiB initial_memory_usage=3.74GiB peak_torch_memory=4.75GiB memory_usage_post_profile=3.76GiB non_torch_memory=0.18GiB kv_cache_size=8.34GiB gpu_memory_utilization=0.90\n",
            "INFO 11-28 07:35:41 gpu_executor.py:113] # GPU blocks: 17090, # CPU blocks: 8192\n",
            "INFO 11-28 07:35:41 gpu_executor.py:117] Maximum concurrency for 1024 tokens per request: 267.03x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ER5XHRuqgWI6"
      },
      "outputs": [],
      "source": [
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(temperature = 0.2, top_p = 0.95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "mvhrogTuxQc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb77baa-241d-461c-ab57-2578debaef45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.62it/s, est. speed input: 969.37 toks/s, output: 42.37 toks/s]\n"
          ]
        }
      ],
      "source": [
        "outputs = llm.generate(prompts, sampling_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for output in outputs:\n",
        "  prompt = output.prompt\n",
        "  generated_text = output.outputs[0].text\n",
        "  print(f'ÏßàÎ¨∏ : {sample_question}')\n",
        "  print(f'ÏÉùÏÑ±Îêú Î™®Îç∏Ïùò ÏùëÎãµ : {generated_text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GujH1mLbs4Yu",
        "outputId": "e3eef298-e8dd-4a82-cef2-e2f0f724d440"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÏßàÎ¨∏ : What is PagedAttention?\n",
            "ÏÉùÏÑ±Îêú Î™®Îç∏Ïùò ÏùëÎãµ :  <p>Our LLM inference and serving engine, vLLM, is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LEpdV1EDtJ3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "31f36e7f85804ebb97fba63eecc8e5ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a73b19498c3a4636af70fea9ae55436c",
              "IPY_MODEL_910716d175e64104acf451d9195f8991",
              "IPY_MODEL_4f592550f5224b7e8fd711dbe8060a29"
            ],
            "layout": "IPY_MODEL_aa900d71b34f47748fede8e9b32f2e1a"
          }
        },
        "a73b19498c3a4636af70fea9ae55436c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eddde34319ac41a5bae8c10c6f746eb8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_56316327df634965bfadcd4a5d408cc8",
            "value": "config.json:‚Äá100%"
          }
        },
        "910716d175e64104acf451d9195f8991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66405af142ab494fa97bfe071143d7db",
            "max": 877,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3b269797227454f8671eafbf70ff011",
            "value": 877
          }
        },
        "4f592550f5224b7e8fd711dbe8060a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79c48dbb95624549a8adf9be469d770b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7f697cdcf4784bc1bf3beade0dafd1ff",
            "value": "‚Äá877/877‚Äá[00:00&lt;00:00,‚Äá19.2kB/s]"
          }
        },
        "aa900d71b34f47748fede8e9b32f2e1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eddde34319ac41a5bae8c10c6f746eb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56316327df634965bfadcd4a5d408cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66405af142ab494fa97bfe071143d7db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3b269797227454f8671eafbf70ff011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79c48dbb95624549a8adf9be469d770b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f697cdcf4784bc1bf3beade0dafd1ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "946a1781dc3449b1b9bad5710e5d9c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd8b2fdb65f7466fb823d91ab6b0a6c2",
              "IPY_MODEL_19ceb78becad499cb8a08bd789bc32b0",
              "IPY_MODEL_2c4b4c1327c7470ba174f54ea116d5e6"
            ],
            "layout": "IPY_MODEL_574fb885edf4470191ba112087182bab"
          }
        },
        "dd8b2fdb65f7466fb823d91ab6b0a6c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d975d2a2561449fb36411b85487e0f7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4236f92e117f439dbc71d0831bc994d2",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "19ceb78becad499cb8a08bd789bc32b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bd48e7af642429ab6f53bdc94e63ee3",
            "max": 54528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb635452bec74cee942339e730a26653",
            "value": 54528
          }
        },
        "2c4b4c1327c7470ba174f54ea116d5e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_204e0f52c3eb419495c4c1ad05739138",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_afe333679eda4e9aa44a6b5d9a5a36fc",
            "value": "‚Äá54.5k/54.5k‚Äá[00:00&lt;00:00,‚Äá3.99MB/s]"
          }
        },
        "574fb885edf4470191ba112087182bab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d975d2a2561449fb36411b85487e0f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4236f92e117f439dbc71d0831bc994d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bd48e7af642429ab6f53bdc94e63ee3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb635452bec74cee942339e730a26653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "204e0f52c3eb419495c4c1ad05739138": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afe333679eda4e9aa44a6b5d9a5a36fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee13cb0ac7d8497db6b7723483742205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f9ded113787403aa26d2b402f92a9cc",
              "IPY_MODEL_ed994b3a362947ce87a965587b07502c",
              "IPY_MODEL_5c0730c483c544b0b392f293048de01f"
            ],
            "layout": "IPY_MODEL_7f64cc8cc3df4bd5a58ce5a5f711f021"
          }
        },
        "7f9ded113787403aa26d2b402f92a9cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbdb470d52014c7598577b5ffef59e2f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_75c84087c6d848eea6ee48fdc74888b3",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "ed994b3a362947ce87a965587b07502c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a921a219935a49c7af2a3f5f04a8c139",
            "max": 9085657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a1e4120d0ef4388869c110c9a514ad3",
            "value": 9085657
          }
        },
        "5c0730c483c544b0b392f293048de01f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8214d893c9194555a35cf3d2bffc37a5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_22f8bea967cd4ead9ab1de8e3cb53412",
            "value": "‚Äá9.09M/9.09M‚Äá[00:00&lt;00:00,‚Äá22.9MB/s]"
          }
        },
        "7f64cc8cc3df4bd5a58ce5a5f711f021": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbdb470d52014c7598577b5ffef59e2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75c84087c6d848eea6ee48fdc74888b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a921a219935a49c7af2a3f5f04a8c139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a1e4120d0ef4388869c110c9a514ad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8214d893c9194555a35cf3d2bffc37a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22f8bea967cd4ead9ab1de8e3cb53412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21157fbf4617445d9fac9be92b84edfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5b84cee5dec4432a44e959ae9d49a71",
              "IPY_MODEL_8258a174f7ba478ea5a42aac892e6e6f",
              "IPY_MODEL_c5af4c01ab2a4539922e6f18c8af9d9c"
            ],
            "layout": "IPY_MODEL_7200dde8ad4c494e92c8146b3d176efc"
          }
        },
        "c5b84cee5dec4432a44e959ae9d49a71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e1b45e9212e407195ae7dca5d88c2ba",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_50c8ab5109e44fbd8910f4bec89432a0",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "8258a174f7ba478ea5a42aac892e6e6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd33341029d04d3da2ec6cfaad56914e",
            "max": 296,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_418c96a721204a9089908fd923a8d5a0",
            "value": 296
          }
        },
        "c5af4c01ab2a4539922e6f18c8af9d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f25e10f0f764085a4a7efd424ea9b25",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_686cfe75065c4accafc68619068886eb",
            "value": "‚Äá296/296‚Äá[00:00&lt;00:00,‚Äá13.3kB/s]"
          }
        },
        "7200dde8ad4c494e92c8146b3d176efc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e1b45e9212e407195ae7dca5d88c2ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50c8ab5109e44fbd8910f4bec89432a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd33341029d04d3da2ec6cfaad56914e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "418c96a721204a9089908fd923a8d5a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f25e10f0f764085a4a7efd424ea9b25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "686cfe75065c4accafc68619068886eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "700f93263cc7459088a7667c70eeea50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_805d0d32e0ba4bbd87a72bd01e94a42c",
              "IPY_MODEL_e973e0c6574343f0834f34248202c408",
              "IPY_MODEL_840c9a5435dc469d8ec0391f16812595"
            ],
            "layout": "IPY_MODEL_f4036280a0ed4f52a57bdb3583c86c80"
          }
        },
        "805d0d32e0ba4bbd87a72bd01e94a42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fd2fc2bb3f84291a5a856263a6fe973",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_682f860e6ffa4c26b31d7ccb5070c2a5",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "e973e0c6574343f0834f34248202c408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaf4c931dd3d4e63b4703e72fc9a606f",
            "max": 189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb93b904daff4564803bd8101b73b9b5",
            "value": 189
          }
        },
        "840c9a5435dc469d8ec0391f16812595": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01668123db954189ba4560f8847a99f7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_001e505cf2314291899367e1e1106977",
            "value": "‚Äá189/189‚Äá[00:00&lt;00:00,‚Äá10.9kB/s]"
          }
        },
        "f4036280a0ed4f52a57bdb3583c86c80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fd2fc2bb3f84291a5a856263a6fe973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "682f860e6ffa4c26b31d7ccb5070c2a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaf4c931dd3d4e63b4703e72fc9a606f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb93b904daff4564803bd8101b73b9b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01668123db954189ba4560f8847a99f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "001e505cf2314291899367e1e1106977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "559a6d4b31db41ba91c4f6508ee7d697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4be18d72b1f4b6c9ee4155d9a510527",
              "IPY_MODEL_1b7a9e1fb97e49ffa26efed26ce6cdf9",
              "IPY_MODEL_f116331cb07741b3a2a00f4a2fb04d1b"
            ],
            "layout": "IPY_MODEL_336a04b83355458b9ce125425cf68583"
          }
        },
        "b4be18d72b1f4b6c9ee4155d9a510527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_368ca25513d747098701b7fa9b5baee0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6622a943baf34c078e470f6875c46e54",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "1b7a9e1fb97e49ffa26efed26ce6cdf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efb86e32f6904eecbecd17a49fd1147e",
            "max": 2471645608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d2e4f931e7a4e208b6f0de18497875e",
            "value": 2471645608
          }
        },
        "f116331cb07741b3a2a00f4a2fb04d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a42cbc4c90094866ac7c076cb88fb128",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_73cab685900f4362a341101132324ba9",
            "value": "‚Äá2.47G/2.47G‚Äá[01:00&lt;00:00,‚Äá43.0MB/s]"
          }
        },
        "336a04b83355458b9ce125425cf68583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "368ca25513d747098701b7fa9b5baee0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6622a943baf34c078e470f6875c46e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efb86e32f6904eecbecd17a49fd1147e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d2e4f931e7a4e208b6f0de18497875e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a42cbc4c90094866ac7c076cb88fb128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73cab685900f4362a341101132324ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ec6ed04f3f14b42b2db4f8fd6c58a7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f29650a290ec4bcb9c5eddf4d1caf74d",
              "IPY_MODEL_69d8826c3b5043489ad4a45ee725947c",
              "IPY_MODEL_5af4743706b94f6f812ca590e779bc72"
            ],
            "layout": "IPY_MODEL_651f5a7eb7d54e2f9f7506f8d0573419"
          }
        },
        "f29650a290ec4bcb9c5eddf4d1caf74d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_901f2f87ed8f4acaa17118c2127636e0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6a85207f1aed4d838d4cf03da6562dec",
            "value": ""
          }
        },
        "69d8826c3b5043489ad4a45ee725947c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad0a9901db534ffbb45a30be9799a277",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f26b2896393d44d3954d97f93125d5a1",
            "value": 1
          }
        },
        "5af4743706b94f6f812ca590e779bc72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e198dbfb529420db4875ba894caf9c3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7fa613c1860a41699d8a61ccb89602d8",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá1/1‚Äá[00:03&lt;00:00,‚Äá‚Äá3.96s/it]\n"
          }
        },
        "651f5a7eb7d54e2f9f7506f8d0573419": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "901f2f87ed8f4acaa17118c2127636e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a85207f1aed4d838d4cf03da6562dec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad0a9901db534ffbb45a30be9799a277": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f26b2896393d44d3954d97f93125d5a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e198dbfb529420db4875ba894caf9c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fa613c1860a41699d8a61ccb89602d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
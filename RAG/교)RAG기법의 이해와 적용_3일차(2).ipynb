{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 🌼 RAG기법의 이해와 적용(2) - 3차시(24.12.02)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CLASS\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "logging.langsmith(\"CLASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./appendix-keywords.txt') as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "source": [
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-text-splitters in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-text-splitters) (0.3.21)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (0.1.147)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (0.28.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (2.23.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-text-splitters) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install langchain-text-splitters tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "# CharacterTextSplitter : 텍스트를 chunk로 분할\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 250,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "# 단순히 Character 단위로 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding'\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.create_documents([file])\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatas = [\n",
    "    {'document' : 1},\n",
    "    {'document' : 2}\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding' metadata={'document': 1}\n"
     ]
    }
   ],
   "source": [
    "documents = text_splitter.create_documents(\n",
    "    [file, file], # metadatas의 개수랑 일치하거나 메타데이터의 갯수가 더 많아야 오류가 안 남\n",
    "    metadatas = metadatas,\n",
    ")\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='멀티모달 (Multimodal)\n",
      "\n",
      "정의: 멀티모달은 여러 종류의 데이터 모드(예: 텍스트, 이미지, 소리 등)를 결합하여 처리하는 기술입니다. 이는 서로 다른 형식의 데이터 간의 상호 작용을 통해 보다 풍부하고 정확한 정보를 추출하거나 예측하는 데 사용됩니다.\n",
      "예시: 이미지와 설명 텍스트를 함께 분석하여 더 정확한 이미지 분류를 수행하는 시스템은 멀티모달 기술의 예입니다.\n",
      "연관키워드: 데이터 융합, 인공지능, 딥러닝' metadata={'document': 2}\n"
     ]
    }
   ],
   "source": [
    "print(documents[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain-text-splitters tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 358, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n",
      "Created a chunk of size 305, which is longer than the specified 300\n",
      "Created a chunk of size 366, which is longer than the specified 300\n",
      "Created a chunk of size 330, which is longer than the specified 300\n",
      "Created a chunk of size 351, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 361, which is longer than the specified 300\n",
      "Created a chunk of size 350, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n",
      "Created a chunk of size 335, which is longer than the specified 300\n",
      "Created a chunk of size 353, which is longer than the specified 300\n",
      "Created a chunk of size 358, which is longer than the specified 300\n",
      "Created a chunk of size 336, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 337, which is longer than the specified 300\n",
      "Created a chunk of size 307, which is longer than the specified 300\n",
      "Created a chunk of size 361, which is longer than the specified 300\n",
      "Created a chunk of size 354, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 381, which is longer than the specified 300\n",
      "Created a chunk of size 365, which is longer than the specified 300\n",
      "Created a chunk of size 377, which is longer than the specified 300\n",
      "Created a chunk of size 329, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 5,\n",
    ")\n",
    "texts = text_splitter.split_text(file)\n",
    "# CharacterTextSplitter.from_tiktoken_encoder(토큰단위로 나눔) : 기본 구분자인 (\\n\\n)를 기준으로 분할하는데,\n",
    "# 구분자가 문장내에서 발견 되지 않아 분할할 수 없는 경우, chunk_size보다 큰 chunk가 생성될 수 있음\n",
    "# 출력값의 의미는 내가 나눈 300보다 더 큰 값으로 나뉘어 졌다는 의미(크게 상관 X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "print(len(texts)) # 51개로 나뉘어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"�\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 0\n",
    ")\n",
    "texts = text_splitter.split_text(file)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.5.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Downloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Downloading torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.9/203.1 MB 14.0 MB/s eta 0:00:15\n",
      "    --------------------------------------- 4.7/203.1 MB 11.0 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 7.1/203.1 MB 11.5 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 9.2/203.1 MB 11.0 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 10.5/203.1 MB 10.9 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 12.1/203.1 MB 9.7 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 14.7/203.1 MB 9.9 MB/s eta 0:00:19\n",
      "   --- ------------------------------------ 16.8/203.1 MB 10.0 MB/s eta 0:00:19\n",
      "   --- ------------------------------------ 19.1/203.1 MB 10.2 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 20.4/203.1 MB 9.7 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 22.5/203.1 MB 9.7 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 24.6/203.1 MB 9.8 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 26.7/203.1 MB 9.7 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 28.3/203.1 MB 9.6 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 29.4/203.1 MB 9.5 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 31.2/203.1 MB 9.2 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 33.0/203.1 MB 9.1 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 34.6/203.1 MB 9.2 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 34.9/203.1 MB 8.6 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 37.0/203.1 MB 8.7 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 39.3/203.1 MB 8.8 MB/s eta 0:00:19\n",
      "   -------- ------------------------------- 41.7/203.1 MB 8.9 MB/s eta 0:00:19\n",
      "   -------- ------------------------------- 44.6/203.1 MB 9.1 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 46.4/203.1 MB 9.1 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 48.8/203.1 MB 9.2 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 49.3/203.1 MB 9.1 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 50.3/203.1 MB 8.7 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 51.4/203.1 MB 8.7 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 52.4/203.1 MB 8.5 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 52.7/203.1 MB 8.3 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 54.5/203.1 MB 8.2 MB/s eta 0:00:19\n",
      "   ----------- ---------------------------- 56.4/203.1 MB 8.3 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 58.7/203.1 MB 8.3 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 60.8/203.1 MB 8.5 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 62.9/203.1 MB 8.4 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 64.5/203.1 MB 8.4 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 66.6/203.1 MB 8.5 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 68.4/203.1 MB 8.5 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 70.0/203.1 MB 8.4 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 71.6/203.1 MB 8.4 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 73.1/203.1 MB 8.4 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 74.4/203.1 MB 8.4 MB/s eta 0:00:16\n",
      "   --------------- ------------------------ 76.3/203.1 MB 8.3 MB/s eta 0:00:16\n",
      "   --------------- ------------------------ 78.1/203.1 MB 8.4 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 79.7/203.1 MB 8.4 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 81.5/203.1 MB 8.3 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 83.9/203.1 MB 8.4 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 86.0/203.1 MB 8.4 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 88.3/203.1 MB 8.5 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 90.2/203.1 MB 8.5 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 92.3/203.1 MB 8.5 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 94.4/203.1 MB 8.5 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 97.0/203.1 MB 8.6 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 99.6/203.1 MB 8.7 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 102.0/203.1 MB 8.7 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 103.8/203.1 MB 8.7 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 105.6/203.1 MB 8.7 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 107.5/203.1 MB 8.7 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 109.8/203.1 MB 8.8 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 112.7/203.1 MB 8.8 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 115.9/203.1 MB 9.0 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 119.3/203.1 MB 9.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 119.5/203.1 MB 9.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 121.6/203.1 MB 9.0 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 124.3/203.1 MB 9.0 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 126.9/203.1 MB 9.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 129.5/203.1 MB 9.1 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 132.1/203.1 MB 9.2 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 135.3/203.1 MB 9.2 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 138.7/203.1 MB 9.3 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 141.3/203.1 MB 9.4 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 144.4/203.1 MB 9.5 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 147.3/203.1 MB 9.5 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 150.2/203.1 MB 9.6 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 153.6/203.1 MB 9.7 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 156.8/203.1 MB 9.7 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 160.4/203.1 MB 9.8 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 164.1/203.1 MB 9.9 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 167.8/203.1 MB 10.0 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 171.2/203.1 MB 10.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 174.9/203.1 MB 10.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 178.3/203.1 MB 10.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 181.4/203.1 MB 10.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 182.5/203.1 MB 10.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 184.8/203.1 MB 10.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 188.5/203.1 MB 10.3 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 192.4/203.1 MB 10.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 194.0/203.1 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  198.2/203.1 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.1/203.1 MB 10.4 MB/s eta 0:00:00\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 7.3/11.0 MB 34.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 29.9 MB/s eta 0:00:00\n",
      "Downloading scipy-1.14.1-cp311-cp311-win_amd64.whl (44.8 MB)\n",
      "   ---------------------------------------- 0.0/44.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 5.5/44.8 MB 23.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 16.8/44.8 MB 40.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 19.9/44.8 MB 37.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 24.6/44.8 MB 28.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 27.3/44.8 MB 28.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 30.4/44.8 MB 23.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 36.4/44.8 MB 24.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.0/44.8 MB 26.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.8/44.8 MB 23.7 MB/s eta 0:00:00\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.20.3-cp311-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 34.2 MB/s eta 0:00:00\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 47.1 MB/s eta 0:00:00\n",
      "Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, threadpoolctl, sympy, scipy, safetensors, networkx, MarkupSafe, fsspec, filelock, scikit-learn, jinja2, huggingface-hub, torch, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.3 jinja2-3.1.4 mpmath-1.3.0 networkx-3.4.2 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sentence-transformers-3.3.1 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.20.3 torch-2.5.1 transformers-4.46.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install sentence-transformers # 성능은 좋지만 시간이 오래 걸리는 텍스트 분할기!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RMARKET\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "# transformer 모델에 특화된 분할기\n",
    "splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7686\n"
     ]
    }
   ],
   "source": [
    "count_start_and_stop_token = 2 # trnasformer에 특화된 분할기 이기 때문에 앞뒤의 sos,eos를 제거하는 변수\n",
    "\n",
    "text_token_count = splitter.count_tokens(text = file) - count_start_and_stop_token\n",
    "print(text_token_count)\n",
    "# 모델 입력 제한을 초과하지 않도록 토큰 개수를 조절하기 위해\n",
    "# eos, sos등 문장의 시작과 끝 토큰을 제외한 나비저 토큰 수를 출력\n",
    "# 참고값으로 출력해서 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = splitter.split_text(text=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 [UNK] 합니다. [UNK] : \" 사과 \" 라는 단어를 [ 0. 65, - 0. 23, 0. 17 ] 과 [UNK] 벡터로 표현합니다. 연관키워드 : 자연어 처리, 벡터화, 딥러닝 token 정의 : 토큰은 텍스트를 더 작은 [UNK] 분할하는 [UNK] 의미합니다. 이는 일반적으로 단어, 문장, [UNK] 구절일 수 [UNK]. [UNK] : 문장 \" 나는 학교에 간다 \" 를 \" 나는 \", \" 학교에 \", \" 간다 \" 로 분할합니다. 연관키워드 : 토큰화, 자연어 처리, 구문 분석 tokenizer\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[1])\n",
    "# [UNK] : 단어 사전에 없는 희귀 단어나 특수단어, 특수기호 등을 대체하는 토큰\n",
    "# # : 문장이 이어진다는 것을 의미 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RMARKET\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 358, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n",
      "Created a chunk of size 305, which is longer than the specified 300\n",
      "Created a chunk of size 366, which is longer than the specified 300\n",
      "Created a chunk of size 330, which is longer than the specified 300\n",
      "Created a chunk of size 351, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 361, which is longer than the specified 300\n",
      "Created a chunk of size 350, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n",
      "Created a chunk of size 335, which is longer than the specified 300\n",
      "Created a chunk of size 353, which is longer than the specified 300\n",
      "Created a chunk of size 358, which is longer than the specified 300\n",
      "Created a chunk of size 336, which is longer than the specified 300\n",
      "Created a chunk of size 324, which is longer than the specified 300\n",
      "Created a chunk of size 337, which is longer than the specified 300\n",
      "Created a chunk of size 307, which is longer than the specified 300\n",
      "Created a chunk of size 361, which is longer than the specified 300\n",
      "Created a chunk of size 354, which is longer than the specified 300\n",
      "Created a chunk of size 378, which is longer than the specified 300\n",
      "Created a chunk of size 381, which is longer than the specified 300\n",
      "Created a chunk of size 365, which is longer than the specified 300\n",
      "Created a chunk of size 377, which is longer than the specified 300\n",
      "Created a chunk of size 329, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n"
     ]
    }
   ],
   "source": [
    "print(texts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model = 'text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '임베딩 텍스트를 하기 위한 샘플 문장입니다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embeddings.embed_query(text) # 단일 문장을 처리할 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.004169010091573,\n",
       " 0.0382189080119133,\n",
       " 0.015991004183888435,\n",
       " 0.004473187029361725,\n",
       " 0.01716681197285652]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.004169010091573,\n",
       " 0.0382189080119133,\n",
       " 0.015991004183888435,\n",
       " 0.004473187029361725,\n",
       " 0.01716681197285652]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result = embeddings.embed_documents([text]) # 다양한 문장을 변환하기 때문에 리스트 형태로 입력\n",
    "doc_result[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 차원을 조절하는 모델\n",
    "embeddings_1024 = OpenAIEmbeddings(model = 'text-embedding-3-small', dimensions=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_1024.embed_documents([text])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장의 유사도를 파악!!\n",
    "sentence1 = '안녕하세요? 처음뵙겠습니다.'\n",
    "sentence2 = '안녕하세요? 처음뵙겠습니다!'\n",
    "sentence3 = '안녕하세요, 만나서 반갑습니다.'\n",
    "sentence4 = 'Hi! Nice to meet you.'\n",
    "sentence5 = 'I like to eat apples.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity # 코사인 유사도, 2차원 배열을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=[sentence1, sentence2, sentence3, sentence4, sentence5]\n",
    "embedded_sentence = embeddings_1024.embed_documents(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(a, b):\n",
    "    return cosine_similarity([a], [b])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[유사도 0.9736] 안녕하세요? 처음뵙겠습니다. < = > 안녕하세요? 처음뵙겠습니다!\n",
      "[유사도 0.6243] 안녕하세요? 처음뵙겠습니다. < = > 안녕하세요, 만나서 반갑습니다.\n",
      "[유사도 0.4927] 안녕하세요? 처음뵙겠습니다. < = > Hi! Nice to meet you.\n",
      "[유사도 0.1812] 안녕하세요? 처음뵙겠습니다. < = > I like to eat apples.\n",
      "[유사도 0.6015] 안녕하세요? 처음뵙겠습니다! < = > 안녕하세요, 만나서 반갑습니다.\n",
      "[유사도 0.4950] 안녕하세요? 처음뵙겠습니다! < = > Hi! Nice to meet you.\n",
      "[유사도 0.1750] 안녕하세요? 처음뵙겠습니다! < = > I like to eat apples.\n",
      "[유사도 0.4807] 안녕하세요, 만나서 반갑습니다. < = > Hi! Nice to meet you.\n",
      "[유사도 0.1498] 안녕하세요, 만나서 반갑습니다. < = > I like to eat apples.\n",
      "[유사도 0.2332] Hi! Nice to meet you. < = > I like to eat apples.\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(embedded_sentence): # enumerate : 인덱스 생성\n",
    "    for j, other_sentence in enumerate(embedded_sentence):\n",
    "        if i < j :\n",
    "            print(f'[유사도 {similarity(sentence, other_sentence):.4f}] {sentences[i]} < = > {sentences[j]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 100, chunk_overlap = 0)\n",
    "\n",
    "loader1 = TextLoader('./nlp-keywords.txt')\n",
    "loader2 = TextLoader('./finance-keywords.txt')\n",
    "# TextLoader로 불러오면 Document 리스트 형태로 변환\n",
    "# Dcoument 리스트 형태는 page_content 와 metadata 를 포함하는 딕셔너리, 클래스 형태로 구성\n",
    "# page_content : 문서의 실제 텍스트 내용\n",
    "# metadata : 문서의 부가 정보(예: 문서 ID, 제목, 작성자, 날짜 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': './nlp-keywords.txt'}, page_content='Semantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\\n\\nEmbedding\\n\\n정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken\\n\\n정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\\n예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\\n연관키워드: 토큰화, 자연어 처리, 구문 분석\\n\\nTokenizer\\n\\n정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\\n예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다.\\n연관키워드: 토큰화, 자연어 처리, 구문 분석\\n\\nVectorStore\\n\\n정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\\n예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.\\n연관키워드: 임베딩, 데이터베이스, 벡터화\\n\\nSQL\\n\\n정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\\n예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.\\n연관키워드: 데이터베이스, 쿼리, 데이터 관리\\n\\nCSV\\n\\n정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\\n예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다.\\n연관키워드: 데이터 형식, 파일 처리, 데이터 교환\\n\\nJSON\\n\\n정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\\n예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\\n연관키워드: 데이터 교환, 웹 개발, API\\n\\nTransformer\\n\\n정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.\\n예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\\n연관키워드: 딥러닝, 자연어 처리, Attention\\n\\nHuggingFace\\n\\n정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다.\\n예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.\\n연관키워드: 자연어 처리, 딥러닝, 라이브러리\\n\\nDigital Transformation\\n\\n정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.\\n예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\\n연관키워드: 혁신, 기술, 비즈니스 모델\\n\\nCrawling\\n\\n정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\\n예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\\n\\nWord2Vec\\n\\n정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\\nLLM (Large Language Model)\\n\\n정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다. 이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.\\n예시: OpenAI의 GPT 시리즈는 대표적인 대규모 언어 모델입니다.\\n연관키워드: 자연어 처리, 딥러닝, 텍스트 생성\\n\\nFAISS (Facebook AI Similarity Search)\\n\\n정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\\n예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\\n연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\\n\\nOpen Source\\n\\n정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.\\n예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업\\n\\nStructured Data\\n\\n정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.\\n예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링\\n\\nParser\\n\\n정의: 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다. 이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.\\n예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n연관키워드: 구문 분석, 컴파일러, 데이터 처리\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency)\\n\\n정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.\\n예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝\\n\\nDeep Learning\\n\\n정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.\\n예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n연관키워드: 인공신경망, 머신러닝, 데이터 분석\\n\\nSchema\\n\\n정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.\\n예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리\\n\\nDataFrame\\n\\n정의: DataFrame은 행과 열로 이루어진 테이블 형태의 데이터 구조로, 주로 데이터 분석 및 처리에 사용됩니다.\\n예시: 판다스 라이브러리에서 DataFrame은 다양한 데이터 타입의 열을 가질 수 있으며, 데이터 조작과 분석을 용이하게 합니다.\\n연관키워드: 데이터 분석, 판다스, 데이터 처리\\n\\nAttention 메커니즘\\n\\n정의: Attention 메커니즘은 딥러닝에서 중요한 정보에 더 많은 \\'주의\\'를 기울이도록 하는 기법입니다. 이는 주로 시퀀스 데이터(예: 텍스트, 시계열 데이터)에서 사용됩니다.\\n예시: 번역 모델에서 Attention 메커니즘은 입력 문장의 중요한 부분에 더 집중하여 정확한 번역을 생성합니다.\\n연관키워드: 딥러닝, 자연어 처리, 시퀀스 모델링\\n\\n판다스 (Pandas)\\n\\n정의: 판다스는 파이썬 프로그래밍 언어를 위한 데이터 분석 및 조작 도구를 제공하는 라이브러리입니다. 이는 데이터 분석 작업을 효율적으로 수행할 수 있게 합니다.\\n예시: 판다스를 사용하여 CSV 파일을 읽고, 데이터를 정제하며, 다양한 분석을 수행할 수 있습니다.\\n연관키워드: 데이터 분석, 파이썬, 데이터 처리\\n\\nGPT (Generative Pretrained Transformer)\\n\\n정의: GPT는 대규모의 데이터셋으로 사전 훈련된 생성적 언어 모델로, 다양한 텍스트 기반 작업에 활용됩니다. 이는 입력된 텍스트에 기반하여 자연스러운 언어를 생성할 수 있습니다.\\n예시: 사용자가 제공한 질문에 대해 자세한 답변을 생성하는 챗봇은 GPT 모델을 사용할 수 있습니다.\\n연관키워드: 자연어 처리, 텍스트 생성, 딥러닝\\n\\nInstructGPT\\n\\n정의: InstructGPT는 사용자의 지시에 따라 특정한 작업을 수행하기 위해 최적화된 GPT 모델입니다. 이 모델은 보다 정확하고 관련성 높은 결과를 생성하도록 설계되었습니다.\\n예시: 사용자가 \"이메일 초안 작성\"과 같은 특정 지시를 제공하면, InstructGPT는 관련 내용을 기반으로 이메일을 작성합니다.\\n연관키워드: 인공지능, 자연어 이해, 명령 기반 처리\\n\\nKeyword Search\\n\\n정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\\n예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\\n연관키워드: 검색 엔진, 데이터 검색, 정보 검색\\n\\nPage Rank\\n\\n정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\\n예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\\n연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\\n\\n데이터 마이닝\\n\\n정의: 데이터 마이닝은 대량의 데이터에서 유용한 정보를 발굴하는 과정입니다. 이는 통계, 머신러닝, 패턴 인식 등의 기술을 활용합니다.\\n예시: 소매업체가 고객 구매 데이터를 분석하여 판매 전략을 수립하는 것은 데이터 마이닝의 예입니다.\\n연관키워드: 빅데이터, 패턴 인식, 예측 분석\\n\\n멀티모달 (Multimodal)\\n\\n정의: 멀티모달은 여러 종류의 데이터 모드(예: 텍스트, 이미지, 소리 등)를 결합하여 처리하는 기술입니다. 이는 서로 다른 형식의 데이터 간의 상호 작용을 통해 보다 풍부하고 정확한 정보를 추출하거나 예측하는 데 사용됩니다.\\n예시: 이미지와 설명 텍스트를 함께 분석하여 더 정확한 이미지 분류를 수행하는 시스템은 멀티모달 기술의 예입니다.\\n연관키워드: 데이터 융합, 인공지능, 딥러닝\\n')]\n"
     ]
    }
   ],
   "source": [
    "documents = loader1.load()\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_doc1 = loader1.load_and_split(text_splitter)\n",
    "split_doc2 = loader2.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': './nlp-keywords.txt'}, page_content='Semantic Search'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Embedding'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Token'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\\n연관키워드: 토큰화, 자연어 처리, 구문 분석'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Tokenizer'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='연관키워드: 토큰화, 자연어 처리, 구문 분석'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='VectorStore'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.\\n연관키워드: 임베딩, 데이터베이스, 벡터화'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='SQL'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='수행할 수 있습니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.\\n연관키워드: 데이터베이스, 쿼리, 데이터 관리'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='CSV'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='사용됩니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다.\\n연관키워드: 데이터 형식, 파일 처리, 데이터 교환'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='JSON'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\\n연관키워드: 데이터 교환, 웹 개발, API'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Transformer'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\\n연관키워드: 딥러닝, 자연어 처리, Attention'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='HuggingFace'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='돕습니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='연관키워드: 자연어 처리, 딥러닝, 라이브러리'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Digital Transformation'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\\n연관키워드: 혁신, 기술, 비즈니스 모델'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Crawling'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Word2Vec'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='LLM (Large Language Model)'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다. 이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: OpenAI의 GPT 시리즈는 대표적인 대규모 언어 모델입니다.\\n연관키워드: 자연어 처리, 딥러닝, 텍스트 생성'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='FAISS (Facebook AI Similarity Search)'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\\n연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Open Source'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Structured Data'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Parser'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다. 이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n연관키워드: 구문 분석, 컴파일러, 데이터 처리'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='TF-IDF (Term Frequency-Inverse Document Frequency)'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Deep Learning'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n연관키워드: 인공신경망, 머신러닝, 데이터 분석'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Schema'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='DataFrame'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: DataFrame은 행과 열로 이루어진 테이블 형태의 데이터 구조로, 주로 데이터 분석 및 처리에 사용됩니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 판다스 라이브러리에서 DataFrame은 다양한 데이터 타입의 열을 가질 수 있으며, 데이터 조작과 분석을 용이하게 합니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='연관키워드: 데이터 분석, 판다스, 데이터 처리'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Attention 메커니즘'), Document(metadata={'source': './nlp-keywords.txt'}, page_content=\"정의: Attention 메커니즘은 딥러닝에서 중요한 정보에 더 많은 '주의'를 기울이도록 하는 기법입니다. 이는 주로 시퀀스 데이터(예: 텍스트, 시계열 데이터)에서\"), Document(metadata={'source': './nlp-keywords.txt'}, page_content='사용됩니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 번역 모델에서 Attention 메커니즘은 입력 문장의 중요한 부분에 더 집중하여 정확한 번역을 생성합니다.\\n연관키워드: 딥러닝, 자연어 처리, 시퀀스 모델링'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='판다스 (Pandas)'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 판다스는 파이썬 프로그래밍 언어를 위한 데이터 분석 및 조작 도구를 제공하는 라이브러리입니다. 이는 데이터 분석 작업을 효율적으로 수행할 수 있게 합니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 판다스를 사용하여 CSV 파일을 읽고, 데이터를 정제하며, 다양한 분석을 수행할 수 있습니다.\\n연관키워드: 데이터 분석, 파이썬, 데이터 처리'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='GPT (Generative Pretrained Transformer)'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: GPT는 대규모의 데이터셋으로 사전 훈련된 생성적 언어 모델로, 다양한 텍스트 기반 작업에 활용됩니다. 이는 입력된 텍스트에 기반하여 자연스러운 언어를 생성할 수'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='있습니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 사용자가 제공한 질문에 대해 자세한 답변을 생성하는 챗봇은 GPT 모델을 사용할 수 있습니다.\\n연관키워드: 자연어 처리, 텍스트 생성, 딥러닝'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='InstructGPT'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: InstructGPT는 사용자의 지시에 따라 특정한 작업을 수행하기 위해 최적화된 GPT 모델입니다. 이 모델은 보다 정확하고 관련성 높은 결과를 생성하도록'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='설계되었습니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 사용자가 \"이메일 초안 작성\"과 같은 특정 지시를 제공하면, InstructGPT는 관련 내용을 기반으로 이메일을 작성합니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='연관키워드: 인공지능, 자연어 이해, 명령 기반 처리'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Keyword Search'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\\n연관키워드: 검색 엔진, 데이터 검색, 정보 검색'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='Page Rank'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\\n연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='데이터 마이닝'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 데이터 마이닝은 대량의 데이터에서 유용한 정보를 발굴하는 과정입니다. 이는 통계, 머신러닝, 패턴 인식 등의 기술을 활용합니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 소매업체가 고객 구매 데이터를 분석하여 판매 전략을 수립하는 것은 데이터 마이닝의 예입니다.\\n연관키워드: 빅데이터, 패턴 인식, 예측 분석'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='멀티모달 (Multimodal)'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 멀티모달은 여러 종류의 데이터 모드(예: 텍스트, 이미지, 소리 등)를 결합하여 처리하는 기술입니다. 이는 서로 다른 형식의 데이터 간의 상호 작용을 통해 보다 풍부하고'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='정확한 정보를 추출하거나 예측하는 데 사용됩니다.'), Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 이미지와 설명 텍스트를 함께 분석하여 더 정확한 이미지 분류를 수행하는 시스템은 멀티모달 기술의 예입니다.\\n연관키워드: 데이터 융합, 인공지능, 딥러닝')]\n"
     ]
    }
   ],
   "source": [
    "print(split_doc1) # 여러개의 chunk로 나뉘어 짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 60)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_doc1), len(split_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.9.0.post1-cp311-cp311-win_amd64.whl (13.8 MB)\n",
      "   ---------------------------------------- 0.0/13.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 13.8/13.8 MB 72.2 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install faiss-cpu # 페이스북에서 만든 벡터 디비 라이브러리\n",
    "# -cpu 붙이면 cpu 사용에 최적화\n",
    "# -gpu 붙이면 gpu 사용에 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 하는 모델 불러오기\n",
    "embeddings = OpenAIEmbeddings(model = 'text-embedding-3-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n"
     ]
    }
   ],
   "source": [
    "dimension_size = len(embeddings.embed_query('임베딩 차원 크기'))\n",
    "print(dimension_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 저장소 생성(직접 생성)\n",
    "db = FAISS(\n",
    "    embedding_function = embeddings,\n",
    "    # 임베딩 함수\n",
    "    index = faiss.IndexFlatL2(dimension_size),\n",
    "    # 사용할 FAISS 인덱스\n",
    "    # L2 : L2 거리 (Euclidean distance) 두 벡터 간의 직선 거리를 계산하는 방법으로, 차이가 클수록 두 벡터는 다르다고 간주됩니다.\n",
    "    docstore = InMemoryDocstore(), # 실행이 되는 동안에 관리하고 저장한다는 의미\n",
    "    # 사용할 문서 저장소\n",
    "    index_to_docstore_id={}\n",
    "    # 인덱스에서 문서 저장소 ID로 매핑\n",
    ")\n",
    "# 0.3, 0.8, 0.7 -> 유클리디안 거리 계싼 결과로 INDEX가 1을 부여 받았다고 가정하면\n",
    "# index_to_docstore_id = {1:[0.3,0.8,0.7]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents=split_doc1, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './nlp-keywords.txt'}, page_content='Semantic Search'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Embedding'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Token'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\\n연관키워드: 토큰화, 자연어 처리, 구문 분석'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Tokenizer'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='연관키워드: 토큰화, 자연어 처리, 구문 분석'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='VectorStore'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.\\n연관키워드: 임베딩, 데이터베이스, 벡터화'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='SQL'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='수행할 수 있습니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.\\n연관키워드: 데이터베이스, 쿼리, 데이터 관리'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='CSV'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='사용됩니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다.\\n연관키워드: 데이터 형식, 파일 처리, 데이터 교환'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='JSON'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\\n연관키워드: 데이터 교환, 웹 개발, API'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Transformer'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\\n연관키워드: 딥러닝, 자연어 처리, Attention'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='HuggingFace'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='돕습니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='연관키워드: 자연어 처리, 딥러닝, 라이브러리'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Digital Transformation'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\\n연관키워드: 혁신, 기술, 비즈니스 모델'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Crawling'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Word2Vec'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='LLM (Large Language Model)'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다. 이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: OpenAI의 GPT 시리즈는 대표적인 대규모 언어 모델입니다.\\n연관키워드: 자연어 처리, 딥러닝, 텍스트 생성'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='FAISS (Facebook AI Similarity Search)'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\\n연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Open Source'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Structured Data'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Parser'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다. 이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n연관키워드: 구문 분석, 컴파일러, 데이터 처리'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='TF-IDF (Term Frequency-Inverse Document Frequency)'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Deep Learning'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n연관키워드: 인공신경망, 머신러닝, 데이터 분석'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Schema'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='DataFrame'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: DataFrame은 행과 열로 이루어진 테이블 형태의 데이터 구조로, 주로 데이터 분석 및 처리에 사용됩니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 판다스 라이브러리에서 DataFrame은 다양한 데이터 타입의 열을 가질 수 있으며, 데이터 조작과 분석을 용이하게 합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='연관키워드: 데이터 분석, 판다스, 데이터 처리'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Attention 메커니즘'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content=\"정의: Attention 메커니즘은 딥러닝에서 중요한 정보에 더 많은 '주의'를 기울이도록 하는 기법입니다. 이는 주로 시퀀스 데이터(예: 텍스트, 시계열 데이터)에서\"),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='사용됩니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 번역 모델에서 Attention 메커니즘은 입력 문장의 중요한 부분에 더 집중하여 정확한 번역을 생성합니다.\\n연관키워드: 딥러닝, 자연어 처리, 시퀀스 모델링'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='판다스 (Pandas)'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 판다스는 파이썬 프로그래밍 언어를 위한 데이터 분석 및 조작 도구를 제공하는 라이브러리입니다. 이는 데이터 분석 작업을 효율적으로 수행할 수 있게 합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 판다스를 사용하여 CSV 파일을 읽고, 데이터를 정제하며, 다양한 분석을 수행할 수 있습니다.\\n연관키워드: 데이터 분석, 파이썬, 데이터 처리'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='GPT (Generative Pretrained Transformer)'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: GPT는 대규모의 데이터셋으로 사전 훈련된 생성적 언어 모델로, 다양한 텍스트 기반 작업에 활용됩니다. 이는 입력된 텍스트에 기반하여 자연스러운 언어를 생성할 수'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='있습니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 사용자가 제공한 질문에 대해 자세한 답변을 생성하는 챗봇은 GPT 모델을 사용할 수 있습니다.\\n연관키워드: 자연어 처리, 텍스트 생성, 딥러닝'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='InstructGPT'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: InstructGPT는 사용자의 지시에 따라 특정한 작업을 수행하기 위해 최적화된 GPT 모델입니다. 이 모델은 보다 정확하고 관련성 높은 결과를 생성하도록'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='설계되었습니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 사용자가 \"이메일 초안 작성\"과 같은 특정 지시를 제공하면, InstructGPT는 관련 내용을 기반으로 이메일을 작성합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='연관키워드: 인공지능, 자연어 이해, 명령 기반 처리'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Keyword Search'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\\n연관키워드: 검색 엔진, 데이터 검색, 정보 검색'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='Page Rank'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\\n연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='데이터 마이닝'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 데이터 마이닝은 대량의 데이터에서 유용한 정보를 발굴하는 과정입니다. 이는 통계, 머신러닝, 패턴 인식 등의 기술을 활용합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 소매업체가 고객 구매 데이터를 분석하여 판매 전략을 수립하는 것은 데이터 마이닝의 예입니다.\\n연관키워드: 빅데이터, 패턴 인식, 예측 분석'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='멀티모달 (Multimodal)'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 멀티모달은 여러 종류의 데이터 모드(예: 텍스트, 이미지, 소리 등)를 결합하여 처리하는 기술입니다. 이는 서로 다른 형식의 데이터 간의 상호 작용을 통해 보다 풍부하고'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정확한 정보를 추출하거나 예측하는 데 사용됩니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 이미지와 설명 텍스트를 함께 분석하여 더 정확한 이미지 분류를 수행하는 시스템은 멀티모달 기술의 예입니다.\\n연관키워드: 데이터 융합, 인공지능, 딥러닝')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '35c756fa-5646-4bf4-ad91-31f2dee028cf',\n",
       " 1: 'a32a89e4-d5f4-4141-84c9-950d0d9ae6ac',\n",
       " 2: '73267bc4-3e76-4b76-a323-ad7267d9a563',\n",
       " 3: '667d6357-cc93-44d9-9667-3f96e4897cbd',\n",
       " 4: '5e4cf004-c056-4093-9b87-066dcbab47e7',\n",
       " 5: 'df6b6958-4085-4b1b-86af-1be4e27678fd',\n",
       " 6: '3e60fb1a-7b03-485e-a651-60d958ffccd1',\n",
       " 7: 'c8a577ab-0f59-4cc6-bc85-d9ed61fb13ee',\n",
       " 8: '565ff0c9-a90a-49d6-96d0-349ad4bd4a91',\n",
       " 9: 'd139ae87-6d97-4671-9d09-5b05e07c5d98',\n",
       " 10: '9fb8e6ca-ec3c-4e59-84ec-8e08a9056791',\n",
       " 11: '878134d3-43b8-4634-9f91-98804dc755dd',\n",
       " 12: '231bfa99-8e6b-4b6d-be3d-527250846beb',\n",
       " 13: '85ae10ed-6e7b-402b-ac4d-a0b18ac32e5d',\n",
       " 14: '1ae7709f-96a8-4c12-827b-886692d71476',\n",
       " 15: '29218b7f-0605-4085-90f1-3883d2647de6',\n",
       " 16: '4614882d-291f-467a-93bc-42c7f5bd085e',\n",
       " 17: '93c36785-a729-4ed4-952e-306c35207f77',\n",
       " 18: 'be3030a9-39a2-4e46-8764-816c7cd74466',\n",
       " 19: '6cb1552e-b55c-411e-ab87-019b24109110',\n",
       " 20: 'e6e30d4c-0bdb-4167-9ab1-ec38c6f81796',\n",
       " 21: '8fc1645e-9569-4937-a6f6-86de671b8f89',\n",
       " 22: '507f6141-6daa-4e6f-bda0-f767c38d9411',\n",
       " 23: '051e54f7-cb7c-44d3-8207-00c8ed4f89a4',\n",
       " 24: 'dd8c1cde-8e41-4eee-9a49-4ef98b9099da',\n",
       " 25: '365c01f8-b5f0-44cf-8ba3-aa94af282af3',\n",
       " 26: '79a08136-a276-4bb3-8346-c2fc8a806566',\n",
       " 27: '6e0c99ed-325e-41c8-8b59-c125d5602ffd',\n",
       " 28: '535334a5-a47c-42ab-a662-08be99c91194',\n",
       " 29: 'f0a906e5-3391-48da-82fd-a4dfad12704b',\n",
       " 30: '3be02ddd-c008-4798-95eb-26885f216c5e',\n",
       " 31: '28548b9e-6751-4a99-a8f4-e36f350922ea',\n",
       " 32: '7f9a01ac-841c-42be-a2da-bbf2828b3f83',\n",
       " 33: 'd272bcd9-1a4e-49aa-bd67-6069ceb48807',\n",
       " 34: 'bf004e67-0dad-4477-928d-41f7926ca85c',\n",
       " 35: '90a795e6-4442-451f-8b91-de8af0279794',\n",
       " 36: '52d96a85-3548-4f81-97a7-5741f245256f',\n",
       " 37: 'bc42c894-1352-4ef6-9a00-c6ba8cdf69fc',\n",
       " 38: 'bfe6ae24-77c8-4427-a585-858eebfc3557',\n",
       " 39: '73434bcb-6130-439c-93b0-4da3b808f7f5',\n",
       " 40: 'c85fbcdd-204e-40dc-a14e-bd6ba6565c58',\n",
       " 41: '846ec9c6-69ad-407d-89aa-f938206a61e4',\n",
       " 42: 'a110482f-3619-4b51-abb1-6d3ff89f4f04',\n",
       " 43: 'a0eba335-606d-42f9-b622-b2a6152e3c60',\n",
       " 44: 'b41cc867-82c2-4138-baec-ae35c938a0c0',\n",
       " 45: '54a46322-7085-42d0-bdd2-5f2c5031a494',\n",
       " 46: 'a498bf8d-e00a-48f8-9083-2a8ec27652f9',\n",
       " 47: '6f3380a5-32bb-44f2-93c1-9e618cdf8fad',\n",
       " 48: '6805af36-68ec-4aab-ad80-8aeaae541c96',\n",
       " 49: 'd6fa1d32-bb0d-4898-a6a9-4428e89e0b7a',\n",
       " 50: '213b8eea-d01a-4645-b0c3-d80fb51d712a',\n",
       " 51: 'cd1f01f5-0ee4-4097-a42f-d9c1ce371077',\n",
       " 52: '40024248-52cf-4fe2-a6a2-11ba7aa6c3f8',\n",
       " 53: 'b73d0f3f-9e87-441f-bdf6-07a6714f89c7',\n",
       " 54: '99a1beb1-370c-44ef-ad66-a2b851ece2a7',\n",
       " 55: 'e5baa7dc-a145-4fff-890a-a304b065b51c',\n",
       " 56: '9169927a-7ac7-4ab2-9d19-006fcadd5a44',\n",
       " 57: 'b33dcfd8-7203-4172-9975-169ea82c0484',\n",
       " 58: '0ac5f321-6447-4d17-a818-6afa28b4d0a1',\n",
       " 59: '3208414c-65bd-4b5f-960d-a6133589b2f4',\n",
       " 60: 'a733cdc1-23af-405b-ae28-b9e3c89501dc',\n",
       " 61: 'b19e7a24-0ccb-4180-9f60-2c5664e4dc5c',\n",
       " 62: '0d47370c-9bc9-48f6-aab2-fe73ca988672',\n",
       " 63: '2bd83aea-291e-4231-8bbe-ded3be339265',\n",
       " 64: 'a8332526-aed3-447d-b513-87d5fd68d621',\n",
       " 65: '40f30a77-ca71-4b0e-aa81-b7dbd2ad1d49',\n",
       " 66: '6ced79ee-6d2d-40c0-b2e0-5caac59e6c42',\n",
       " 67: '2113856a-0b11-49ba-85ff-c547775f3f67',\n",
       " 68: '4b4e2661-d3a3-4ed3-89ab-68a34963cb97',\n",
       " 69: 'e89ccbf6-2659-4b09-a625-d0dd31ba266e',\n",
       " 70: '1264dd27-ce64-470f-834a-18585d8c9801',\n",
       " 71: '6b9235c5-bd63-4bb3-ba8b-86d34e445369',\n",
       " 72: '4b6ea75e-3d06-4d3f-acc2-9e97b535b722',\n",
       " 73: '8c992d3c-40d4-449c-90a4-1cab87d0ec3e',\n",
       " 74: '7124eb1c-678f-4d1a-8799-2a296b43850a',\n",
       " 75: '809c9fd5-8917-47ce-8ab6-e9fb1ba63b18',\n",
       " 76: '4eea10a7-1db8-4541-a03c-d68b3f7b9e1e',\n",
       " 77: 'c952ed8c-a0b5-4236-bded-3d8565ae0277',\n",
       " 78: '4d27c03d-2124-4172-b205-634184db2e33',\n",
       " 79: '30dd4404-9641-41e7-adc4-9bc0dd04ffbc',\n",
       " 80: 'be3279b1-44fe-43af-9ec9-4fc8d3ff673e',\n",
       " 81: '0c786c0b-da53-41db-954f-3ba60f3f9e11',\n",
       " 82: '33b73421-0022-4b74-9973-6d2553b8d3a2',\n",
       " 83: '612b2580-8727-48cf-82fb-b6854a9be552',\n",
       " 84: '3ebd04b3-ffbf-4f2f-a95b-7de6141d4524',\n",
       " 85: 'fdf2b528-3ef3-43d7-903e-e188b03d4c1e',\n",
       " 86: '02843bdd-084f-4451-8d88-94cd6b6fb80b',\n",
       " 87: '306f2a10-13ec-4e6c-a3b0-f022cfb926d2',\n",
       " 88: 'c01669c2-2bc5-48c3-9c24-adb090625df9',\n",
       " 89: '8cbf2447-4a6c-4632-b3be-9d310c8e59b2',\n",
       " 90: '714b8dc6-cb16-4214-8903-3c1a4ad0a016',\n",
       " 91: '684946b6-0fb5-455e-88f0-b36a06d6c61c',\n",
       " 92: '86818b79-423a-4621-a76b-58cf60b8af52',\n",
       " 93: 'ed63e277-202e-429a-aa5c-802b4e9b3367',\n",
       " 94: '47b4939c-f63a-465d-abe1-6e21a6d9e981',\n",
       " 95: 'a51b8051-ba5f-4209-bcd4-d04713db094c',\n",
       " 96: 'fd5f80cb-2b0f-466f-bd40-dbc914397051',\n",
       " 97: '56985056-dd31-4de5-a3da-eee7a479eff1',\n",
       " 98: 'ca4b6223-09a7-4e89-9c0b-efea779eedb6',\n",
       " 99: '6b8ef83e-d1e2-4daa-9e40-103b9f2a0c9e',\n",
       " 100: 'aa77b1ba-4b3b-41a6-968c-6ed804f9e082'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'35c756fa-5646-4bf4-ad91-31f2dee028cf': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Semantic Search'),\n",
       " 'a32a89e4-d5f4-4141-84c9-950d0d9ae6ac': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.'),\n",
       " '73267bc4-3e76-4b76-a323-ad7267d9a563': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝'),\n",
       " '667d6357-cc93-44d9-9667-3f96e4897cbd': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Embedding'),\n",
       " '5e4cf004-c056-4093-9b87-066dcbab47e7': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.'),\n",
       " 'df6b6958-4085-4b1b-86af-1be4e27678fd': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝'),\n",
       " '3e60fb1a-7b03-485e-a651-60d958ffccd1': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Token'),\n",
       " 'c8a577ab-0f59-4cc6-bc85-d9ed61fb13ee': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.'),\n",
       " '565ff0c9-a90a-49d6-96d0-349ad4bd4a91': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\\n연관키워드: 토큰화, 자연어 처리, 구문 분석'),\n",
       " 'd139ae87-6d97-4671-9d09-5b05e07c5d98': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Tokenizer'),\n",
       " '9fb8e6ca-ec3c-4e59-84ec-8e08a9056791': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.'),\n",
       " '878134d3-43b8-4634-9f91-98804dc755dd': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다.'),\n",
       " '231bfa99-8e6b-4b6d-be3d-527250846beb': Document(metadata={'source': './nlp-keywords.txt'}, page_content='연관키워드: 토큰화, 자연어 처리, 구문 분석'),\n",
       " '85ae10ed-6e7b-402b-ac4d-a0b18ac32e5d': Document(metadata={'source': './nlp-keywords.txt'}, page_content='VectorStore'),\n",
       " '1ae7709f-96a8-4c12-827b-886692d71476': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.'),\n",
       " '29218b7f-0605-4085-90f1-3883d2647de6': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다.\\n연관키워드: 임베딩, 데이터베이스, 벡터화'),\n",
       " '4614882d-291f-467a-93bc-42c7f5bd085e': Document(metadata={'source': './nlp-keywords.txt'}, page_content='SQL'),\n",
       " '93c36785-a729-4ed4-952e-306c35207f77': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을'),\n",
       " 'be3030a9-39a2-4e46-8764-816c7cd74466': Document(metadata={'source': './nlp-keywords.txt'}, page_content='수행할 수 있습니다.'),\n",
       " '6cb1552e-b55c-411e-ab87-019b24109110': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다.\\n연관키워드: 데이터베이스, 쿼리, 데이터 관리'),\n",
       " 'e6e30d4c-0bdb-4167-9ab1-ec38c6f81796': Document(metadata={'source': './nlp-keywords.txt'}, page_content='CSV'),\n",
       " '8fc1645e-9569-4937-a6f6-86de671b8f89': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때'),\n",
       " '507f6141-6daa-4e6f-bda0-f767c38d9411': Document(metadata={'source': './nlp-keywords.txt'}, page_content='사용됩니다.'),\n",
       " '051e54f7-cb7c-44d3-8207-00c8ed4f89a4': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다.\\n연관키워드: 데이터 형식, 파일 처리, 데이터 교환'),\n",
       " 'dd8c1cde-8e41-4eee-9a49-4ef98b9099da': Document(metadata={'source': './nlp-keywords.txt'}, page_content='JSON'),\n",
       " '365c01f8-b5f0-44cf-8ba3-aa94af282af3': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.'),\n",
       " '79a08136-a276-4bb3-8346-c2fc8a806566': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\\n연관키워드: 데이터 교환, 웹 개발, API'),\n",
       " '6e0c99ed-325e-41c8-8b59-c125d5602ffd': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Transformer'),\n",
       " '535334a5-a47c-42ab-a662-08be99c91194': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.'),\n",
       " 'f0a906e5-3391-48da-82fd-a4dfad12704b': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다.\\n연관키워드: 딥러닝, 자연어 처리, Attention'),\n",
       " '3be02ddd-c008-4798-95eb-26885f216c5e': Document(metadata={'source': './nlp-keywords.txt'}, page_content='HuggingFace'),\n",
       " '28548b9e-6751-4a99-a8f4-e36f350922ea': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록'),\n",
       " '7f9a01ac-841c-42be-a2da-bbf2828b3f83': Document(metadata={'source': './nlp-keywords.txt'}, page_content='돕습니다.'),\n",
       " 'd272bcd9-1a4e-49aa-bd67-6069ceb48807': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.'),\n",
       " 'bf004e67-0dad-4477-928d-41f7926ca85c': Document(metadata={'source': './nlp-keywords.txt'}, page_content='연관키워드: 자연어 처리, 딥러닝, 라이브러리'),\n",
       " '90a795e6-4442-451f-8b91-de8af0279794': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Digital Transformation'),\n",
       " '52d96a85-3548-4f81-97a7-5741f245256f': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.'),\n",
       " 'bc42c894-1352-4ef6-9a00-c6ba8cdf69fc': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다.\\n연관키워드: 혁신, 기술, 비즈니스 모델'),\n",
       " 'bfe6ae24-77c8-4427-a585-858eebfc3557': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Crawling'),\n",
       " '73434bcb-6130-439c-93b0-4da3b808f7f5': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.'),\n",
       " 'c85fbcdd-204e-40dc-a14e-bd6ba6565c58': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\\n연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진'),\n",
       " '846ec9c6-69ad-407d-89aa-f938206a61e4': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Word2Vec'),\n",
       " 'a110482f-3619-4b51-abb1-6d3ff89f4f04': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.'),\n",
       " 'a0eba335-606d-42f9-b622-b2a6152e3c60': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성'),\n",
       " 'b41cc867-82c2-4138-baec-ae35c938a0c0': Document(metadata={'source': './nlp-keywords.txt'}, page_content='LLM (Large Language Model)'),\n",
       " '54a46322-7085-42d0-bdd2-5f2c5031a494': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: LLM은 대규모의 텍스트 데이터로 훈련된 큰 규모의 언어 모델을 의미합니다. 이러한 모델은 다양한 자연어 이해 및 생성 작업에 사용됩니다.'),\n",
       " 'a498bf8d-e00a-48f8-9083-2a8ec27652f9': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: OpenAI의 GPT 시리즈는 대표적인 대규모 언어 모델입니다.\\n연관키워드: 자연어 처리, 딥러닝, 텍스트 생성'),\n",
       " '6f3380a5-32bb-44f2-93c1-9e618cdf8fad': Document(metadata={'source': './nlp-keywords.txt'}, page_content='FAISS (Facebook AI Similarity Search)'),\n",
       " '6805af36-68ec-4aab-ad80-8aeaae541c96': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.'),\n",
       " 'd6fa1d32-bb0d-4898-a6a9-4428e89e0b7a': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\\n연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화'),\n",
       " '213b8eea-d01a-4645-b0c3-d80fb51d712a': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Open Source'),\n",
       " 'cd1f01f5-0ee4-4097-a42f-d9c1ce371077': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 오픈 소스는 소스 코드가 공개되어 누구나 자유롭게 사용, 수정, 배포할 수 있는 소프트웨어를 의미합니다. 이는 협업과 혁신을 촉진하는 데 중요한 역할을 합니다.'),\n",
       " '40024248-52cf-4fe2-a6a2-11ba7aa6c3f8': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 리눅스 운영 체제는 대표적인 오픈 소스 프로젝트입니다.\\n연관키워드: 소프트웨어 개발, 커뮤니티, 기술 협업'),\n",
       " 'b73d0f3f-9e87-441f-bdf6-07a6714f89c7': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Structured Data'),\n",
       " '99a1beb1-370c-44ef-ad66-a2b851ece2a7': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 구조화된 데이터는 정해진 형식이나 스키마에 따라 조직된 데이터입니다. 이는 데이터베이스, 스프레드시트 등에서 쉽게 검색하고 분석할 수 있습니다.'),\n",
       " 'e5baa7dc-a145-4fff-890a-a304b065b51c': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 관계형 데이터베이스에 저장된 고객 정보 테이블은 구조화된 데이터의 예입니다.\\n연관키워드: 데이터베이스, 데이터 분석, 데이터 모델링'),\n",
       " '9169927a-7ac7-4ab2-9d19-006fcadd5a44': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Parser'),\n",
       " 'b33dcfd8-7203-4172-9975-169ea82c0484': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 파서는 주어진 데이터(문자열, 파일 등)를 분석하여 구조화된 형태로 변환하는 도구입니다. 이는 프로그래밍 언어의 구문 분석이나 파일 데이터 처리에 사용됩니다.'),\n",
       " '0ac5f321-6447-4d17-a818-6afa28b4d0a1': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: HTML 문서를 구문 분석하여 웹 페이지의 DOM 구조를 생성하는 것은 파싱의 한 예입니다.\\n연관키워드: 구문 분석, 컴파일러, 데이터 처리'),\n",
       " '3208414c-65bd-4b5f-960d-a6133589b2f4': Document(metadata={'source': './nlp-keywords.txt'}, page_content='TF-IDF (Term Frequency-Inverse Document Frequency)'),\n",
       " 'a733cdc1-23af-405b-ae28-b9e3c89501dc': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.'),\n",
       " 'b19e7a24-0ccb-4180-9f60-2c5664e4dc5c': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝'),\n",
       " '0d47370c-9bc9-48f6-aab2-fe73ca988672': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Deep Learning'),\n",
       " '2bd83aea-291e-4231-8bbe-ded3be339265': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 딥러닝은 인공신경망을 이용하여 복잡한 문제를 해결하는 머신러닝의 한 분야입니다. 이는 데이터에서 고수준의 표현을 학습하는 데 중점을 둡니다.'),\n",
       " 'a8332526-aed3-447d-b513-87d5fd68d621': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 이미지 인식, 음성 인식, 자연어 처리 등에서 딥러닝 모델이 활용됩니다.\\n연관키워드: 인공신경망, 머신러닝, 데이터 분석'),\n",
       " '40f30a77-ca71-4b0e-aa81-b7dbd2ad1d49': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Schema'),\n",
       " '6ced79ee-6d2d-40c0-b2e0-5caac59e6c42': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 스키마는 데이터베이스나 파일의 구조를 정의하는 것으로, 데이터가 어떻게 저장되고 조직되는지에 대한 청사진을 제공합니다.'),\n",
       " '2113856a-0b11-49ba-85ff-c547775f3f67': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 관계형 데이터베이스의 테이블 스키마는 열 이름, 데이터 타입, 키 제약 조건 등을 정의합니다.\\n연관키워드: 데이터베이스, 데이터 모델링, 데이터 관리'),\n",
       " '4b4e2661-d3a3-4ed3-89ab-68a34963cb97': Document(metadata={'source': './nlp-keywords.txt'}, page_content='DataFrame'),\n",
       " 'e89ccbf6-2659-4b09-a625-d0dd31ba266e': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: DataFrame은 행과 열로 이루어진 테이블 형태의 데이터 구조로, 주로 데이터 분석 및 처리에 사용됩니다.'),\n",
       " '1264dd27-ce64-470f-834a-18585d8c9801': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 판다스 라이브러리에서 DataFrame은 다양한 데이터 타입의 열을 가질 수 있으며, 데이터 조작과 분석을 용이하게 합니다.'),\n",
       " '6b9235c5-bd63-4bb3-ba8b-86d34e445369': Document(metadata={'source': './nlp-keywords.txt'}, page_content='연관키워드: 데이터 분석, 판다스, 데이터 처리'),\n",
       " '4b6ea75e-3d06-4d3f-acc2-9e97b535b722': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Attention 메커니즘'),\n",
       " '8c992d3c-40d4-449c-90a4-1cab87d0ec3e': Document(metadata={'source': './nlp-keywords.txt'}, page_content=\"정의: Attention 메커니즘은 딥러닝에서 중요한 정보에 더 많은 '주의'를 기울이도록 하는 기법입니다. 이는 주로 시퀀스 데이터(예: 텍스트, 시계열 데이터)에서\"),\n",
       " '7124eb1c-678f-4d1a-8799-2a296b43850a': Document(metadata={'source': './nlp-keywords.txt'}, page_content='사용됩니다.'),\n",
       " '809c9fd5-8917-47ce-8ab6-e9fb1ba63b18': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 번역 모델에서 Attention 메커니즘은 입력 문장의 중요한 부분에 더 집중하여 정확한 번역을 생성합니다.\\n연관키워드: 딥러닝, 자연어 처리, 시퀀스 모델링'),\n",
       " '4eea10a7-1db8-4541-a03c-d68b3f7b9e1e': Document(metadata={'source': './nlp-keywords.txt'}, page_content='판다스 (Pandas)'),\n",
       " 'c952ed8c-a0b5-4236-bded-3d8565ae0277': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 판다스는 파이썬 프로그래밍 언어를 위한 데이터 분석 및 조작 도구를 제공하는 라이브러리입니다. 이는 데이터 분석 작업을 효율적으로 수행할 수 있게 합니다.'),\n",
       " '4d27c03d-2124-4172-b205-634184db2e33': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 판다스를 사용하여 CSV 파일을 읽고, 데이터를 정제하며, 다양한 분석을 수행할 수 있습니다.\\n연관키워드: 데이터 분석, 파이썬, 데이터 처리'),\n",
       " '30dd4404-9641-41e7-adc4-9bc0dd04ffbc': Document(metadata={'source': './nlp-keywords.txt'}, page_content='GPT (Generative Pretrained Transformer)'),\n",
       " 'be3279b1-44fe-43af-9ec9-4fc8d3ff673e': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: GPT는 대규모의 데이터셋으로 사전 훈련된 생성적 언어 모델로, 다양한 텍스트 기반 작업에 활용됩니다. 이는 입력된 텍스트에 기반하여 자연스러운 언어를 생성할 수'),\n",
       " '0c786c0b-da53-41db-954f-3ba60f3f9e11': Document(metadata={'source': './nlp-keywords.txt'}, page_content='있습니다.'),\n",
       " '33b73421-0022-4b74-9973-6d2553b8d3a2': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 사용자가 제공한 질문에 대해 자세한 답변을 생성하는 챗봇은 GPT 모델을 사용할 수 있습니다.\\n연관키워드: 자연어 처리, 텍스트 생성, 딥러닝'),\n",
       " '612b2580-8727-48cf-82fb-b6854a9be552': Document(metadata={'source': './nlp-keywords.txt'}, page_content='InstructGPT'),\n",
       " '3ebd04b3-ffbf-4f2f-a95b-7de6141d4524': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: InstructGPT는 사용자의 지시에 따라 특정한 작업을 수행하기 위해 최적화된 GPT 모델입니다. 이 모델은 보다 정확하고 관련성 높은 결과를 생성하도록'),\n",
       " 'fdf2b528-3ef3-43d7-903e-e188b03d4c1e': Document(metadata={'source': './nlp-keywords.txt'}, page_content='설계되었습니다.'),\n",
       " '02843bdd-084f-4451-8d88-94cd6b6fb80b': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 사용자가 \"이메일 초안 작성\"과 같은 특정 지시를 제공하면, InstructGPT는 관련 내용을 기반으로 이메일을 작성합니다.'),\n",
       " '306f2a10-13ec-4e6c-a3b0-f022cfb926d2': Document(metadata={'source': './nlp-keywords.txt'}, page_content='연관키워드: 인공지능, 자연어 이해, 명령 기반 처리'),\n",
       " 'c01669c2-2bc5-48c3-9c24-adb090625df9': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Keyword Search'),\n",
       " '8cbf2447-4a6c-4632-b3be-9d310c8e59b2': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.'),\n",
       " '714b8dc6-cb16-4214-8903-3c1a4ad0a016': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\\n연관키워드: 검색 엔진, 데이터 검색, 정보 검색'),\n",
       " '684946b6-0fb5-455e-88f0-b36a06d6c61c': Document(metadata={'source': './nlp-keywords.txt'}, page_content='Page Rank'),\n",
       " '86818b79-423a-4621-a76b-58cf60b8af52': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.'),\n",
       " 'ed63e277-202e-429a-aa5c-802b4e9b3367': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\\n연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석'),\n",
       " '47b4939c-f63a-465d-abe1-6e21a6d9e981': Document(metadata={'source': './nlp-keywords.txt'}, page_content='데이터 마이닝'),\n",
       " 'a51b8051-ba5f-4209-bcd4-d04713db094c': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 데이터 마이닝은 대량의 데이터에서 유용한 정보를 발굴하는 과정입니다. 이는 통계, 머신러닝, 패턴 인식 등의 기술을 활용합니다.'),\n",
       " 'fd5f80cb-2b0f-466f-bd40-dbc914397051': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 소매업체가 고객 구매 데이터를 분석하여 판매 전략을 수립하는 것은 데이터 마이닝의 예입니다.\\n연관키워드: 빅데이터, 패턴 인식, 예측 분석'),\n",
       " '56985056-dd31-4de5-a3da-eee7a479eff1': Document(metadata={'source': './nlp-keywords.txt'}, page_content='멀티모달 (Multimodal)'),\n",
       " 'ca4b6223-09a7-4e89-9c0b-efea779eedb6': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 멀티모달은 여러 종류의 데이터 모드(예: 텍스트, 이미지, 소리 등)를 결합하여 처리하는 기술입니다. 이는 서로 다른 형식의 데이터 간의 상호 작용을 통해 보다 풍부하고'),\n",
       " '6b8ef83e-d1e2-4daa-9e40-103b9f2a0c9e': Document(metadata={'source': './nlp-keywords.txt'}, page_content='정확한 정보를 추출하거나 예측하는 데 사용됩니다.'),\n",
       " 'aa77b1ba-4b3b-41a6-968c-6ed804f9e082': Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 이미지와 설명 텍스트를 함께 분석하여 더 정확한 이미지 분류를 수행하는 시스템은 멀티모달 기술의 예입니다.\\n연관키워드: 데이터 융합, 인공지능, 딥러닝')}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.docstore._dict # _dict : 모든 데이터 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = FAISS.from_texts(\n",
    "    ['안녕하세요 반갑습니다.', '제 이름은 수빈입니다'],\n",
    "    embedding = OpenAIEmbeddings(),\n",
    "    metadatas = [{'source' : '텍스트 문서1'}, {'source' : '텍스트 문서2'}],\n",
    "    ids = ['doc1', 'doc2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc1': Document(metadata={'source': '텍스트 문서1'}, page_content='안녕하세요 반갑습니다.'),\n",
       " 'doc2': Document(metadata={'source': '텍스트 문서2'}, page_content='제 이름은 수빈입니다')}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db2.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='TF-IDF (Term Frequency-Inverse Document Frequency)'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다.')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search('TF IDF 가 뭐야?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='SQL')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search('SQL에 대해 알려줘', k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: TF-IDF는 문서 내에서 단어의 중요도를 평가하는 데 사용되는 통계적 척도입니다. 이는 문서 내 단어의 빈도와 전체 문서 집합에서 그 단어의 희소성을 고려합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: 많은 문서에서 자주 등장하지 않는 단어는 높은 TF-IDF 값을 가집니다.\\n연관키워드: 자연어 처리, 정보 검색, 데이터 마이닝')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search('TF IDF에 대해 알려줘', filter = {'source' : './nlp-keywords.txt'}, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search('TF IDF에 대해 알려줘', filter = {'source' : './finance-keywords.txt'}, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_doc1']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터를 추가하고 삭제하는 방법\n",
    "from langchain_core.documents import Document\n",
    "db.add_documents(\n",
    "    [\n",
    "        Document(\n",
    "            page_content = '좋습니다. 이번엔 새로 document를 추가해 볼게요',\n",
    "            metadata = {'sorce' : 'mydata.txt'}\n",
    "        )\n",
    "    ],\n",
    "    ids = ['new_doc1']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'sorce': 'mydata.txt'}, page_content='좋습니다. 이번엔 새로 document를 추가해 볼게요')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search('좋습니다', k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_doc2', 'new_doc3']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.add_texts(\n",
    "    ['이번엔 텍스트 데이터를 추가합니다','추가한 2번째 텍스트 데이터 입니다'],\n",
    "    metadatas = [{'source' : 'mydata.txt'}, {'source' : 'mydata.txt'}],\n",
    "    ids = ['new_doc2', 'new_doc3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '35c756fa-5646-4bf4-ad91-31f2dee028cf',\n",
       " 1: 'a32a89e4-d5f4-4141-84c9-950d0d9ae6ac',\n",
       " 2: '73267bc4-3e76-4b76-a323-ad7267d9a563',\n",
       " 3: '667d6357-cc93-44d9-9667-3f96e4897cbd',\n",
       " 4: '5e4cf004-c056-4093-9b87-066dcbab47e7',\n",
       " 5: 'df6b6958-4085-4b1b-86af-1be4e27678fd',\n",
       " 6: '3e60fb1a-7b03-485e-a651-60d958ffccd1',\n",
       " 7: 'c8a577ab-0f59-4cc6-bc85-d9ed61fb13ee',\n",
       " 8: '565ff0c9-a90a-49d6-96d0-349ad4bd4a91',\n",
       " 9: 'd139ae87-6d97-4671-9d09-5b05e07c5d98',\n",
       " 10: '9fb8e6ca-ec3c-4e59-84ec-8e08a9056791',\n",
       " 11: '878134d3-43b8-4634-9f91-98804dc755dd',\n",
       " 12: '231bfa99-8e6b-4b6d-be3d-527250846beb',\n",
       " 13: '85ae10ed-6e7b-402b-ac4d-a0b18ac32e5d',\n",
       " 14: '1ae7709f-96a8-4c12-827b-886692d71476',\n",
       " 15: '29218b7f-0605-4085-90f1-3883d2647de6',\n",
       " 16: '4614882d-291f-467a-93bc-42c7f5bd085e',\n",
       " 17: '93c36785-a729-4ed4-952e-306c35207f77',\n",
       " 18: 'be3030a9-39a2-4e46-8764-816c7cd74466',\n",
       " 19: '6cb1552e-b55c-411e-ab87-019b24109110',\n",
       " 20: 'e6e30d4c-0bdb-4167-9ab1-ec38c6f81796',\n",
       " 21: '8fc1645e-9569-4937-a6f6-86de671b8f89',\n",
       " 22: '507f6141-6daa-4e6f-bda0-f767c38d9411',\n",
       " 23: '051e54f7-cb7c-44d3-8207-00c8ed4f89a4',\n",
       " 24: 'dd8c1cde-8e41-4eee-9a49-4ef98b9099da',\n",
       " 25: '365c01f8-b5f0-44cf-8ba3-aa94af282af3',\n",
       " 26: '79a08136-a276-4bb3-8346-c2fc8a806566',\n",
       " 27: '6e0c99ed-325e-41c8-8b59-c125d5602ffd',\n",
       " 28: '535334a5-a47c-42ab-a662-08be99c91194',\n",
       " 29: 'f0a906e5-3391-48da-82fd-a4dfad12704b',\n",
       " 30: '3be02ddd-c008-4798-95eb-26885f216c5e',\n",
       " 31: '28548b9e-6751-4a99-a8f4-e36f350922ea',\n",
       " 32: '7f9a01ac-841c-42be-a2da-bbf2828b3f83',\n",
       " 33: 'd272bcd9-1a4e-49aa-bd67-6069ceb48807',\n",
       " 34: 'bf004e67-0dad-4477-928d-41f7926ca85c',\n",
       " 35: '90a795e6-4442-451f-8b91-de8af0279794',\n",
       " 36: '52d96a85-3548-4f81-97a7-5741f245256f',\n",
       " 37: 'bc42c894-1352-4ef6-9a00-c6ba8cdf69fc',\n",
       " 38: 'bfe6ae24-77c8-4427-a585-858eebfc3557',\n",
       " 39: '73434bcb-6130-439c-93b0-4da3b808f7f5',\n",
       " 40: 'c85fbcdd-204e-40dc-a14e-bd6ba6565c58',\n",
       " 41: '846ec9c6-69ad-407d-89aa-f938206a61e4',\n",
       " 42: 'a110482f-3619-4b51-abb1-6d3ff89f4f04',\n",
       " 43: 'a0eba335-606d-42f9-b622-b2a6152e3c60',\n",
       " 44: 'b41cc867-82c2-4138-baec-ae35c938a0c0',\n",
       " 45: '54a46322-7085-42d0-bdd2-5f2c5031a494',\n",
       " 46: 'a498bf8d-e00a-48f8-9083-2a8ec27652f9',\n",
       " 47: '6f3380a5-32bb-44f2-93c1-9e618cdf8fad',\n",
       " 48: '6805af36-68ec-4aab-ad80-8aeaae541c96',\n",
       " 49: 'd6fa1d32-bb0d-4898-a6a9-4428e89e0b7a',\n",
       " 50: '213b8eea-d01a-4645-b0c3-d80fb51d712a',\n",
       " 51: 'cd1f01f5-0ee4-4097-a42f-d9c1ce371077',\n",
       " 52: '40024248-52cf-4fe2-a6a2-11ba7aa6c3f8',\n",
       " 53: 'b73d0f3f-9e87-441f-bdf6-07a6714f89c7',\n",
       " 54: '99a1beb1-370c-44ef-ad66-a2b851ece2a7',\n",
       " 55: 'e5baa7dc-a145-4fff-890a-a304b065b51c',\n",
       " 56: '9169927a-7ac7-4ab2-9d19-006fcadd5a44',\n",
       " 57: 'b33dcfd8-7203-4172-9975-169ea82c0484',\n",
       " 58: '0ac5f321-6447-4d17-a818-6afa28b4d0a1',\n",
       " 59: '3208414c-65bd-4b5f-960d-a6133589b2f4',\n",
       " 60: 'a733cdc1-23af-405b-ae28-b9e3c89501dc',\n",
       " 61: 'b19e7a24-0ccb-4180-9f60-2c5664e4dc5c',\n",
       " 62: '0d47370c-9bc9-48f6-aab2-fe73ca988672',\n",
       " 63: '2bd83aea-291e-4231-8bbe-ded3be339265',\n",
       " 64: 'a8332526-aed3-447d-b513-87d5fd68d621',\n",
       " 65: '40f30a77-ca71-4b0e-aa81-b7dbd2ad1d49',\n",
       " 66: '6ced79ee-6d2d-40c0-b2e0-5caac59e6c42',\n",
       " 67: '2113856a-0b11-49ba-85ff-c547775f3f67',\n",
       " 68: '4b4e2661-d3a3-4ed3-89ab-68a34963cb97',\n",
       " 69: 'e89ccbf6-2659-4b09-a625-d0dd31ba266e',\n",
       " 70: '1264dd27-ce64-470f-834a-18585d8c9801',\n",
       " 71: '6b9235c5-bd63-4bb3-ba8b-86d34e445369',\n",
       " 72: '4b6ea75e-3d06-4d3f-acc2-9e97b535b722',\n",
       " 73: '8c992d3c-40d4-449c-90a4-1cab87d0ec3e',\n",
       " 74: '7124eb1c-678f-4d1a-8799-2a296b43850a',\n",
       " 75: '809c9fd5-8917-47ce-8ab6-e9fb1ba63b18',\n",
       " 76: '4eea10a7-1db8-4541-a03c-d68b3f7b9e1e',\n",
       " 77: 'c952ed8c-a0b5-4236-bded-3d8565ae0277',\n",
       " 78: '4d27c03d-2124-4172-b205-634184db2e33',\n",
       " 79: '30dd4404-9641-41e7-adc4-9bc0dd04ffbc',\n",
       " 80: 'be3279b1-44fe-43af-9ec9-4fc8d3ff673e',\n",
       " 81: '0c786c0b-da53-41db-954f-3ba60f3f9e11',\n",
       " 82: '33b73421-0022-4b74-9973-6d2553b8d3a2',\n",
       " 83: '612b2580-8727-48cf-82fb-b6854a9be552',\n",
       " 84: '3ebd04b3-ffbf-4f2f-a95b-7de6141d4524',\n",
       " 85: 'fdf2b528-3ef3-43d7-903e-e188b03d4c1e',\n",
       " 86: '02843bdd-084f-4451-8d88-94cd6b6fb80b',\n",
       " 87: '306f2a10-13ec-4e6c-a3b0-f022cfb926d2',\n",
       " 88: 'c01669c2-2bc5-48c3-9c24-adb090625df9',\n",
       " 89: '8cbf2447-4a6c-4632-b3be-9d310c8e59b2',\n",
       " 90: '714b8dc6-cb16-4214-8903-3c1a4ad0a016',\n",
       " 91: '684946b6-0fb5-455e-88f0-b36a06d6c61c',\n",
       " 92: '86818b79-423a-4621-a76b-58cf60b8af52',\n",
       " 93: 'ed63e277-202e-429a-aa5c-802b4e9b3367',\n",
       " 94: '47b4939c-f63a-465d-abe1-6e21a6d9e981',\n",
       " 95: 'a51b8051-ba5f-4209-bcd4-d04713db094c',\n",
       " 96: 'fd5f80cb-2b0f-466f-bd40-dbc914397051',\n",
       " 97: '56985056-dd31-4de5-a3da-eee7a479eff1',\n",
       " 98: 'ca4b6223-09a7-4e89-9c0b-efea779eedb6',\n",
       " 99: '6b8ef83e-d1e2-4daa-9e40-103b9f2a0c9e',\n",
       " 100: 'aa77b1ba-4b3b-41a6-968c-6ed804f9e082',\n",
       " 101: 'cf2bdc69-2011-4073-abad-4019483b5cfb',\n",
       " 102: 'new_doc1',\n",
       " 103: 'new_doc2',\n",
       " 104: 'new_doc3'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index_to_docstore_id # 맨 밑에 확인해 보면 new_doc1,2,3 이 추가 된 것을 확인가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_ids = db.add_texts(\n",
    "    ['이번엔 텍스트 데이터를 추가합니다','추가한 2번째 텍스트 데이터 입니다'],\n",
    "    metadatas = [{'source' : 'mydata.txt'}, {'source' : 'mydata.txt'}],\n",
    "    ids = ['del_doc2', 'del_doc3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['del_doc2', 'del_doc3']\n"
     ]
    }
   ],
   "source": [
    "print(del_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.delete(del_ids) # 잘 삭제되면 True가 출력 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '35c756fa-5646-4bf4-ad91-31f2dee028cf',\n",
       " 1: 'a32a89e4-d5f4-4141-84c9-950d0d9ae6ac',\n",
       " 2: '73267bc4-3e76-4b76-a323-ad7267d9a563',\n",
       " 3: '667d6357-cc93-44d9-9667-3f96e4897cbd',\n",
       " 4: '5e4cf004-c056-4093-9b87-066dcbab47e7',\n",
       " 5: 'df6b6958-4085-4b1b-86af-1be4e27678fd',\n",
       " 6: '3e60fb1a-7b03-485e-a651-60d958ffccd1',\n",
       " 7: 'c8a577ab-0f59-4cc6-bc85-d9ed61fb13ee',\n",
       " 8: '565ff0c9-a90a-49d6-96d0-349ad4bd4a91',\n",
       " 9: 'd139ae87-6d97-4671-9d09-5b05e07c5d98',\n",
       " 10: '9fb8e6ca-ec3c-4e59-84ec-8e08a9056791',\n",
       " 11: '878134d3-43b8-4634-9f91-98804dc755dd',\n",
       " 12: '231bfa99-8e6b-4b6d-be3d-527250846beb',\n",
       " 13: '85ae10ed-6e7b-402b-ac4d-a0b18ac32e5d',\n",
       " 14: '1ae7709f-96a8-4c12-827b-886692d71476',\n",
       " 15: '29218b7f-0605-4085-90f1-3883d2647de6',\n",
       " 16: '4614882d-291f-467a-93bc-42c7f5bd085e',\n",
       " 17: '93c36785-a729-4ed4-952e-306c35207f77',\n",
       " 18: 'be3030a9-39a2-4e46-8764-816c7cd74466',\n",
       " 19: '6cb1552e-b55c-411e-ab87-019b24109110',\n",
       " 20: 'e6e30d4c-0bdb-4167-9ab1-ec38c6f81796',\n",
       " 21: '8fc1645e-9569-4937-a6f6-86de671b8f89',\n",
       " 22: '507f6141-6daa-4e6f-bda0-f767c38d9411',\n",
       " 23: '051e54f7-cb7c-44d3-8207-00c8ed4f89a4',\n",
       " 24: 'dd8c1cde-8e41-4eee-9a49-4ef98b9099da',\n",
       " 25: '365c01f8-b5f0-44cf-8ba3-aa94af282af3',\n",
       " 26: '79a08136-a276-4bb3-8346-c2fc8a806566',\n",
       " 27: '6e0c99ed-325e-41c8-8b59-c125d5602ffd',\n",
       " 28: '535334a5-a47c-42ab-a662-08be99c91194',\n",
       " 29: 'f0a906e5-3391-48da-82fd-a4dfad12704b',\n",
       " 30: '3be02ddd-c008-4798-95eb-26885f216c5e',\n",
       " 31: '28548b9e-6751-4a99-a8f4-e36f350922ea',\n",
       " 32: '7f9a01ac-841c-42be-a2da-bbf2828b3f83',\n",
       " 33: 'd272bcd9-1a4e-49aa-bd67-6069ceb48807',\n",
       " 34: 'bf004e67-0dad-4477-928d-41f7926ca85c',\n",
       " 35: '90a795e6-4442-451f-8b91-de8af0279794',\n",
       " 36: '52d96a85-3548-4f81-97a7-5741f245256f',\n",
       " 37: 'bc42c894-1352-4ef6-9a00-c6ba8cdf69fc',\n",
       " 38: 'bfe6ae24-77c8-4427-a585-858eebfc3557',\n",
       " 39: '73434bcb-6130-439c-93b0-4da3b808f7f5',\n",
       " 40: 'c85fbcdd-204e-40dc-a14e-bd6ba6565c58',\n",
       " 41: '846ec9c6-69ad-407d-89aa-f938206a61e4',\n",
       " 42: 'a110482f-3619-4b51-abb1-6d3ff89f4f04',\n",
       " 43: 'a0eba335-606d-42f9-b622-b2a6152e3c60',\n",
       " 44: 'b41cc867-82c2-4138-baec-ae35c938a0c0',\n",
       " 45: '54a46322-7085-42d0-bdd2-5f2c5031a494',\n",
       " 46: 'a498bf8d-e00a-48f8-9083-2a8ec27652f9',\n",
       " 47: '6f3380a5-32bb-44f2-93c1-9e618cdf8fad',\n",
       " 48: '6805af36-68ec-4aab-ad80-8aeaae541c96',\n",
       " 49: 'd6fa1d32-bb0d-4898-a6a9-4428e89e0b7a',\n",
       " 50: '213b8eea-d01a-4645-b0c3-d80fb51d712a',\n",
       " 51: 'cd1f01f5-0ee4-4097-a42f-d9c1ce371077',\n",
       " 52: '40024248-52cf-4fe2-a6a2-11ba7aa6c3f8',\n",
       " 53: 'b73d0f3f-9e87-441f-bdf6-07a6714f89c7',\n",
       " 54: '99a1beb1-370c-44ef-ad66-a2b851ece2a7',\n",
       " 55: 'e5baa7dc-a145-4fff-890a-a304b065b51c',\n",
       " 56: '9169927a-7ac7-4ab2-9d19-006fcadd5a44',\n",
       " 57: 'b33dcfd8-7203-4172-9975-169ea82c0484',\n",
       " 58: '0ac5f321-6447-4d17-a818-6afa28b4d0a1',\n",
       " 59: '3208414c-65bd-4b5f-960d-a6133589b2f4',\n",
       " 60: 'a733cdc1-23af-405b-ae28-b9e3c89501dc',\n",
       " 61: 'b19e7a24-0ccb-4180-9f60-2c5664e4dc5c',\n",
       " 62: '0d47370c-9bc9-48f6-aab2-fe73ca988672',\n",
       " 63: '2bd83aea-291e-4231-8bbe-ded3be339265',\n",
       " 64: 'a8332526-aed3-447d-b513-87d5fd68d621',\n",
       " 65: '40f30a77-ca71-4b0e-aa81-b7dbd2ad1d49',\n",
       " 66: '6ced79ee-6d2d-40c0-b2e0-5caac59e6c42',\n",
       " 67: '2113856a-0b11-49ba-85ff-c547775f3f67',\n",
       " 68: '4b4e2661-d3a3-4ed3-89ab-68a34963cb97',\n",
       " 69: 'e89ccbf6-2659-4b09-a625-d0dd31ba266e',\n",
       " 70: '1264dd27-ce64-470f-834a-18585d8c9801',\n",
       " 71: '6b9235c5-bd63-4bb3-ba8b-86d34e445369',\n",
       " 72: '4b6ea75e-3d06-4d3f-acc2-9e97b535b722',\n",
       " 73: '8c992d3c-40d4-449c-90a4-1cab87d0ec3e',\n",
       " 74: '7124eb1c-678f-4d1a-8799-2a296b43850a',\n",
       " 75: '809c9fd5-8917-47ce-8ab6-e9fb1ba63b18',\n",
       " 76: '4eea10a7-1db8-4541-a03c-d68b3f7b9e1e',\n",
       " 77: 'c952ed8c-a0b5-4236-bded-3d8565ae0277',\n",
       " 78: '4d27c03d-2124-4172-b205-634184db2e33',\n",
       " 79: '30dd4404-9641-41e7-adc4-9bc0dd04ffbc',\n",
       " 80: 'be3279b1-44fe-43af-9ec9-4fc8d3ff673e',\n",
       " 81: '0c786c0b-da53-41db-954f-3ba60f3f9e11',\n",
       " 82: '33b73421-0022-4b74-9973-6d2553b8d3a2',\n",
       " 83: '612b2580-8727-48cf-82fb-b6854a9be552',\n",
       " 84: '3ebd04b3-ffbf-4f2f-a95b-7de6141d4524',\n",
       " 85: 'fdf2b528-3ef3-43d7-903e-e188b03d4c1e',\n",
       " 86: '02843bdd-084f-4451-8d88-94cd6b6fb80b',\n",
       " 87: '306f2a10-13ec-4e6c-a3b0-f022cfb926d2',\n",
       " 88: 'c01669c2-2bc5-48c3-9c24-adb090625df9',\n",
       " 89: '8cbf2447-4a6c-4632-b3be-9d310c8e59b2',\n",
       " 90: '714b8dc6-cb16-4214-8903-3c1a4ad0a016',\n",
       " 91: '684946b6-0fb5-455e-88f0-b36a06d6c61c',\n",
       " 92: '86818b79-423a-4621-a76b-58cf60b8af52',\n",
       " 93: 'ed63e277-202e-429a-aa5c-802b4e9b3367',\n",
       " 94: '47b4939c-f63a-465d-abe1-6e21a6d9e981',\n",
       " 95: 'a51b8051-ba5f-4209-bcd4-d04713db094c',\n",
       " 96: 'fd5f80cb-2b0f-466f-bd40-dbc914397051',\n",
       " 97: '56985056-dd31-4de5-a3da-eee7a479eff1',\n",
       " 98: 'ca4b6223-09a7-4e89-9c0b-efea779eedb6',\n",
       " 99: '6b8ef83e-d1e2-4daa-9e40-103b9f2a0c9e',\n",
       " 100: 'aa77b1ba-4b3b-41a6-968c-6ed804f9e082',\n",
       " 101: 'cf2bdc69-2011-4073-abad-4019483b5cfb',\n",
       " 102: 'new_doc1',\n",
       " 103: 'new_doc2',\n",
       " 104: 'new_doc3'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(folder_path = 'faiss_db', index_name = 'faiss_index')\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_db = FAISS.load_local(\n",
    "    folder_path = 'faiss_db',\n",
    "    index_name='faiss_index',\n",
    "    embeddings = embeddings, # 임베딩 모델을 뭘 쓸거냐..\n",
    "    allow_dangerous_deserialization=True # 역 직렬화(파이썬 객체로 변환한다)\n",
    "    # 역직렬화의 위험성을 확인하고 사용자의 명시적 허가를 요구한다는 의미\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '35c756fa-5646-4bf4-ad91-31f2dee028cf',\n",
       " 1: 'a32a89e4-d5f4-4141-84c9-950d0d9ae6ac',\n",
       " 2: '73267bc4-3e76-4b76-a323-ad7267d9a563',\n",
       " 3: '667d6357-cc93-44d9-9667-3f96e4897cbd',\n",
       " 4: '5e4cf004-c056-4093-9b87-066dcbab47e7',\n",
       " 5: 'df6b6958-4085-4b1b-86af-1be4e27678fd',\n",
       " 6: '3e60fb1a-7b03-485e-a651-60d958ffccd1',\n",
       " 7: 'c8a577ab-0f59-4cc6-bc85-d9ed61fb13ee',\n",
       " 8: '565ff0c9-a90a-49d6-96d0-349ad4bd4a91',\n",
       " 9: 'd139ae87-6d97-4671-9d09-5b05e07c5d98',\n",
       " 10: '9fb8e6ca-ec3c-4e59-84ec-8e08a9056791',\n",
       " 11: '878134d3-43b8-4634-9f91-98804dc755dd',\n",
       " 12: '231bfa99-8e6b-4b6d-be3d-527250846beb',\n",
       " 13: '85ae10ed-6e7b-402b-ac4d-a0b18ac32e5d',\n",
       " 14: '1ae7709f-96a8-4c12-827b-886692d71476',\n",
       " 15: '29218b7f-0605-4085-90f1-3883d2647de6',\n",
       " 16: '4614882d-291f-467a-93bc-42c7f5bd085e',\n",
       " 17: '93c36785-a729-4ed4-952e-306c35207f77',\n",
       " 18: 'be3030a9-39a2-4e46-8764-816c7cd74466',\n",
       " 19: '6cb1552e-b55c-411e-ab87-019b24109110',\n",
       " 20: 'e6e30d4c-0bdb-4167-9ab1-ec38c6f81796',\n",
       " 21: '8fc1645e-9569-4937-a6f6-86de671b8f89',\n",
       " 22: '507f6141-6daa-4e6f-bda0-f767c38d9411',\n",
       " 23: '051e54f7-cb7c-44d3-8207-00c8ed4f89a4',\n",
       " 24: 'dd8c1cde-8e41-4eee-9a49-4ef98b9099da',\n",
       " 25: '365c01f8-b5f0-44cf-8ba3-aa94af282af3',\n",
       " 26: '79a08136-a276-4bb3-8346-c2fc8a806566',\n",
       " 27: '6e0c99ed-325e-41c8-8b59-c125d5602ffd',\n",
       " 28: '535334a5-a47c-42ab-a662-08be99c91194',\n",
       " 29: 'f0a906e5-3391-48da-82fd-a4dfad12704b',\n",
       " 30: '3be02ddd-c008-4798-95eb-26885f216c5e',\n",
       " 31: '28548b9e-6751-4a99-a8f4-e36f350922ea',\n",
       " 32: '7f9a01ac-841c-42be-a2da-bbf2828b3f83',\n",
       " 33: 'd272bcd9-1a4e-49aa-bd67-6069ceb48807',\n",
       " 34: 'bf004e67-0dad-4477-928d-41f7926ca85c',\n",
       " 35: '90a795e6-4442-451f-8b91-de8af0279794',\n",
       " 36: '52d96a85-3548-4f81-97a7-5741f245256f',\n",
       " 37: 'bc42c894-1352-4ef6-9a00-c6ba8cdf69fc',\n",
       " 38: 'bfe6ae24-77c8-4427-a585-858eebfc3557',\n",
       " 39: '73434bcb-6130-439c-93b0-4da3b808f7f5',\n",
       " 40: 'c85fbcdd-204e-40dc-a14e-bd6ba6565c58',\n",
       " 41: '846ec9c6-69ad-407d-89aa-f938206a61e4',\n",
       " 42: 'a110482f-3619-4b51-abb1-6d3ff89f4f04',\n",
       " 43: 'a0eba335-606d-42f9-b622-b2a6152e3c60',\n",
       " 44: 'b41cc867-82c2-4138-baec-ae35c938a0c0',\n",
       " 45: '54a46322-7085-42d0-bdd2-5f2c5031a494',\n",
       " 46: 'a498bf8d-e00a-48f8-9083-2a8ec27652f9',\n",
       " 47: '6f3380a5-32bb-44f2-93c1-9e618cdf8fad',\n",
       " 48: '6805af36-68ec-4aab-ad80-8aeaae541c96',\n",
       " 49: 'd6fa1d32-bb0d-4898-a6a9-4428e89e0b7a',\n",
       " 50: '213b8eea-d01a-4645-b0c3-d80fb51d712a',\n",
       " 51: 'cd1f01f5-0ee4-4097-a42f-d9c1ce371077',\n",
       " 52: '40024248-52cf-4fe2-a6a2-11ba7aa6c3f8',\n",
       " 53: 'b73d0f3f-9e87-441f-bdf6-07a6714f89c7',\n",
       " 54: '99a1beb1-370c-44ef-ad66-a2b851ece2a7',\n",
       " 55: 'e5baa7dc-a145-4fff-890a-a304b065b51c',\n",
       " 56: '9169927a-7ac7-4ab2-9d19-006fcadd5a44',\n",
       " 57: 'b33dcfd8-7203-4172-9975-169ea82c0484',\n",
       " 58: '0ac5f321-6447-4d17-a818-6afa28b4d0a1',\n",
       " 59: '3208414c-65bd-4b5f-960d-a6133589b2f4',\n",
       " 60: 'a733cdc1-23af-405b-ae28-b9e3c89501dc',\n",
       " 61: 'b19e7a24-0ccb-4180-9f60-2c5664e4dc5c',\n",
       " 62: '0d47370c-9bc9-48f6-aab2-fe73ca988672',\n",
       " 63: '2bd83aea-291e-4231-8bbe-ded3be339265',\n",
       " 64: 'a8332526-aed3-447d-b513-87d5fd68d621',\n",
       " 65: '40f30a77-ca71-4b0e-aa81-b7dbd2ad1d49',\n",
       " 66: '6ced79ee-6d2d-40c0-b2e0-5caac59e6c42',\n",
       " 67: '2113856a-0b11-49ba-85ff-c547775f3f67',\n",
       " 68: '4b4e2661-d3a3-4ed3-89ab-68a34963cb97',\n",
       " 69: 'e89ccbf6-2659-4b09-a625-d0dd31ba266e',\n",
       " 70: '1264dd27-ce64-470f-834a-18585d8c9801',\n",
       " 71: '6b9235c5-bd63-4bb3-ba8b-86d34e445369',\n",
       " 72: '4b6ea75e-3d06-4d3f-acc2-9e97b535b722',\n",
       " 73: '8c992d3c-40d4-449c-90a4-1cab87d0ec3e',\n",
       " 74: '7124eb1c-678f-4d1a-8799-2a296b43850a',\n",
       " 75: '809c9fd5-8917-47ce-8ab6-e9fb1ba63b18',\n",
       " 76: '4eea10a7-1db8-4541-a03c-d68b3f7b9e1e',\n",
       " 77: 'c952ed8c-a0b5-4236-bded-3d8565ae0277',\n",
       " 78: '4d27c03d-2124-4172-b205-634184db2e33',\n",
       " 79: '30dd4404-9641-41e7-adc4-9bc0dd04ffbc',\n",
       " 80: 'be3279b1-44fe-43af-9ec9-4fc8d3ff673e',\n",
       " 81: '0c786c0b-da53-41db-954f-3ba60f3f9e11',\n",
       " 82: '33b73421-0022-4b74-9973-6d2553b8d3a2',\n",
       " 83: '612b2580-8727-48cf-82fb-b6854a9be552',\n",
       " 84: '3ebd04b3-ffbf-4f2f-a95b-7de6141d4524',\n",
       " 85: 'fdf2b528-3ef3-43d7-903e-e188b03d4c1e',\n",
       " 86: '02843bdd-084f-4451-8d88-94cd6b6fb80b',\n",
       " 87: '306f2a10-13ec-4e6c-a3b0-f022cfb926d2',\n",
       " 88: 'c01669c2-2bc5-48c3-9c24-adb090625df9',\n",
       " 89: '8cbf2447-4a6c-4632-b3be-9d310c8e59b2',\n",
       " 90: '714b8dc6-cb16-4214-8903-3c1a4ad0a016',\n",
       " 91: '684946b6-0fb5-455e-88f0-b36a06d6c61c',\n",
       " 92: '86818b79-423a-4621-a76b-58cf60b8af52',\n",
       " 93: 'ed63e277-202e-429a-aa5c-802b4e9b3367',\n",
       " 94: '47b4939c-f63a-465d-abe1-6e21a6d9e981',\n",
       " 95: 'a51b8051-ba5f-4209-bcd4-d04713db094c',\n",
       " 96: 'fd5f80cb-2b0f-466f-bd40-dbc914397051',\n",
       " 97: '56985056-dd31-4de5-a3da-eee7a479eff1',\n",
       " 98: 'ca4b6223-09a7-4e89-9c0b-efea779eedb6',\n",
       " 99: '6b8ef83e-d1e2-4daa-9e40-103b9f2a0c9e',\n",
       " 100: 'aa77b1ba-4b3b-41a6-968c-6ed804f9e082',\n",
       " 101: 'cf2bdc69-2011-4073-abad-4019483b5cfb',\n",
       " 102: 'new_doc1',\n",
       " 103: 'new_doc2',\n",
       " 104: 'new_doc3'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_db.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = FAISS.from_documents(documents = split_doc2, embedding = OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '8f20d388-a361-4e22-83da-c7a49a0f7703',\n",
       " 1: '73fb1c21-b5de-4d54-b3a6-e21aaf3c4f4d',\n",
       " 2: 'b2699095-58a0-4a14-9c33-e0a666219792',\n",
       " 3: 'e8f30366-3e7a-423e-88c2-642523472834',\n",
       " 4: '5dfc5419-600d-4a46-a2ba-1f6a744bc3bf',\n",
       " 5: 'c355743a-830f-4078-a81f-6998770f4671',\n",
       " 6: '592a4576-f76c-495a-b5bd-c5d1f2828944',\n",
       " 7: '83d738d7-984f-464b-81bf-ff23470597a3',\n",
       " 8: 'a1bd6e6a-6f9c-4ba8-aeda-ee04b84968e3',\n",
       " 9: '6a8d2fa5-19c5-4de6-ab7a-aaad51ac19fb',\n",
       " 10: '02e17e1b-e90d-43ac-8b31-b07efc72e0d3',\n",
       " 11: '97d74846-bf5f-4d3d-b2b5-610b44d213e4',\n",
       " 12: '115cafe2-053f-447e-aa02-c690cae70656',\n",
       " 13: '2c345ddd-af2e-4d76-8d9e-834e18cef9b4',\n",
       " 14: '34601afb-d534-476c-b9c3-01003a6bdfb1',\n",
       " 15: '50f2d8f0-c75b-40f8-9aa9-5bdd7c6cca14',\n",
       " 16: '536d7d57-5332-4e1b-8de7-56a6db37020a',\n",
       " 17: '122ec4b7-0b79-4a98-93a9-acf64de20cee',\n",
       " 18: '7407e604-0243-49f5-8dcb-27821081de8a',\n",
       " 19: '3bb7a897-4e19-4582-9f50-8acd08a76119',\n",
       " 20: '290e45f7-5be9-4e23-a256-b94c1d592b29',\n",
       " 21: '74b59725-74a6-4b67-84ae-81f4963dc6c1',\n",
       " 22: '098f1cef-a376-4432-a28a-fd691f53a8a2',\n",
       " 23: '371eb2e2-c012-421f-ab39-d89489d0c319',\n",
       " 24: 'ad437dc9-503e-44cf-b7f0-c1006000e33f',\n",
       " 25: '4e10badc-5d12-4bfe-ae40-2fac39e9bb0c',\n",
       " 26: '849a7323-b5cb-4488-a1b8-2ea56c198b54',\n",
       " 27: '08c929ec-1b59-4efd-a328-926c74c568ba',\n",
       " 28: '4b99c3f3-f548-4b67-b058-c54eac13dab3',\n",
       " 29: 'c1b8a32a-e610-4e90-922a-5d3d6f52f275',\n",
       " 30: '76784da4-73eb-4731-b458-9336607f9056',\n",
       " 31: 'd0bb562d-7743-4f8d-941a-6ed13d92005b',\n",
       " 32: 'd098f39d-4544-40fe-8383-17c733c40dd6',\n",
       " 33: 'a8677a61-c4a7-43b4-af04-df6d6a438d31',\n",
       " 34: '218f0911-7897-4352-ac84-f217cec73c2f',\n",
       " 35: 'b5bb638d-322d-40fc-a94b-cb1a45fb0093',\n",
       " 36: '3085b7dc-613d-40c7-a62f-a802438b0744',\n",
       " 37: 'e4fea77a-f795-42fb-8bf1-e8d090e64101',\n",
       " 38: '71f02946-ab5e-467f-b314-bfa3660d5bf1',\n",
       " 39: 'bf2a82cf-9c52-4990-8386-d7b5813da959',\n",
       " 40: 'a30efcea-b990-42d9-824b-9fff6a82eb24',\n",
       " 41: '867b47c4-b207-4a7d-8d25-dfc957e96fd9',\n",
       " 42: '372940af-8056-439d-8500-3b3666f989b0',\n",
       " 43: 'a43fe60c-a741-4718-b0b7-047b05bc4b74',\n",
       " 44: '25f0f97b-51d6-4bae-b1b0-9b48036c9a90',\n",
       " 45: '58c2edbc-c1f2-41ac-b191-9564b0996fa8',\n",
       " 46: 'dd648882-c11e-470c-afec-7224b9755dde',\n",
       " 47: 'ab057aaf-f38a-4f6c-990a-80b04c6fa6ab',\n",
       " 48: 'a1a663bc-6be7-4513-a4b6-5f1e603ba1a8',\n",
       " 49: '2d8ff478-f02f-4016-b718-56c173583553',\n",
       " 50: '0ee0520e-b4e8-4a56-ab66-142338fccf7f',\n",
       " 51: 'f09464a3-705b-4a59-86dc-4181a23fda1e',\n",
       " 52: 'b79f4271-18c5-4bff-b41f-893132399718',\n",
       " 53: 'f56f2b03-1773-4ecc-b6c7-325b17de8a2d',\n",
       " 54: '1a7f122f-70c5-4446-8109-feb8e58e0215',\n",
       " 55: 'ac865a73-9616-49b1-ac68-468581a8d487',\n",
       " 56: '8be9d060-3622-43fe-8cd2-5d65857b504f',\n",
       " 57: '2c037973-d6d9-4a88-add9-0bcdaf9efa56',\n",
       " 58: '74d77a46-b71c-46e6-b216-d22d6f8e9ad9',\n",
       " 59: '3a94f620-60f9-4464-b2b1-6a649b4edd54'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db2.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.merge_from(db2) # db에 db2를 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '35c756fa-5646-4bf4-ad91-31f2dee028cf',\n",
       " 1: 'a32a89e4-d5f4-4141-84c9-950d0d9ae6ac',\n",
       " 2: '73267bc4-3e76-4b76-a323-ad7267d9a563',\n",
       " 3: '667d6357-cc93-44d9-9667-3f96e4897cbd',\n",
       " 4: '5e4cf004-c056-4093-9b87-066dcbab47e7',\n",
       " 5: 'df6b6958-4085-4b1b-86af-1be4e27678fd',\n",
       " 6: '3e60fb1a-7b03-485e-a651-60d958ffccd1',\n",
       " 7: 'c8a577ab-0f59-4cc6-bc85-d9ed61fb13ee',\n",
       " 8: '565ff0c9-a90a-49d6-96d0-349ad4bd4a91',\n",
       " 9: 'd139ae87-6d97-4671-9d09-5b05e07c5d98',\n",
       " 10: '9fb8e6ca-ec3c-4e59-84ec-8e08a9056791',\n",
       " 11: '878134d3-43b8-4634-9f91-98804dc755dd',\n",
       " 12: '231bfa99-8e6b-4b6d-be3d-527250846beb',\n",
       " 13: '85ae10ed-6e7b-402b-ac4d-a0b18ac32e5d',\n",
       " 14: '1ae7709f-96a8-4c12-827b-886692d71476',\n",
       " 15: '29218b7f-0605-4085-90f1-3883d2647de6',\n",
       " 16: '4614882d-291f-467a-93bc-42c7f5bd085e',\n",
       " 17: '93c36785-a729-4ed4-952e-306c35207f77',\n",
       " 18: 'be3030a9-39a2-4e46-8764-816c7cd74466',\n",
       " 19: '6cb1552e-b55c-411e-ab87-019b24109110',\n",
       " 20: 'e6e30d4c-0bdb-4167-9ab1-ec38c6f81796',\n",
       " 21: '8fc1645e-9569-4937-a6f6-86de671b8f89',\n",
       " 22: '507f6141-6daa-4e6f-bda0-f767c38d9411',\n",
       " 23: '051e54f7-cb7c-44d3-8207-00c8ed4f89a4',\n",
       " 24: 'dd8c1cde-8e41-4eee-9a49-4ef98b9099da',\n",
       " 25: '365c01f8-b5f0-44cf-8ba3-aa94af282af3',\n",
       " 26: '79a08136-a276-4bb3-8346-c2fc8a806566',\n",
       " 27: '6e0c99ed-325e-41c8-8b59-c125d5602ffd',\n",
       " 28: '535334a5-a47c-42ab-a662-08be99c91194',\n",
       " 29: 'f0a906e5-3391-48da-82fd-a4dfad12704b',\n",
       " 30: '3be02ddd-c008-4798-95eb-26885f216c5e',\n",
       " 31: '28548b9e-6751-4a99-a8f4-e36f350922ea',\n",
       " 32: '7f9a01ac-841c-42be-a2da-bbf2828b3f83',\n",
       " 33: 'd272bcd9-1a4e-49aa-bd67-6069ceb48807',\n",
       " 34: 'bf004e67-0dad-4477-928d-41f7926ca85c',\n",
       " 35: '90a795e6-4442-451f-8b91-de8af0279794',\n",
       " 36: '52d96a85-3548-4f81-97a7-5741f245256f',\n",
       " 37: 'bc42c894-1352-4ef6-9a00-c6ba8cdf69fc',\n",
       " 38: 'bfe6ae24-77c8-4427-a585-858eebfc3557',\n",
       " 39: '73434bcb-6130-439c-93b0-4da3b808f7f5',\n",
       " 40: 'c85fbcdd-204e-40dc-a14e-bd6ba6565c58',\n",
       " 41: '846ec9c6-69ad-407d-89aa-f938206a61e4',\n",
       " 42: 'a110482f-3619-4b51-abb1-6d3ff89f4f04',\n",
       " 43: 'a0eba335-606d-42f9-b622-b2a6152e3c60',\n",
       " 44: 'b41cc867-82c2-4138-baec-ae35c938a0c0',\n",
       " 45: '54a46322-7085-42d0-bdd2-5f2c5031a494',\n",
       " 46: 'a498bf8d-e00a-48f8-9083-2a8ec27652f9',\n",
       " 47: '6f3380a5-32bb-44f2-93c1-9e618cdf8fad',\n",
       " 48: '6805af36-68ec-4aab-ad80-8aeaae541c96',\n",
       " 49: 'd6fa1d32-bb0d-4898-a6a9-4428e89e0b7a',\n",
       " 50: '213b8eea-d01a-4645-b0c3-d80fb51d712a',\n",
       " 51: 'cd1f01f5-0ee4-4097-a42f-d9c1ce371077',\n",
       " 52: '40024248-52cf-4fe2-a6a2-11ba7aa6c3f8',\n",
       " 53: 'b73d0f3f-9e87-441f-bdf6-07a6714f89c7',\n",
       " 54: '99a1beb1-370c-44ef-ad66-a2b851ece2a7',\n",
       " 55: 'e5baa7dc-a145-4fff-890a-a304b065b51c',\n",
       " 56: '9169927a-7ac7-4ab2-9d19-006fcadd5a44',\n",
       " 57: 'b33dcfd8-7203-4172-9975-169ea82c0484',\n",
       " 58: '0ac5f321-6447-4d17-a818-6afa28b4d0a1',\n",
       " 59: '3208414c-65bd-4b5f-960d-a6133589b2f4',\n",
       " 60: 'a733cdc1-23af-405b-ae28-b9e3c89501dc',\n",
       " 61: 'b19e7a24-0ccb-4180-9f60-2c5664e4dc5c',\n",
       " 62: '0d47370c-9bc9-48f6-aab2-fe73ca988672',\n",
       " 63: '2bd83aea-291e-4231-8bbe-ded3be339265',\n",
       " 64: 'a8332526-aed3-447d-b513-87d5fd68d621',\n",
       " 65: '40f30a77-ca71-4b0e-aa81-b7dbd2ad1d49',\n",
       " 66: '6ced79ee-6d2d-40c0-b2e0-5caac59e6c42',\n",
       " 67: '2113856a-0b11-49ba-85ff-c547775f3f67',\n",
       " 68: '4b4e2661-d3a3-4ed3-89ab-68a34963cb97',\n",
       " 69: 'e89ccbf6-2659-4b09-a625-d0dd31ba266e',\n",
       " 70: '1264dd27-ce64-470f-834a-18585d8c9801',\n",
       " 71: '6b9235c5-bd63-4bb3-ba8b-86d34e445369',\n",
       " 72: '4b6ea75e-3d06-4d3f-acc2-9e97b535b722',\n",
       " 73: '8c992d3c-40d4-449c-90a4-1cab87d0ec3e',\n",
       " 74: '7124eb1c-678f-4d1a-8799-2a296b43850a',\n",
       " 75: '809c9fd5-8917-47ce-8ab6-e9fb1ba63b18',\n",
       " 76: '4eea10a7-1db8-4541-a03c-d68b3f7b9e1e',\n",
       " 77: 'c952ed8c-a0b5-4236-bded-3d8565ae0277',\n",
       " 78: '4d27c03d-2124-4172-b205-634184db2e33',\n",
       " 79: '30dd4404-9641-41e7-adc4-9bc0dd04ffbc',\n",
       " 80: 'be3279b1-44fe-43af-9ec9-4fc8d3ff673e',\n",
       " 81: '0c786c0b-da53-41db-954f-3ba60f3f9e11',\n",
       " 82: '33b73421-0022-4b74-9973-6d2553b8d3a2',\n",
       " 83: '612b2580-8727-48cf-82fb-b6854a9be552',\n",
       " 84: '3ebd04b3-ffbf-4f2f-a95b-7de6141d4524',\n",
       " 85: 'fdf2b528-3ef3-43d7-903e-e188b03d4c1e',\n",
       " 86: '02843bdd-084f-4451-8d88-94cd6b6fb80b',\n",
       " 87: '306f2a10-13ec-4e6c-a3b0-f022cfb926d2',\n",
       " 88: 'c01669c2-2bc5-48c3-9c24-adb090625df9',\n",
       " 89: '8cbf2447-4a6c-4632-b3be-9d310c8e59b2',\n",
       " 90: '714b8dc6-cb16-4214-8903-3c1a4ad0a016',\n",
       " 91: '684946b6-0fb5-455e-88f0-b36a06d6c61c',\n",
       " 92: '86818b79-423a-4621-a76b-58cf60b8af52',\n",
       " 93: 'ed63e277-202e-429a-aa5c-802b4e9b3367',\n",
       " 94: '47b4939c-f63a-465d-abe1-6e21a6d9e981',\n",
       " 95: 'a51b8051-ba5f-4209-bcd4-d04713db094c',\n",
       " 96: 'fd5f80cb-2b0f-466f-bd40-dbc914397051',\n",
       " 97: '56985056-dd31-4de5-a3da-eee7a479eff1',\n",
       " 98: 'ca4b6223-09a7-4e89-9c0b-efea779eedb6',\n",
       " 99: '6b8ef83e-d1e2-4daa-9e40-103b9f2a0c9e',\n",
       " 100: 'aa77b1ba-4b3b-41a6-968c-6ed804f9e082',\n",
       " 101: 'cf2bdc69-2011-4073-abad-4019483b5cfb',\n",
       " 102: 'new_doc1',\n",
       " 103: 'new_doc2',\n",
       " 104: 'new_doc3',\n",
       " 105: '8f20d388-a361-4e22-83da-c7a49a0f7703',\n",
       " 106: '73fb1c21-b5de-4d54-b3a6-e21aaf3c4f4d',\n",
       " 107: 'b2699095-58a0-4a14-9c33-e0a666219792',\n",
       " 108: 'e8f30366-3e7a-423e-88c2-642523472834',\n",
       " 109: '5dfc5419-600d-4a46-a2ba-1f6a744bc3bf',\n",
       " 110: 'c355743a-830f-4078-a81f-6998770f4671',\n",
       " 111: '592a4576-f76c-495a-b5bd-c5d1f2828944',\n",
       " 112: '83d738d7-984f-464b-81bf-ff23470597a3',\n",
       " 113: 'a1bd6e6a-6f9c-4ba8-aeda-ee04b84968e3',\n",
       " 114: '6a8d2fa5-19c5-4de6-ab7a-aaad51ac19fb',\n",
       " 115: '02e17e1b-e90d-43ac-8b31-b07efc72e0d3',\n",
       " 116: '97d74846-bf5f-4d3d-b2b5-610b44d213e4',\n",
       " 117: '115cafe2-053f-447e-aa02-c690cae70656',\n",
       " 118: '2c345ddd-af2e-4d76-8d9e-834e18cef9b4',\n",
       " 119: '34601afb-d534-476c-b9c3-01003a6bdfb1',\n",
       " 120: '50f2d8f0-c75b-40f8-9aa9-5bdd7c6cca14',\n",
       " 121: '536d7d57-5332-4e1b-8de7-56a6db37020a',\n",
       " 122: '122ec4b7-0b79-4a98-93a9-acf64de20cee',\n",
       " 123: '7407e604-0243-49f5-8dcb-27821081de8a',\n",
       " 124: '3bb7a897-4e19-4582-9f50-8acd08a76119',\n",
       " 125: '290e45f7-5be9-4e23-a256-b94c1d592b29',\n",
       " 126: '74b59725-74a6-4b67-84ae-81f4963dc6c1',\n",
       " 127: '098f1cef-a376-4432-a28a-fd691f53a8a2',\n",
       " 128: '371eb2e2-c012-421f-ab39-d89489d0c319',\n",
       " 129: 'ad437dc9-503e-44cf-b7f0-c1006000e33f',\n",
       " 130: '4e10badc-5d12-4bfe-ae40-2fac39e9bb0c',\n",
       " 131: '849a7323-b5cb-4488-a1b8-2ea56c198b54',\n",
       " 132: '08c929ec-1b59-4efd-a328-926c74c568ba',\n",
       " 133: '4b99c3f3-f548-4b67-b058-c54eac13dab3',\n",
       " 134: 'c1b8a32a-e610-4e90-922a-5d3d6f52f275',\n",
       " 135: '76784da4-73eb-4731-b458-9336607f9056',\n",
       " 136: 'd0bb562d-7743-4f8d-941a-6ed13d92005b',\n",
       " 137: 'd098f39d-4544-40fe-8383-17c733c40dd6',\n",
       " 138: 'a8677a61-c4a7-43b4-af04-df6d6a438d31',\n",
       " 139: '218f0911-7897-4352-ac84-f217cec73c2f',\n",
       " 140: 'b5bb638d-322d-40fc-a94b-cb1a45fb0093',\n",
       " 141: '3085b7dc-613d-40c7-a62f-a802438b0744',\n",
       " 142: 'e4fea77a-f795-42fb-8bf1-e8d090e64101',\n",
       " 143: '71f02946-ab5e-467f-b314-bfa3660d5bf1',\n",
       " 144: 'bf2a82cf-9c52-4990-8386-d7b5813da959',\n",
       " 145: 'a30efcea-b990-42d9-824b-9fff6a82eb24',\n",
       " 146: '867b47c4-b207-4a7d-8d25-dfc957e96fd9',\n",
       " 147: '372940af-8056-439d-8500-3b3666f989b0',\n",
       " 148: 'a43fe60c-a741-4718-b0b7-047b05bc4b74',\n",
       " 149: '25f0f97b-51d6-4bae-b1b0-9b48036c9a90',\n",
       " 150: '58c2edbc-c1f2-41ac-b191-9564b0996fa8',\n",
       " 151: 'dd648882-c11e-470c-afec-7224b9755dde',\n",
       " 152: 'ab057aaf-f38a-4f6c-990a-80b04c6fa6ab',\n",
       " 153: 'a1a663bc-6be7-4513-a4b6-5f1e603ba1a8',\n",
       " 154: '2d8ff478-f02f-4016-b718-56c173583553',\n",
       " 155: '0ee0520e-b4e8-4a56-ab66-142338fccf7f',\n",
       " 156: 'f09464a3-705b-4a59-86dc-4181a23fda1e',\n",
       " 157: 'b79f4271-18c5-4bff-b41f-893132399718',\n",
       " 158: 'f56f2b03-1773-4ecc-b6c7-325b17de8a2d',\n",
       " 159: '1a7f122f-70c5-4446-8109-feb8e58e0215',\n",
       " 160: 'ac865a73-9616-49b1-ac68-468581a8d487',\n",
       " 161: '8be9d060-3622-43fe-8cd2-5d65857b504f',\n",
       " 162: '2c037973-d6d9-4a88-add9-0bcdaf9efa56',\n",
       " 163: '74d77a46-b71c-46e6-b216-d22d6f8e9ad9',\n",
       " 164: '3a94f620-60f9-4464-b2b1-6a649b4edd54'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index_to_docstore_id # db에 병합된 결과 확인!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넣을 때 부터 한번에 합쳐서 넣기\n",
    "db = FAISS.from_documents(\n",
    "    documents = split_doc1 + split_doc2, embedding = OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '613d74f4-752b-440b-af3c-65d2271eee1c',\n",
       " 1: '31871ed9-917c-495c-a6a4-2879c0893bfc',\n",
       " 2: '869af566-abb3-40ea-9518-664dc3ed870c',\n",
       " 3: 'cc399a8c-0d87-4ec8-bb65-35aea8ba3753',\n",
       " 4: '8679775f-2750-4452-97a2-4daea7682bfa',\n",
       " 5: '44b7aa28-1c92-4937-8527-ae8ac2680948',\n",
       " 6: '4feffbf5-aa52-49bd-97b3-7b36718ccb18',\n",
       " 7: '162aabc8-7a75-4e25-bc00-7280a83b8c79',\n",
       " 8: '8a327778-ba3b-4379-b29d-5bb14cc12c19',\n",
       " 9: '6e5a8303-65f4-471a-8879-9281c3bfdd4a',\n",
       " 10: '06364eb2-a277-42bb-aeba-55989b1b9caa',\n",
       " 11: '0cb38353-55cc-4746-b4a7-fa8ad1df1b91',\n",
       " 12: 'a0289c1c-6627-4b87-8ac7-dd26fb9302fc',\n",
       " 13: '8d4c3deb-40a8-40f9-b605-22a68dc0cc56',\n",
       " 14: '06b0f030-cbf5-4139-917e-bf6175042322',\n",
       " 15: 'a36d70b7-ce95-4900-9ee8-712449eee823',\n",
       " 16: 'b580fea0-2f87-40fa-be39-75ab08e8a199',\n",
       " 17: '56d0c092-1cad-4a32-8d57-11e54c0a5493',\n",
       " 18: '6d5a237e-9fb4-4dd5-b335-aaa1b174ee7d',\n",
       " 19: 'bec96b77-f386-4950-91d0-cd2f2462ba95',\n",
       " 20: '825a1066-aa44-4dda-992a-3d82067808d8',\n",
       " 21: '595ca7de-7dd1-480b-8961-67a52061f35c',\n",
       " 22: '3ebdcd10-6f12-4ae3-8795-ff88e8672dea',\n",
       " 23: '30011bf3-d401-4532-ac7e-6fbb5f896a3d',\n",
       " 24: 'cefbe3f4-62d1-49e1-b689-2dafe46e45de',\n",
       " 25: '8cad8790-d2e8-4ad4-b8c0-2d67a53c1dc1',\n",
       " 26: '9770c068-b1b6-4ec0-8efb-fffc9ba58b58',\n",
       " 27: '9a7b20d2-75bf-4a98-b5b7-4883838a6493',\n",
       " 28: '61c8cdbb-2256-4fce-a190-40399ca216c4',\n",
       " 29: '221c8347-486b-46b2-955d-9c00d4e6485a',\n",
       " 30: '426698e7-9338-4522-991b-4ddbf992288a',\n",
       " 31: '56274cb4-755a-4449-a38f-ebd686a3ee3a',\n",
       " 32: '35abe9eb-2766-4fb0-8db2-fe06b4fd1415',\n",
       " 33: '8ad01857-7722-4630-8669-5e5b65ce8d3a',\n",
       " 34: '7250c989-6b71-4657-9d16-01b6fe38c235',\n",
       " 35: 'c9ee95f0-8111-4144-8bec-3e22ff517907',\n",
       " 36: 'b42bb62d-47c1-48d2-9fb1-ee4dfecdb2c0',\n",
       " 37: 'a88c4a83-9cb8-4196-8fee-0f2eb96c1607',\n",
       " 38: '9891d266-3863-4ae2-a3a6-f4c8fb4568a7',\n",
       " 39: 'b428dfde-a903-4a07-a5df-a91763f6ede3',\n",
       " 40: '2aa97cdd-3c90-441c-8ff3-8e67d8c1bc0f',\n",
       " 41: '7fc85977-2cf1-423e-829a-7edf8ac275e7',\n",
       " 42: '01d8de9a-2d50-4958-8e48-4809de6f8e2e',\n",
       " 43: '15ce2c70-efcf-4201-866f-bb29cad675ae',\n",
       " 44: '3a15e2f7-0b81-4ed1-b7b0-a01ed7d1274e',\n",
       " 45: 'b9a07027-534a-4254-a33c-4fb752f89941',\n",
       " 46: '6b2aaff5-b29a-4eae-97d0-853a35e03857',\n",
       " 47: '88d7d377-1395-4c20-92c0-838d6cad69d5',\n",
       " 48: '85c22dfe-f448-4b3e-80de-ce7b5e2c6d4d',\n",
       " 49: 'bfb3a554-1adb-4bdc-a00d-2aa26b022454',\n",
       " 50: 'f8ecaa00-1105-411a-bd64-7a558a804732',\n",
       " 51: '6ec4a78f-5ee9-4c44-9731-6554edfd1e8e',\n",
       " 52: '662f0a09-0bb3-4e57-9890-23a993f72b70',\n",
       " 53: '51663afc-6a85-4126-ab99-a8890324e476',\n",
       " 54: '9e4dac2f-e6ec-4c4c-a18f-e6de4dfbefd2',\n",
       " 55: '85c99995-c2fe-4eaa-93ce-842edcf192d1',\n",
       " 56: '17d37975-4a4d-4227-8f35-30c655493140',\n",
       " 57: '33cb49f5-2ee1-45c0-bc56-e50dc6258d21',\n",
       " 58: 'c95c6f85-9d6b-4701-a688-e7e3acefa27b',\n",
       " 59: '0fc476bc-f8f3-4fbc-bfed-64e87630ad51',\n",
       " 60: '8159c536-829e-4406-947b-43e05f8d294b',\n",
       " 61: 'a60386e3-e0ea-4a23-a9f7-dba0829ad35a',\n",
       " 62: 'a00d9938-d333-457d-a8d6-3c77af27b0ad',\n",
       " 63: '508e3a2c-352a-4860-a79d-7b0ba7d4f9db',\n",
       " 64: '8da13a01-fb1c-41f2-9b18-dc2f126b5a32',\n",
       " 65: 'dbc063a0-ca20-477c-b73e-fee959ec0bf7',\n",
       " 66: 'de6568c7-bc10-4748-9d01-1ecd5a20ae1d',\n",
       " 67: '6a5036da-eaaa-4419-a775-66a2140bfefb',\n",
       " 68: '60dc22cb-8abf-42fc-8aa6-e1117b7d18c1',\n",
       " 69: '3e2de8e9-8bcc-41fe-92ac-0c26df570528',\n",
       " 70: '8408f34c-9797-4191-bdd9-7078a47a3157',\n",
       " 71: 'b50f762d-4b7a-4c78-bdb3-64ce1c3cec39',\n",
       " 72: '3084d873-8f4c-4486-a950-d2efc2f72e47',\n",
       " 73: '3043a572-666b-4e96-89a4-ff016575d2ac',\n",
       " 74: '47f98e9e-3c6a-4ff2-82e2-558c72dba825',\n",
       " 75: '2c4901c4-a846-4faa-a0e4-f7f1701ceaae',\n",
       " 76: '6e5ce496-c724-4e2a-8f24-7598c88915d1',\n",
       " 77: '3642d498-21f5-4041-a2f1-aa2bfc5c5263',\n",
       " 78: '43acb25d-a4f0-4ec7-8c6f-1d30e1303895',\n",
       " 79: 'c035b03a-3aa8-432e-892a-c2ec32c552d5',\n",
       " 80: '07a70218-9c49-48da-8685-cc811c1e78f3',\n",
       " 81: '3f4e7414-3e41-4808-8a52-0f5965b08b73',\n",
       " 82: '2aa6a611-8bf0-4dac-9570-0c56ad6f4a46',\n",
       " 83: '6ee899b9-6745-481c-a906-1b589c156466',\n",
       " 84: '40d47bba-8f4b-41bc-80ca-f6eaee2c92e7',\n",
       " 85: '07b02e0f-c03c-4b83-bfbd-ebc20c575a43',\n",
       " 86: '75354b55-e1a3-4e96-ae74-7f1b1b5468a8',\n",
       " 87: 'f982eeb0-fa82-488f-a174-8b868c640b03',\n",
       " 88: 'eddd5ee6-704c-4868-acc5-3e2a41ccf8c5',\n",
       " 89: '90e170eb-49dc-47be-a9d5-3bc5d049216c',\n",
       " 90: '373f1686-4dc5-424e-a6ff-ad8806965478',\n",
       " 91: '29e51e5f-b563-489d-ba4f-7a79b40020f5',\n",
       " 92: '1e6ff4f5-b67b-4326-8e73-141aa68ca7e4',\n",
       " 93: '69f5c22a-0c0c-47af-8f92-be0398383774',\n",
       " 94: 'f16a5ea7-8a74-4d98-a242-79b0b17b7f7f',\n",
       " 95: '54a0a61e-a2bf-4424-ab13-8f7e919e471e',\n",
       " 96: '9d4a9c5c-2cc0-4d46-a81c-c496ca6eaf9d',\n",
       " 97: '1ad8362e-fc33-45fc-9447-aa8af75e93cc',\n",
       " 98: '3aa9d299-0240-472d-ad00-4937c668d17c',\n",
       " 99: 'c1d10799-8de5-4e1d-9a46-40596ac92e22',\n",
       " 100: 'd1db30f6-f361-4e8a-bfe4-a74adf4fa4f4',\n",
       " 101: '9d5be732-7130-4206-8186-5b13cbad15a6',\n",
       " 102: '6570fef2-112b-4b67-98a4-e7a168f903a5',\n",
       " 103: '2e388009-b3ab-4aa3-a1ba-28cacb617db4',\n",
       " 104: '95aae586-8273-452b-858a-2ee84a94129a',\n",
       " 105: 'efcb2eca-ac39-4045-8d18-eda23b1e4988',\n",
       " 106: '9413fc2a-4be2-4109-92c5-f5e5e36e28a9',\n",
       " 107: '3718c7f3-a5bc-4435-b8f2-7de2b7354756',\n",
       " 108: 'a51f966a-fa62-4c9f-bd3a-c5c5b6fa33a6',\n",
       " 109: '4c55f751-16a3-4109-8da4-acafde2ba826',\n",
       " 110: '0ec330fe-660a-407f-ac0d-bbac62ac470c',\n",
       " 111: '737ae137-c109-4045-96bd-f1a4e6c19b1d',\n",
       " 112: '6802f062-8f5b-4380-8f8b-f42851a7bd03',\n",
       " 113: 'e5d07e34-0919-4cde-a539-d232fc37e0d5',\n",
       " 114: '89ca8698-9c12-4b85-9248-21db67d69602',\n",
       " 115: '4e61535f-0716-4fc6-ad45-04752f10b939',\n",
       " 116: 'b8dbe810-bded-4d89-a5a5-0e65e07ad5cb',\n",
       " 117: 'bfceee04-8380-468d-8e78-81cea3e9ee40',\n",
       " 118: '6b332d0d-77f8-4ead-a6ae-b0be591fbc0d',\n",
       " 119: 'eda33d9c-f5e1-44e9-9db9-3b3cf79137f9',\n",
       " 120: '7c5659de-0711-403e-9706-9841ade5d1ec',\n",
       " 121: '909659cc-0825-4f1c-a3ee-3b3c2efa1095',\n",
       " 122: '7fe9d055-3c50-4141-9b01-4e92d67807c4',\n",
       " 123: '07f5e4f9-e243-4fb8-b829-7d79229aaa9c',\n",
       " 124: '70ed386a-1163-4a2a-bebf-b5382a8e4fdb',\n",
       " 125: '6381eff8-4d81-426e-8ea3-fac9abc4debd',\n",
       " 126: '4c7e6925-0e1d-4de7-a399-3cc02e513607',\n",
       " 127: 'c8a62bff-eadc-4ee2-9283-28013dc652da',\n",
       " 128: 'd37df490-d792-4979-b08d-4bbec2fc1190',\n",
       " 129: '94e0c7c5-1ff4-439e-b9f1-56b22aff67fc',\n",
       " 130: '5809a63c-42c7-4948-b07f-6a4873451965',\n",
       " 131: 'b4885c5d-3ef0-49f7-99ec-063d58a9a00f',\n",
       " 132: 'efc9d171-8c4c-4e0c-9d7e-d772c7ce93a2',\n",
       " 133: '461ea6c1-71de-4eaa-a337-c16094107962',\n",
       " 134: 'c9e79f8f-476b-4222-aadd-7f522067f9b4',\n",
       " 135: '6cfd4e04-6590-4dfd-bb14-62c9cbcc3e1c',\n",
       " 136: 'a3aceaa4-6fc4-419d-a664-f7323e1d145b',\n",
       " 137: '3d291a68-9e81-4a3a-955e-7c04cd6404ad',\n",
       " 138: 'cd7e132c-e74f-4e53-b5b8-7836b6f7cf29',\n",
       " 139: 'bb1c468a-dbec-40a7-9c89-910bc76c882b',\n",
       " 140: '3a712eed-8231-44c4-8634-4d0a9a8e1f3e',\n",
       " 141: '78089518-dfc0-4ba5-ba2f-fc77590eb703',\n",
       " 142: '4325b47c-d027-4a80-87dd-781642d8a25c',\n",
       " 143: '7e93c25f-d651-4399-a6e1-61269fafce1c',\n",
       " 144: 'fc757a11-e70d-4f74-a300-b4a8b021792a',\n",
       " 145: 'd0f03190-3f6f-45a2-a642-c9ce1926b4b0',\n",
       " 146: 'a10316cc-ebdb-4f6a-b851-3213e439d706',\n",
       " 147: 'b1cf7c93-23ed-4e24-bc96-f5bd3f9c813c',\n",
       " 148: '2987dc6b-2017-425b-a3e1-69291586c64e',\n",
       " 149: '8e4bce08-6eaf-4aa0-87e3-d060a8eaf9fe',\n",
       " 150: '0c43429c-008f-4789-9d79-b4a31fe56216',\n",
       " 151: '5a6824c6-b12b-4b96-9052-c43ef723fbf9',\n",
       " 152: '9e038ebd-c281-44ee-89ad-56308144858b',\n",
       " 153: '18a2a8e1-f4b1-49b0-b968-4166dfc106c2',\n",
       " 154: '05d06aef-2d76-4def-8d73-d28e3f027ce2',\n",
       " 155: '6cf15f54-b004-42ee-9ec9-396015733621',\n",
       " 156: 'bded4f85-c793-40e7-9571-7bfdb6a944cd',\n",
       " 157: '06f25996-9d13-465c-b3d7-36092529826a',\n",
       " 158: 'df800634-fcec-4e76-bfe2-5a78241b126d',\n",
       " 159: '6a9e040d-9c20-4237-b834-edaf43246387',\n",
       " 160: '30b7d564-30e0-4867-8a7f-e3fddee51276'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db 저장소에 검색 기능 추가\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './nlp-keywords.txt'}, page_content='Word2Vec'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke('Word2Vec에 대해 알려줘') # search를 실행한 것과 동일한 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type = 'mmr', search_kwargs = {'k' : 5, 'lambda_mult' : 0.25, 'fetch_k' : 10}\n",
    ")\n",
    "# MMR 검색 : 유사성과 다양성을 균형있게 고려하여 검색 결과를 반환하는 검색 방법\n",
    "# lambda_mult : 0에 가까울 수록 유사성을 더 중시하고, 1에 가까울수록 다양성을 더 고려(기본값은 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './nlp-keywords.txt'}, page_content='Word2Vec'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정확한 정보를 추출하거나 예측하는 데 사용됩니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke('Word2Vec에 대해 알려줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './nlp-keywords.txt'}, page_content='Word2Vec'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정확한 정보를 추출하거나 예측하는 데 사용됩니다.')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type = 'mmr', search_kwargs = {'k' : 2, 'fetch_k' : 10} # 유사성과 다양성을 고려\n",
    ")\n",
    "retriever.invoke('Word2Vec에 대해 알려줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './nlp-keywords.txt'}, page_content='Word2Vec'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.'),\n",
       " Document(metadata={'source': './nlp-keywords.txt'}, page_content='예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type = 'similarity_score_threshold', search_kwargs = {'score_threshold' : 0.8}\n",
    "    # 유사도 점수 기반\n",
    "    # 검색된 문서의 유사도 점수가 기준값인 score_threshold 이상일 경우만 결과에 포함\n",
    ")\n",
    "retriever.invoke('Word2Vec에 대해 알려줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './nlp-keywords.txt'}, page_content='Word2Vec')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_kwargs = {'k':1})\n",
    "retriever.invoke('Word2Vec에 대해 알려줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './finance-keywords.txt'}, page_content='Growth Stock'),\n",
       " Document(metadata={'source': './finance-keywords.txt'}, page_content='정의: 성장주는 평균 이상의 높은 성장률을 보이는 기업의 주식을 의미합니다.\\n예시: 페이스북(메타)과 같은 기술 기업들은 S&P 500에 포함된 대표적인 성장주로 꼽힙니다.')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever(search_kwargs={'filter' : {'source' : './finance-keywords.txt'}, 'k':2})\n",
    "retriever.invoke('Growth Stock에 대해 알려줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bs4.element.SoupStrainer at 0x2c0e67a4d90>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs4.SoupStrainer(\n",
    "    'div',\n",
    "    attrs = {'class' : ['newsct_article _article_body', 'media_end_head_title']}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_path = ('https://n.news.naver.com/article/437/0000420199?sid=101'),\n",
    "    bs_kwargs = dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            'div',\n",
    "        attrs = {'class' : ['newsct_article _article_body', 'media_end_head_title']}\n",
    "        )       \n",
    "    )\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 100, chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "# 벡터 db 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "# 검색기 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    당신은 질문에 따라 답변을 수행하는 친절한 AI 비서 입니다. 당신은 주어진 contect에서 주어진 question에 답하는 것을 수행합니다.\n",
    "    검색된 결과인 다음 context를 사용하여 질문인 question에 답하세요.\n",
    "    만약, context에서 답을 찾을 수 없거나, 답을 모른다면 '주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다.' 라고 한글로 답변해 주세요.\n",
    "    이름이나 기술적인 용어는 번역하지 않고 그대로 출력해 주세요.\n",
    "\n",
    "    # Question : {question}\n",
    "    # Context : {context}\n",
    "    # Answer : \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = 'gpt-4o', temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {'context' : retriever, 'question' : RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    |StrOutputParser() # 출력을 깔꼼하게 정리!!!!!!!!!!!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다."
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_response\n",
    "answer = rag_chain.stream('로봇의 이름')\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현대차 그룹이 공개한 산업용 웨어러블 로봇의 이름은 '엑스블 숄더'입니다."
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_response\n",
    "answer = rag_chain.stream('로봇 이름')\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조끼 무게는 2kg가 채 되지 않습니다."
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_response\n",
    "answer = rag_chain.stream('조끼의 무게')\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조끼의 장점은 어깨에 들어가는 힘을 최대 60%까지 줄여주어 오래 팔을 들고 있어도 크게 무리가 가지 않는다는 점입니다. 또한, 조끼의 무게는 2kg가 채 되지 않아 가볍습니다."
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_response\n",
    "answer = rag_chain.stream('조끼의 장점')\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'엑스블 숄더'는 현대차 그룹이 공개한 산업용 웨어러블 로봇으로, 어깨를 지지해 주는 장치가 연결되어 있습니다. 이 로봇은 팔을 위로 올려 부품을 조립해야 하는 완성차 제조공장에서 생산직 노동자들에게 도움을 줄 수 있으며, 조끼를 입기만 하면 힘을 강하게 만들어 주는 기능을 가지고 있습니다. 또한, 산업 현장의 생산직 노동자뿐만 아니라 재활 치료와 일상생활에도 도움을 줄 수 있는 웨어러블 로봇입니다."
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_response\n",
    "answer = rag_chain.stream('엑스블 숄더에 대해 설명해줘')\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
